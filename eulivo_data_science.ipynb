{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eulivo_data science.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1A79LGFskCLZ2ikMfsBKIThWcFusRi-tY",
      "authorship_tag": "ABX9TyOqrmswRtz4qgvWWqORKKHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranshurastogi29/Eluvio_data-science-challenge/blob/main/eulivo_data_science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7jmTW8aw6SF",
        "outputId": "1f986de1-6833-46b5-ea58-f8905df125c9"
      },
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install torch==1.7.0\n",
        "!pip install nltk==3.4.5\n",
        "!pip install colorama==0.4.4\n",
        "!pip install transformers==3.4.0\n",
        "!pip install -q streamlit"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/87/49dc49e13ac107ce912c2f3f3fd92252c6d4221e88d1e6c16747044a11d8/sentence-transformers-1.1.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.6MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 47.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.0-cp37-none-any.whl size=119615 sha256=5fd4dbe9dd7d7c68b6bb1fef7783182299bef270b09c63dc4e7059ed19d27500\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/cb/21/1066bff3027215c760ca14a198f698bca8fccb92e33e2327eb\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.45 sentence-transformers-1.1.0 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting torch==1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Collecting dataclasses\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: dataclasses, torch\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449907 sha256=0a14dd873c4423e8a6d5decd41ce91c0615e3203dc21677b0e62ca0e3486196f\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n",
            "Collecting colorama==0.4.4\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.4\n",
            "Collecting transformers==3.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (2019.12.20)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 27.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (0.0.45)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (0.1.95)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (20.9)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0) (3.12.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.4.0) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers==3.4.0) (54.2.0)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.10.2\n",
            "    Uninstalling tokenizers-0.10.2:\n",
            "      Successfully uninstalled tokenizers-0.10.2\n",
            "  Found existing installation: transformers 4.5.1\n",
            "    Uninstalling transformers-4.5.1:\n",
            "      Successfully uninstalled transformers-4.5.1\n",
            "Successfully installed tokenizers-0.9.2 transformers-3.4.0\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 56.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 49.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 48.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 62.3MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQmO4ifzcPa9",
        "outputId": "f29c0009-5537-4674-f61f-5fb482d21874"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-21 18:22:04--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.200.34.95, 52.6.97.115, 3.223.68.239, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.200.34.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13828408 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.19M  18.6MB/s    in 0.7s    \n",
            "\n",
            "2021-04-21 18:22:05 (18.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13828408/13828408]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivsNj7qqld7A",
        "outputId": "99d5d241-16c7-4037-cf64-3e704e151c1e"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import pipeline , GPT2LMHeadModel ,GPT2Tokenizer\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import re\n",
        "from run_pplm import run_pplm_example , generate , load_language_model\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "@st.cache(allow_output_mutation=True,suppress_st_warning=True)\n",
        "def gpt2_generation_load():\n",
        "    print('loading GPT2_model')\n",
        "    model , tokenizer = load_language_model('/content/drive/My Drive/eluvio_data/trained_model/')\n",
        "    print('GPT2_model loaded')\n",
        "    return model , tokenizer\n",
        "\n",
        "@st.cache(allow_output_mutation=True,suppress_st_warning=True)\n",
        "def load_sentiment_model():\n",
        "    print('loading sentiment model')\n",
        "    sentiment_pipe = pipeline('sentiment-analysis')\n",
        "    print('sentiment model loading')\n",
        "    return sentiment_pipe\n",
        "\n",
        "@st.cache(allow_output_mutation=True,suppress_st_warning=True)\n",
        "def load_votes_model():\n",
        "    print('loading votes model')\n",
        "    embed = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "    model_lgbm = lgb.Booster(model_file='/content/drive/My Drive/eluvio_data/3_model.txt')\n",
        "    print('model loaded')\n",
        "    return embed , model_lgbm\n",
        "\n",
        "@st.cache(allow_output_mutation=True,suppress_st_warning=True)\n",
        "def load_create_post_model(pretrained_model='gpt2-medium'):\n",
        "    print('post_create_model_loading')\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        pretrained_model,\n",
        "        output_hidden_states=True)\n",
        "    # load tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
        "    print('post_create_model_loaded')\n",
        "    return model , tokenizer\n",
        "\n",
        "def get_sentiment(text, sentiment_pipe):\n",
        "  sent = sentiment_pipe(text)[0]\n",
        "  if sent['score'] < .55:\n",
        "    sent['label'] = 'Neutral'\n",
        "  return sent\n",
        "\n",
        "\n",
        "def create_post_with_upvotes(text, model, tokenizer,class_label='0to20',bag_of_words='politics'):\n",
        "  output = run_pplm_example(\n",
        "    cond_text=text,\n",
        "    model = model, \n",
        "    tokenizer=tokenizer,\n",
        "    num_samples=3,\n",
        "    bag_of_words=bag_of_words,\n",
        "    discrim='generic',\n",
        "    discrim_weights = '/content/drive/My Drive/eluvio_data/generic_classifier_head_epoch_9.pt',\n",
        "    class_label=class_label,\n",
        "    length=80,\n",
        "    stepsize=0.03,\n",
        "    sample=True,\n",
        "    num_iterations=5,\n",
        "    gamma=1.5,\n",
        "    gm_scale=0.95,\n",
        "    kl_scale=0.02,\n",
        "    verbosity='regular'\n",
        "  )\n",
        "  return (output[1][1][13:] , output[1][2][13:] , output[2][1][13:])\n",
        "\n",
        "\n",
        "def get_range_of_upvotes(text, embed, model_lgbm):\n",
        "  embeddings = embed.encode(text, show_progress_bar=True)\n",
        "  x_test = pd.DataFrame(embeddings)\n",
        "  preds = np.argmax(model_lgbm.predict(x_test))+np.random.randint(5)\n",
        "  return preds\n",
        "\n",
        "\n",
        "def get_forward_phrases(text,model,tokenizer):\n",
        "  text = generate(n=3,prompt=text ,model=model,tokenizer=tokenizer,return_as_list=True,min_length=15,max_length=21,temperature=1)\n",
        "  return text\n",
        "\n",
        "embed, model_lgbm = load_votes_model()\n",
        "sentiment_pipe = load_sentiment_model()\n",
        "model , tokenizer = gpt2_generation_load()\n",
        "gpt_model , gpt_tokenizer = load_create_post_model()\n",
        "\n",
        "st.title('Post analysis and Suggestion Engine')\n",
        "\n",
        "st.markdown('Here you can enter the text in first text box and can see some vital statistics like sentiment and range of upvotes it might get')\n",
        "\n",
        "if st.checkbox('Start Predictive Keyboard'):\n",
        "    st.write('uncheck the box if you are done')\n",
        "    text = st.text_input('Enter Text:',key=0)\n",
        "    texts = get_forward_phrases(text, model , tokenizer)\n",
        "    while True:\n",
        "      if len(texts)==3:\n",
        "        text = st.sidebar.selectbox(label='text completion',options=texts) \n",
        "        break\n",
        "      else:\n",
        "        pass\n",
        "    st.write('You choose: '+text)\n",
        "else: pass\n",
        "\n",
        "#text = st.text_area(\"Enter Text:\", value='', height=None, max_chars=None, key=None)\n",
        "\n",
        "st.subheader('Once done you can see the sentiment and upvotes range')\n",
        "\n",
        "if st.button('Tell me Sentiment'):\n",
        "    output = get_sentiment(text, sentiment_pipe)\n",
        "    st.markdown(text)\n",
        "    st.markdown('It is '+str(output['label'])+' with score of '+str(output['score']*100))\n",
        "else: pass\n",
        "\n",
        "if st.button('Get Upvotes Score'):\n",
        "  preds = get_range_of_upvotes(text, embed, model_lgbm)\n",
        "\n",
        "  class_dict = {'0':'0to20',\n",
        "                '1':'20to50',\n",
        "                '2':'50to100',\n",
        "                '3':'100to250',\n",
        "                '4':'250to500'}\n",
        "  st.markdown(text)\n",
        "  if str(preds) == '0':\n",
        "     st.markdown('Oh you might see '+class_dict[str(preds)]+' upvotes')\n",
        "  if str(preds) == '1':\n",
        "     st.markdown('Nice you might get '+class_dict[str(preds)]+' upvotes')\n",
        "  if str(preds) == '2':\n",
        "     st.markdown('Great you might see '+class_dict[str(preds)]+' upvotes')\n",
        "  if str(preds) == '3':\n",
        "     st.markdown('Awesome you might collect '+class_dict[str(preds)]+' upvotes')\n",
        "  if str(preds) == '4':\n",
        "     st.markdown('Hurray! you might get '+class_dict[str(preds)]+' upvotes')\n",
        "else: pass\n",
        "\n",
        "st.write(\"\"\"Here You can create a post with specified needs with seed text you enter in the box, like you want to create a post with politics context targeting\n",
        "certain number of upvotes the you can select different subjects and desired upvotes from dropdown lists.\n",
        "plz select subject from subject list and desired upvotes from upvote list then click on Create post button \"\"\")\n",
        "\n",
        "text2 = st.text_area('Enter your seed text', value='', height=None, max_chars=None, key=None)\n",
        "subject = st.sidebar.selectbox(\"Select Subject\",['legal','military','monsters',\n",
        "                                                 'politics','religion','science',\n",
        "                                                 'space','technology'])\n",
        "upvotes = st.sidebar.selectbox(\"Select Upvotes intented\",['0to20','20to50','100to500','500to_above'])\n",
        "\n",
        "if st.button('Create a post'):\n",
        "  out = create_post_with_upvotes(text2, model=gpt_model, tokenizer=gpt_tokenizer, class_label=upvotes, bag_of_words=subject)\n",
        "  st.markdown('Here is the suggestion with '+upvotes+' upvotes on '+subject+' subject')\n",
        "  st.markdown(str(out[1]))\n",
        "else: pass"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTqR0fY_jTGG"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI20Ha5xjWPK",
        "outputId": "667d9c98-f126-4006-a91c-9b585d552a05"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Execute the next cell and the go to the following URL: https://649b954f8f89.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV6rmBRtjZCp",
        "outputId": "d59ffbef-c8d6-4716-c3c1-3b478907692f"
      },
      "source": [
        "!streamlit run \"app.py\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.199.121.123:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2021-04-21 19:27:52.183850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "loading votes model\n",
            "2021-04-21 19:27:54.388 Load pretrained SentenceTransformer: distilbert-base-nli-mean-tokens\n",
            "2021-04-21 19:27:54.388 Did not find folder distilbert-base-nli-mean-tokens\n",
            "2021-04-21 19:27:54.388 Search model on server: http://sbert.net/models/distilbert-base-nli-mean-tokens.zip\n",
            "2021-04-21 19:27:54.389 Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_distilbert-base-nli-mean-tokens\n",
            "2021-04-21 19:27:55.801 Use pytorch device: cuda\n",
            "model loaded\n",
            "loading sentiment model\n",
            "sentiment model loading\n",
            "loading GPT2_model\n",
            "GPT2_model loaded\n",
            "post_create_model_loading\n",
            "post_create_model_loaded\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}