{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "honest-update",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-17T15:49:52.783767Z",
     "iopub.status.busy": "2021-04-17T15:49:52.783018Z",
     "iopub.status.idle": "2021-04-17T15:50:44.196597Z",
     "shell.execute_reply": "2021-04-17T15:50:44.195931Z"
    },
    "papermill": {
     "duration": 51.422963,
     "end_time": "2021-04-17T15:50:44.196757",
     "exception": false,
     "start_time": "2021-04-17T15:49:52.773794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.7.0 in /opt/conda/lib/python3.7/site-packages (1.7.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.7.0) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7.0) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch==1.7.0) (0.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7.0) (1.19.5)\r\n",
      "Collecting nltk==3.4.5\r\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 1.2 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk==3.4.5) (1.15.0)\r\n",
      "Building wheels for collected packages: nltk\r\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449908 sha256=b1d36a79386a3920f35e5818b9f6399a3e19e661be70d9177ee336924475b2f0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\r\n",
      "Successfully built nltk\r\n",
      "Installing collected packages: nltk\r\n",
      "  Attempting uninstall: nltk\r\n",
      "    Found existing installation: nltk 3.2.4\r\n",
      "    Uninstalling nltk-3.2.4:\r\n",
      "      Successfully uninstalled nltk-3.2.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.4.5 which is incompatible.\u001b[0m\r\n",
      "Successfully installed nltk-3.4.5\r\n",
      "Requirement already satisfied: colorama==0.4.4 in /opt/conda/lib/python3.7/site-packages (0.4.4)\r\n",
      "Collecting transformers==3.4.0\r\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 1.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (3.15.6)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (0.1.95)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (0.0.43)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (4.56.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (20.9)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (2.25.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (1.19.5)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (3.0.12)\r\n",
      "Collecting tokenizers==0.9.2\r\n",
      "  Downloading tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 6.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.4.0) (2020.11.13)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.4.0) (2.4.7)\r\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from protobuf->transformers==3.4.0) (1.15.0)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (2020.12.5)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (1.26.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.4.0) (2.10)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.4.0) (7.1.2)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.4.0) (1.0.1)\r\n",
      "Installing collected packages: tokenizers, transformers\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.10.1\r\n",
      "    Uninstalling tokenizers-0.10.1:\r\n",
      "      Successfully uninstalled tokenizers-0.10.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.4.2\r\n",
      "    Uninstalling transformers-4.4.2:\r\n",
      "      Successfully uninstalled transformers-4.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "allennlp 2.2.0 requires transformers<4.5,>=4.1, but you have transformers 3.4.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed tokenizers-0.9.2 transformers-3.4.0\r\n",
      "Collecting torchtext==0.3.1\r\n",
      "  Downloading torchtext-0.3.1-py3-none-any.whl (62 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 62 kB 524 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext==0.3.1) (1.7.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.3.1) (4.56.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.3.1) (1.19.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.3.1) (2.25.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.3.1) (1.26.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.3.1) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.3.1) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.3.1) (2020.12.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.3.1) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.3.1) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.3.1) (0.6)\r\n",
      "Installing collected packages: torchtext\r\n",
      "  Attempting uninstall: torchtext\r\n",
      "    Found existing installation: torchtext 0.8.0a0+cd6902d\r\n",
      "    Uninstalling torchtext-0.8.0a0+cd6902d:\r\n",
      "      Successfully uninstalled torchtext-0.8.0a0+cd6902d\r\n",
      "Successfully installed torchtext-0.3.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.7.0\n",
    "!pip install nltk==3.4.5\n",
    "!pip install colorama==0.4.4\n",
    "!pip install transformers==3.4.0\n",
    "!pip install torchtext==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legal-frost",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-17T15:50:44.286082Z",
     "iopub.status.busy": "2021-04-17T15:50:44.268342Z",
     "iopub.status.idle": "2021-04-17T23:02:47.284093Z",
     "shell.execute_reply": "2021-04-17T23:02:47.283557Z"
    },
    "papermill": {
     "duration": 25923.066212,
     "end_time": "2021-04-17T23:02:47.284240",
     "exception": false,
     "start_time": "2021-04-17T15:50:44.218028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359236, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25521it [00:00, 255194.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing generic dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359236it [00:01, 267168.15it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1a1118bdae4610bcf1abd10ef5252c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1e9e0c0d984f018fc6fc596f701ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2119e126703a4911884abb85ab51cdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1390031f729544b5b5564fee8d235345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 359235 data points\n",
      "Data preprocessing took: 194.578s\n",
      "\n",
      "Epoch 1\n",
      "Train Epoch: 1 [256/359035 (0%)]\tLoss: 1.474851\n",
      "Train Epoch: 1 [1536/359035 (0%)]\tLoss: 1.070327\n",
      "Train Epoch: 1 [2816/359035 (1%)]\tLoss: 0.944658\n",
      "Train Epoch: 1 [4096/359035 (1%)]\tLoss: 0.988597\n",
      "Train Epoch: 1 [5376/359035 (1%)]\tLoss: 0.812448\n",
      "Train Epoch: 1 [6656/359035 (2%)]\tLoss: 0.932296\n",
      "Train Epoch: 1 [7936/359035 (2%)]\tLoss: 0.921614\n",
      "Train Epoch: 1 [9216/359035 (3%)]\tLoss: 0.962106\n",
      "Train Epoch: 1 [10496/359035 (3%)]\tLoss: 0.960466\n",
      "Train Epoch: 1 [11776/359035 (3%)]\tLoss: 1.073434\n",
      "Train Epoch: 1 [13056/359035 (4%)]\tLoss: 0.993202\n",
      "Train Epoch: 1 [14336/359035 (4%)]\tLoss: 0.953431\n",
      "Train Epoch: 1 [15616/359035 (4%)]\tLoss: 0.925358\n",
      "Train Epoch: 1 [16896/359035 (5%)]\tLoss: 0.892808\n",
      "Train Epoch: 1 [18176/359035 (5%)]\tLoss: 0.934491\n",
      "Train Epoch: 1 [19456/359035 (5%)]\tLoss: 0.899869\n",
      "Train Epoch: 1 [20736/359035 (6%)]\tLoss: 0.917047\n",
      "Train Epoch: 1 [22016/359035 (6%)]\tLoss: 0.888925\n",
      "Train Epoch: 1 [23296/359035 (6%)]\tLoss: 0.984600\n",
      "Train Epoch: 1 [24576/359035 (7%)]\tLoss: 0.883777\n",
      "Train Epoch: 1 [25856/359035 (7%)]\tLoss: 0.861095\n",
      "Train Epoch: 1 [27136/359035 (8%)]\tLoss: 0.862661\n",
      "Train Epoch: 1 [28416/359035 (8%)]\tLoss: 0.908977\n",
      "Train Epoch: 1 [29696/359035 (8%)]\tLoss: 0.899600\n",
      "Train Epoch: 1 [30976/359035 (9%)]\tLoss: 0.920099\n",
      "Train Epoch: 1 [32256/359035 (9%)]\tLoss: 1.004804\n",
      "Train Epoch: 1 [33536/359035 (9%)]\tLoss: 0.915654\n",
      "Train Epoch: 1 [34816/359035 (10%)]\tLoss: 0.835808\n",
      "Train Epoch: 1 [36096/359035 (10%)]\tLoss: 0.983476\n",
      "Train Epoch: 1 [37376/359035 (10%)]\tLoss: 0.942596\n",
      "Train Epoch: 1 [38656/359035 (11%)]\tLoss: 0.871733\n",
      "Train Epoch: 1 [39936/359035 (11%)]\tLoss: 0.950460\n",
      "Train Epoch: 1 [41216/359035 (11%)]\tLoss: 0.902022\n",
      "Train Epoch: 1 [42496/359035 (12%)]\tLoss: 1.023981\n",
      "Train Epoch: 1 [43776/359035 (12%)]\tLoss: 0.872952\n",
      "Train Epoch: 1 [45056/359035 (13%)]\tLoss: 0.841815\n",
      "Train Epoch: 1 [46336/359035 (13%)]\tLoss: 0.918158\n",
      "Train Epoch: 1 [47616/359035 (13%)]\tLoss: 0.916756\n",
      "Train Epoch: 1 [48896/359035 (14%)]\tLoss: 0.950182\n",
      "Train Epoch: 1 [50176/359035 (14%)]\tLoss: 1.015605\n",
      "Train Epoch: 1 [51456/359035 (14%)]\tLoss: 0.905658\n",
      "Train Epoch: 1 [52736/359035 (15%)]\tLoss: 0.976143\n",
      "Train Epoch: 1 [54016/359035 (15%)]\tLoss: 0.769839\n",
      "Train Epoch: 1 [55296/359035 (15%)]\tLoss: 0.920794\n",
      "Train Epoch: 1 [56576/359035 (16%)]\tLoss: 1.012024\n",
      "Train Epoch: 1 [57856/359035 (16%)]\tLoss: 0.888963\n",
      "Train Epoch: 1 [59136/359035 (16%)]\tLoss: 0.803964\n",
      "Train Epoch: 1 [60416/359035 (17%)]\tLoss: 0.801234\n",
      "Train Epoch: 1 [61696/359035 (17%)]\tLoss: 0.898758\n",
      "Train Epoch: 1 [62976/359035 (18%)]\tLoss: 0.933054\n",
      "Train Epoch: 1 [64256/359035 (18%)]\tLoss: 0.920436\n",
      "Train Epoch: 1 [65536/359035 (18%)]\tLoss: 0.923307\n",
      "Train Epoch: 1 [66816/359035 (19%)]\tLoss: 0.940667\n",
      "Train Epoch: 1 [68096/359035 (19%)]\tLoss: 0.871462\n",
      "Train Epoch: 1 [69376/359035 (19%)]\tLoss: 0.865690\n",
      "Train Epoch: 1 [70656/359035 (20%)]\tLoss: 0.847754\n",
      "Train Epoch: 1 [71936/359035 (20%)]\tLoss: 0.905361\n",
      "Train Epoch: 1 [73216/359035 (20%)]\tLoss: 0.851231\n",
      "Train Epoch: 1 [74496/359035 (21%)]\tLoss: 0.879968\n",
      "Train Epoch: 1 [75776/359035 (21%)]\tLoss: 0.887115\n",
      "Train Epoch: 1 [77056/359035 (21%)]\tLoss: 0.908059\n",
      "Train Epoch: 1 [78336/359035 (22%)]\tLoss: 0.875918\n",
      "Train Epoch: 1 [79616/359035 (22%)]\tLoss: 0.824369\n",
      "Train Epoch: 1 [80896/359035 (23%)]\tLoss: 0.972619\n",
      "Train Epoch: 1 [82176/359035 (23%)]\tLoss: 0.890274\n",
      "Train Epoch: 1 [83456/359035 (23%)]\tLoss: 1.004563\n",
      "Train Epoch: 1 [84736/359035 (24%)]\tLoss: 0.937348\n",
      "Train Epoch: 1 [86016/359035 (24%)]\tLoss: 0.982002\n",
      "Train Epoch: 1 [87296/359035 (24%)]\tLoss: 0.811587\n",
      "Train Epoch: 1 [88576/359035 (25%)]\tLoss: 0.933330\n",
      "Train Epoch: 1 [89856/359035 (25%)]\tLoss: 0.815331\n",
      "Train Epoch: 1 [91136/359035 (25%)]\tLoss: 0.924585\n",
      "Train Epoch: 1 [92416/359035 (26%)]\tLoss: 0.891282\n",
      "Train Epoch: 1 [93696/359035 (26%)]\tLoss: 0.895394\n",
      "Train Epoch: 1 [94976/359035 (26%)]\tLoss: 0.994373\n",
      "Train Epoch: 1 [96256/359035 (27%)]\tLoss: 0.881693\n",
      "Train Epoch: 1 [97536/359035 (27%)]\tLoss: 0.795826\n",
      "Train Epoch: 1 [98816/359035 (28%)]\tLoss: 0.893352\n",
      "Train Epoch: 1 [100096/359035 (28%)]\tLoss: 0.884935\n",
      "Train Epoch: 1 [101376/359035 (28%)]\tLoss: 0.890265\n",
      "Train Epoch: 1 [102656/359035 (29%)]\tLoss: 0.886116\n",
      "Train Epoch: 1 [103936/359035 (29%)]\tLoss: 0.882794\n",
      "Train Epoch: 1 [105216/359035 (29%)]\tLoss: 0.884470\n",
      "Train Epoch: 1 [106496/359035 (30%)]\tLoss: 0.835606\n",
      "Train Epoch: 1 [107776/359035 (30%)]\tLoss: 0.904440\n",
      "Train Epoch: 1 [109056/359035 (30%)]\tLoss: 0.842860\n",
      "Train Epoch: 1 [110336/359035 (31%)]\tLoss: 0.936800\n",
      "Train Epoch: 1 [111616/359035 (31%)]\tLoss: 0.963554\n",
      "Train Epoch: 1 [112896/359035 (31%)]\tLoss: 0.899176\n",
      "Train Epoch: 1 [114176/359035 (32%)]\tLoss: 0.896509\n",
      "Train Epoch: 1 [115456/359035 (32%)]\tLoss: 0.934017\n",
      "Train Epoch: 1 [116736/359035 (33%)]\tLoss: 0.907981\n",
      "Train Epoch: 1 [118016/359035 (33%)]\tLoss: 0.825949\n",
      "Train Epoch: 1 [119296/359035 (33%)]\tLoss: 0.923714\n",
      "Train Epoch: 1 [120576/359035 (34%)]\tLoss: 0.916609\n",
      "Train Epoch: 1 [121856/359035 (34%)]\tLoss: 0.883282\n",
      "Train Epoch: 1 [123136/359035 (34%)]\tLoss: 0.777341\n",
      "Train Epoch: 1 [124416/359035 (35%)]\tLoss: 0.849099\n",
      "Train Epoch: 1 [125696/359035 (35%)]\tLoss: 0.934460\n",
      "Train Epoch: 1 [126976/359035 (35%)]\tLoss: 0.907433\n",
      "Train Epoch: 1 [128256/359035 (36%)]\tLoss: 0.874297\n",
      "Train Epoch: 1 [129536/359035 (36%)]\tLoss: 0.804303\n",
      "Train Epoch: 1 [130816/359035 (36%)]\tLoss: 0.865502\n",
      "Train Epoch: 1 [132096/359035 (37%)]\tLoss: 0.878679\n",
      "Train Epoch: 1 [133376/359035 (37%)]\tLoss: 0.950901\n",
      "Train Epoch: 1 [134656/359035 (38%)]\tLoss: 0.922660\n",
      "Train Epoch: 1 [135936/359035 (38%)]\tLoss: 0.855180\n",
      "Train Epoch: 1 [137216/359035 (38%)]\tLoss: 0.916573\n",
      "Train Epoch: 1 [138496/359035 (39%)]\tLoss: 0.856302\n",
      "Train Epoch: 1 [139776/359035 (39%)]\tLoss: 0.921451\n",
      "Train Epoch: 1 [141056/359035 (39%)]\tLoss: 0.880737\n",
      "Train Epoch: 1 [142336/359035 (40%)]\tLoss: 0.975389\n",
      "Train Epoch: 1 [143616/359035 (40%)]\tLoss: 0.870207\n",
      "Train Epoch: 1 [144896/359035 (40%)]\tLoss: 0.799444\n",
      "Train Epoch: 1 [146176/359035 (41%)]\tLoss: 0.865437\n",
      "Train Epoch: 1 [147456/359035 (41%)]\tLoss: 0.931988\n",
      "Train Epoch: 1 [148736/359035 (41%)]\tLoss: 0.910865\n",
      "Train Epoch: 1 [150016/359035 (42%)]\tLoss: 0.907490\n",
      "Train Epoch: 1 [151296/359035 (42%)]\tLoss: 0.862611\n",
      "Train Epoch: 1 [152576/359035 (42%)]\tLoss: 0.871273\n",
      "Train Epoch: 1 [153856/359035 (43%)]\tLoss: 0.815811\n",
      "Train Epoch: 1 [155136/359035 (43%)]\tLoss: 0.931756\n",
      "Train Epoch: 1 [156416/359035 (44%)]\tLoss: 0.850396\n",
      "Train Epoch: 1 [157696/359035 (44%)]\tLoss: 0.905895\n",
      "Train Epoch: 1 [158976/359035 (44%)]\tLoss: 0.859779\n",
      "Train Epoch: 1 [160256/359035 (45%)]\tLoss: 0.801268\n",
      "Train Epoch: 1 [161536/359035 (45%)]\tLoss: 0.907915\n",
      "Train Epoch: 1 [162816/359035 (45%)]\tLoss: 0.876044\n",
      "Train Epoch: 1 [164096/359035 (46%)]\tLoss: 0.818300\n",
      "Train Epoch: 1 [165376/359035 (46%)]\tLoss: 0.848749\n",
      "Train Epoch: 1 [166656/359035 (46%)]\tLoss: 0.982204\n",
      "Train Epoch: 1 [167936/359035 (47%)]\tLoss: 0.933765\n",
      "Train Epoch: 1 [169216/359035 (47%)]\tLoss: 0.903345\n",
      "Train Epoch: 1 [170496/359035 (47%)]\tLoss: 0.871984\n",
      "Train Epoch: 1 [171776/359035 (48%)]\tLoss: 0.913896\n",
      "Train Epoch: 1 [173056/359035 (48%)]\tLoss: 0.841853\n",
      "Train Epoch: 1 [174336/359035 (49%)]\tLoss: 0.886839\n",
      "Train Epoch: 1 [175616/359035 (49%)]\tLoss: 0.854892\n",
      "Train Epoch: 1 [176896/359035 (49%)]\tLoss: 0.886350\n",
      "Train Epoch: 1 [178176/359035 (50%)]\tLoss: 0.924007\n",
      "Train Epoch: 1 [179456/359035 (50%)]\tLoss: 0.844409\n",
      "Train Epoch: 1 [180736/359035 (50%)]\tLoss: 0.972367\n",
      "Train Epoch: 1 [182016/359035 (51%)]\tLoss: 0.944752\n",
      "Train Epoch: 1 [183296/359035 (51%)]\tLoss: 0.901078\n",
      "Train Epoch: 1 [184576/359035 (51%)]\tLoss: 0.752923\n",
      "Train Epoch: 1 [185856/359035 (52%)]\tLoss: 0.901691\n",
      "Train Epoch: 1 [187136/359035 (52%)]\tLoss: 0.850521\n",
      "Train Epoch: 1 [188416/359035 (52%)]\tLoss: 0.935947\n",
      "Train Epoch: 1 [189696/359035 (53%)]\tLoss: 0.869290\n",
      "Train Epoch: 1 [190976/359035 (53%)]\tLoss: 0.836268\n",
      "Train Epoch: 1 [192256/359035 (54%)]\tLoss: 0.785155\n",
      "Train Epoch: 1 [193536/359035 (54%)]\tLoss: 0.965777\n",
      "Train Epoch: 1 [194816/359035 (54%)]\tLoss: 0.848460\n",
      "Train Epoch: 1 [196096/359035 (55%)]\tLoss: 0.754765\n",
      "Train Epoch: 1 [197376/359035 (55%)]\tLoss: 0.891227\n",
      "Train Epoch: 1 [198656/359035 (55%)]\tLoss: 0.866065\n",
      "Train Epoch: 1 [199936/359035 (56%)]\tLoss: 0.919115\n",
      "Train Epoch: 1 [201216/359035 (56%)]\tLoss: 0.851669\n",
      "Train Epoch: 1 [202496/359035 (56%)]\tLoss: 0.936464\n",
      "Train Epoch: 1 [203776/359035 (57%)]\tLoss: 0.865708\n",
      "Train Epoch: 1 [205056/359035 (57%)]\tLoss: 0.965166\n",
      "Train Epoch: 1 [206336/359035 (57%)]\tLoss: 0.920718\n",
      "Train Epoch: 1 [207616/359035 (58%)]\tLoss: 0.802270\n",
      "Train Epoch: 1 [208896/359035 (58%)]\tLoss: 0.844596\n",
      "Train Epoch: 1 [210176/359035 (59%)]\tLoss: 0.916387\n",
      "Train Epoch: 1 [211456/359035 (59%)]\tLoss: 0.785662\n",
      "Train Epoch: 1 [212736/359035 (59%)]\tLoss: 0.931277\n",
      "Train Epoch: 1 [214016/359035 (60%)]\tLoss: 0.887871\n",
      "Train Epoch: 1 [215296/359035 (60%)]\tLoss: 0.801769\n",
      "Train Epoch: 1 [216576/359035 (60%)]\tLoss: 0.808124\n",
      "Train Epoch: 1 [217856/359035 (61%)]\tLoss: 0.950167\n",
      "Train Epoch: 1 [219136/359035 (61%)]\tLoss: 0.851775\n",
      "Train Epoch: 1 [220416/359035 (61%)]\tLoss: 0.964578\n",
      "Train Epoch: 1 [221696/359035 (62%)]\tLoss: 0.988535\n",
      "Train Epoch: 1 [222976/359035 (62%)]\tLoss: 0.806215\n",
      "Train Epoch: 1 [224256/359035 (62%)]\tLoss: 1.055814\n",
      "Train Epoch: 1 [225536/359035 (63%)]\tLoss: 0.874591\n",
      "Train Epoch: 1 [226816/359035 (63%)]\tLoss: 0.814590\n",
      "Train Epoch: 1 [228096/359035 (64%)]\tLoss: 0.943513\n",
      "Train Epoch: 1 [229376/359035 (64%)]\tLoss: 0.891792\n",
      "Train Epoch: 1 [230656/359035 (64%)]\tLoss: 0.800192\n",
      "Train Epoch: 1 [231936/359035 (65%)]\tLoss: 0.885504\n",
      "Train Epoch: 1 [233216/359035 (65%)]\tLoss: 0.942979\n",
      "Train Epoch: 1 [234496/359035 (65%)]\tLoss: 0.827274\n",
      "Train Epoch: 1 [235776/359035 (66%)]\tLoss: 0.852972\n",
      "Train Epoch: 1 [237056/359035 (66%)]\tLoss: 0.934834\n",
      "Train Epoch: 1 [238336/359035 (66%)]\tLoss: 0.984688\n",
      "Train Epoch: 1 [239616/359035 (67%)]\tLoss: 0.834317\n",
      "Train Epoch: 1 [240896/359035 (67%)]\tLoss: 0.815592\n",
      "Train Epoch: 1 [242176/359035 (67%)]\tLoss: 0.854575\n",
      "Train Epoch: 1 [243456/359035 (68%)]\tLoss: 0.889575\n",
      "Train Epoch: 1 [244736/359035 (68%)]\tLoss: 0.883530\n",
      "Train Epoch: 1 [246016/359035 (69%)]\tLoss: 0.802893\n",
      "Train Epoch: 1 [247296/359035 (69%)]\tLoss: 0.900582\n",
      "Train Epoch: 1 [248576/359035 (69%)]\tLoss: 0.935948\n",
      "Train Epoch: 1 [249856/359035 (70%)]\tLoss: 0.837074\n",
      "Train Epoch: 1 [251136/359035 (70%)]\tLoss: 0.953252\n",
      "Train Epoch: 1 [252416/359035 (70%)]\tLoss: 0.805949\n",
      "Train Epoch: 1 [253696/359035 (71%)]\tLoss: 0.910651\n",
      "Train Epoch: 1 [254976/359035 (71%)]\tLoss: 0.803432\n",
      "Train Epoch: 1 [256256/359035 (71%)]\tLoss: 0.840611\n",
      "Train Epoch: 1 [257536/359035 (72%)]\tLoss: 0.925411\n",
      "Train Epoch: 1 [258816/359035 (72%)]\tLoss: 0.883334\n",
      "Train Epoch: 1 [260096/359035 (72%)]\tLoss: 0.895304\n",
      "Train Epoch: 1 [261376/359035 (73%)]\tLoss: 0.889763\n",
      "Train Epoch: 1 [262656/359035 (73%)]\tLoss: 0.893112\n",
      "Train Epoch: 1 [263936/359035 (74%)]\tLoss: 0.884008\n",
      "Train Epoch: 1 [265216/359035 (74%)]\tLoss: 0.803783\n",
      "Train Epoch: 1 [266496/359035 (74%)]\tLoss: 0.848600\n",
      "Train Epoch: 1 [267776/359035 (75%)]\tLoss: 0.812391\n",
      "Train Epoch: 1 [269056/359035 (75%)]\tLoss: 0.861560\n",
      "Train Epoch: 1 [270336/359035 (75%)]\tLoss: 0.783982\n",
      "Train Epoch: 1 [271616/359035 (76%)]\tLoss: 0.850664\n",
      "Train Epoch: 1 [272896/359035 (76%)]\tLoss: 0.894764\n",
      "Train Epoch: 1 [274176/359035 (76%)]\tLoss: 0.901266\n",
      "Train Epoch: 1 [275456/359035 (77%)]\tLoss: 0.890554\n",
      "Train Epoch: 1 [276736/359035 (77%)]\tLoss: 0.897230\n",
      "Train Epoch: 1 [278016/359035 (77%)]\tLoss: 0.921277\n",
      "Train Epoch: 1 [279296/359035 (78%)]\tLoss: 0.831752\n",
      "Train Epoch: 1 [280576/359035 (78%)]\tLoss: 0.792363\n",
      "Train Epoch: 1 [281856/359035 (79%)]\tLoss: 0.904011\n",
      "Train Epoch: 1 [283136/359035 (79%)]\tLoss: 0.922146\n",
      "Train Epoch: 1 [284416/359035 (79%)]\tLoss: 0.842023\n",
      "Train Epoch: 1 [285696/359035 (80%)]\tLoss: 0.756913\n",
      "Train Epoch: 1 [286976/359035 (80%)]\tLoss: 0.922773\n",
      "Train Epoch: 1 [288256/359035 (80%)]\tLoss: 1.019198\n",
      "Train Epoch: 1 [289536/359035 (81%)]\tLoss: 0.835894\n",
      "Train Epoch: 1 [290816/359035 (81%)]\tLoss: 0.883010\n",
      "Train Epoch: 1 [292096/359035 (81%)]\tLoss: 0.891497\n",
      "Train Epoch: 1 [293376/359035 (82%)]\tLoss: 0.898776\n",
      "Train Epoch: 1 [294656/359035 (82%)]\tLoss: 0.873104\n",
      "Train Epoch: 1 [295936/359035 (82%)]\tLoss: 0.831155\n",
      "Train Epoch: 1 [297216/359035 (83%)]\tLoss: 0.766280\n",
      "Train Epoch: 1 [298496/359035 (83%)]\tLoss: 0.903980\n",
      "Train Epoch: 1 [299776/359035 (83%)]\tLoss: 0.994572\n",
      "Train Epoch: 1 [301056/359035 (84%)]\tLoss: 1.000756\n",
      "Train Epoch: 1 [302336/359035 (84%)]\tLoss: 0.976525\n",
      "Train Epoch: 1 [303616/359035 (85%)]\tLoss: 0.854362\n",
      "Train Epoch: 1 [304896/359035 (85%)]\tLoss: 0.925715\n",
      "Train Epoch: 1 [306176/359035 (85%)]\tLoss: 0.950234\n",
      "Train Epoch: 1 [307456/359035 (86%)]\tLoss: 0.839760\n",
      "Train Epoch: 1 [308736/359035 (86%)]\tLoss: 0.885871\n",
      "Train Epoch: 1 [310016/359035 (86%)]\tLoss: 0.844400\n",
      "Train Epoch: 1 [311296/359035 (87%)]\tLoss: 0.940365\n",
      "Train Epoch: 1 [312576/359035 (87%)]\tLoss: 0.872396\n",
      "Train Epoch: 1 [313856/359035 (87%)]\tLoss: 0.827430\n",
      "Train Epoch: 1 [315136/359035 (88%)]\tLoss: 0.915870\n",
      "Train Epoch: 1 [316416/359035 (88%)]\tLoss: 0.839840\n",
      "Train Epoch: 1 [317696/359035 (88%)]\tLoss: 0.845821\n",
      "Train Epoch: 1 [318976/359035 (89%)]\tLoss: 0.830725\n",
      "Train Epoch: 1 [320256/359035 (89%)]\tLoss: 1.044346\n",
      "Train Epoch: 1 [321536/359035 (90%)]\tLoss: 0.904219\n",
      "Train Epoch: 1 [322816/359035 (90%)]\tLoss: 0.786547\n",
      "Train Epoch: 1 [324096/359035 (90%)]\tLoss: 0.839802\n",
      "Train Epoch: 1 [325376/359035 (91%)]\tLoss: 0.889959\n",
      "Train Epoch: 1 [326656/359035 (91%)]\tLoss: 0.912821\n",
      "Train Epoch: 1 [327936/359035 (91%)]\tLoss: 0.826178\n",
      "Train Epoch: 1 [329216/359035 (92%)]\tLoss: 0.884553\n",
      "Train Epoch: 1 [330496/359035 (92%)]\tLoss: 0.933721\n",
      "Train Epoch: 1 [331776/359035 (92%)]\tLoss: 0.816216\n",
      "Train Epoch: 1 [333056/359035 (93%)]\tLoss: 0.962165\n",
      "Train Epoch: 1 [334336/359035 (93%)]\tLoss: 0.826315\n",
      "Train Epoch: 1 [335616/359035 (93%)]\tLoss: 0.890475\n",
      "Train Epoch: 1 [336896/359035 (94%)]\tLoss: 0.936421\n",
      "Train Epoch: 1 [338176/359035 (94%)]\tLoss: 0.857625\n",
      "Train Epoch: 1 [339456/359035 (95%)]\tLoss: 0.924626\n",
      "Train Epoch: 1 [340736/359035 (95%)]\tLoss: 0.875533\n",
      "Train Epoch: 1 [342016/359035 (95%)]\tLoss: 0.828651\n",
      "Train Epoch: 1 [343296/359035 (96%)]\tLoss: 0.825326\n",
      "Train Epoch: 1 [344576/359035 (96%)]\tLoss: 0.889053\n",
      "Train Epoch: 1 [345856/359035 (96%)]\tLoss: 0.850359\n",
      "Train Epoch: 1 [347136/359035 (97%)]\tLoss: 0.864346\n",
      "Train Epoch: 1 [348416/359035 (97%)]\tLoss: 0.872954\n",
      "Train Epoch: 1 [349696/359035 (97%)]\tLoss: 0.857534\n",
      "Train Epoch: 1 [350976/359035 (98%)]\tLoss: 0.842330\n",
      "Train Epoch: 1 [352256/359035 (98%)]\tLoss: 0.929688\n",
      "Train Epoch: 1 [353536/359035 (98%)]\tLoss: 0.855234\n",
      "Train Epoch: 1 [354816/359035 (99%)]\tLoss: 0.916930\n",
      "Train Epoch: 1 [356096/359035 (99%)]\tLoss: 0.876365\n",
      "Train Epoch: 1 [357376/359035 (100%)]\tLoss: 0.850884\n",
      "Train Epoch: 1 [358656/359035 (100%)]\tLoss: 0.858655\n",
      "Performance on test set: Average loss: 0.9464, Accuracy: 133/200 (66%)\n",
      "Epoch took: 2570.973s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.6113, 100to500: 0.1566, 20to50: 0.1161, 500to_above: 0.1160\n",
      "\n",
      "Epoch 2\n",
      "Train Epoch: 2 [256/359035 (0%)]\tLoss: 0.954050\n",
      "Train Epoch: 2 [1536/359035 (0%)]\tLoss: 0.965641\n",
      "Train Epoch: 2 [2816/359035 (1%)]\tLoss: 0.792149\n",
      "Train Epoch: 2 [4096/359035 (1%)]\tLoss: 0.969412\n",
      "Train Epoch: 2 [5376/359035 (1%)]\tLoss: 0.915588\n",
      "Train Epoch: 2 [6656/359035 (2%)]\tLoss: 0.902994\n",
      "Train Epoch: 2 [7936/359035 (2%)]\tLoss: 0.906251\n",
      "Train Epoch: 2 [9216/359035 (3%)]\tLoss: 0.889480\n",
      "Train Epoch: 2 [10496/359035 (3%)]\tLoss: 0.857302\n",
      "Train Epoch: 2 [11776/359035 (3%)]\tLoss: 0.968380\n",
      "Train Epoch: 2 [13056/359035 (4%)]\tLoss: 0.775323\n",
      "Train Epoch: 2 [14336/359035 (4%)]\tLoss: 0.931224\n",
      "Train Epoch: 2 [15616/359035 (4%)]\tLoss: 0.846401\n",
      "Train Epoch: 2 [16896/359035 (5%)]\tLoss: 0.866829\n",
      "Train Epoch: 2 [18176/359035 (5%)]\tLoss: 0.948517\n",
      "Train Epoch: 2 [19456/359035 (5%)]\tLoss: 0.802914\n",
      "Train Epoch: 2 [20736/359035 (6%)]\tLoss: 0.800282\n",
      "Train Epoch: 2 [22016/359035 (6%)]\tLoss: 0.869941\n",
      "Train Epoch: 2 [23296/359035 (6%)]\tLoss: 0.934254\n",
      "Train Epoch: 2 [24576/359035 (7%)]\tLoss: 0.859249\n",
      "Train Epoch: 2 [25856/359035 (7%)]\tLoss: 0.910674\n",
      "Train Epoch: 2 [27136/359035 (8%)]\tLoss: 0.815403\n",
      "Train Epoch: 2 [28416/359035 (8%)]\tLoss: 0.891496\n",
      "Train Epoch: 2 [29696/359035 (8%)]\tLoss: 0.971285\n",
      "Train Epoch: 2 [30976/359035 (9%)]\tLoss: 0.944711\n",
      "Train Epoch: 2 [32256/359035 (9%)]\tLoss: 0.877097\n",
      "Train Epoch: 2 [33536/359035 (9%)]\tLoss: 0.872904\n",
      "Train Epoch: 2 [34816/359035 (10%)]\tLoss: 0.913302\n",
      "Train Epoch: 2 [36096/359035 (10%)]\tLoss: 0.862335\n",
      "Train Epoch: 2 [37376/359035 (10%)]\tLoss: 0.866857\n",
      "Train Epoch: 2 [38656/359035 (11%)]\tLoss: 0.812329\n",
      "Train Epoch: 2 [39936/359035 (11%)]\tLoss: 0.928791\n",
      "Train Epoch: 2 [41216/359035 (11%)]\tLoss: 0.934030\n",
      "Train Epoch: 2 [42496/359035 (12%)]\tLoss: 0.847784\n",
      "Train Epoch: 2 [43776/359035 (12%)]\tLoss: 0.820161\n",
      "Train Epoch: 2 [45056/359035 (13%)]\tLoss: 0.860878\n",
      "Train Epoch: 2 [46336/359035 (13%)]\tLoss: 0.896703\n",
      "Train Epoch: 2 [47616/359035 (13%)]\tLoss: 0.896494\n",
      "Train Epoch: 2 [48896/359035 (14%)]\tLoss: 0.894591\n",
      "Train Epoch: 2 [50176/359035 (14%)]\tLoss: 0.836461\n",
      "Train Epoch: 2 [51456/359035 (14%)]\tLoss: 0.887916\n",
      "Train Epoch: 2 [52736/359035 (15%)]\tLoss: 0.866244\n",
      "Train Epoch: 2 [54016/359035 (15%)]\tLoss: 0.865288\n",
      "Train Epoch: 2 [55296/359035 (15%)]\tLoss: 0.895741\n",
      "Train Epoch: 2 [56576/359035 (16%)]\tLoss: 0.892930\n",
      "Train Epoch: 2 [57856/359035 (16%)]\tLoss: 0.863118\n",
      "Train Epoch: 2 [59136/359035 (16%)]\tLoss: 0.931702\n",
      "Train Epoch: 2 [60416/359035 (17%)]\tLoss: 0.898677\n",
      "Train Epoch: 2 [61696/359035 (17%)]\tLoss: 0.770573\n",
      "Train Epoch: 2 [62976/359035 (18%)]\tLoss: 0.908760\n",
      "Train Epoch: 2 [64256/359035 (18%)]\tLoss: 0.869584\n",
      "Train Epoch: 2 [65536/359035 (18%)]\tLoss: 0.914661\n",
      "Train Epoch: 2 [66816/359035 (19%)]\tLoss: 0.897806\n",
      "Train Epoch: 2 [68096/359035 (19%)]\tLoss: 0.918798\n",
      "Train Epoch: 2 [69376/359035 (19%)]\tLoss: 0.946825\n",
      "Train Epoch: 2 [70656/359035 (20%)]\tLoss: 0.824991\n",
      "Train Epoch: 2 [71936/359035 (20%)]\tLoss: 1.004440\n",
      "Train Epoch: 2 [73216/359035 (20%)]\tLoss: 0.892307\n",
      "Train Epoch: 2 [74496/359035 (21%)]\tLoss: 0.874338\n",
      "Train Epoch: 2 [75776/359035 (21%)]\tLoss: 0.856735\n",
      "Train Epoch: 2 [77056/359035 (21%)]\tLoss: 0.757947\n",
      "Train Epoch: 2 [78336/359035 (22%)]\tLoss: 0.809936\n",
      "Train Epoch: 2 [79616/359035 (22%)]\tLoss: 0.842706\n",
      "Train Epoch: 2 [80896/359035 (23%)]\tLoss: 0.895952\n",
      "Train Epoch: 2 [82176/359035 (23%)]\tLoss: 0.829241\n",
      "Train Epoch: 2 [83456/359035 (23%)]\tLoss: 0.885974\n",
      "Train Epoch: 2 [84736/359035 (24%)]\tLoss: 0.833979\n",
      "Train Epoch: 2 [86016/359035 (24%)]\tLoss: 0.888875\n",
      "Train Epoch: 2 [87296/359035 (24%)]\tLoss: 0.831610\n",
      "Train Epoch: 2 [88576/359035 (25%)]\tLoss: 0.938160\n",
      "Train Epoch: 2 [89856/359035 (25%)]\tLoss: 0.787887\n",
      "Train Epoch: 2 [91136/359035 (25%)]\tLoss: 0.822781\n",
      "Train Epoch: 2 [92416/359035 (26%)]\tLoss: 0.823651\n",
      "Train Epoch: 2 [93696/359035 (26%)]\tLoss: 0.788354\n",
      "Train Epoch: 2 [94976/359035 (26%)]\tLoss: 0.875377\n",
      "Train Epoch: 2 [96256/359035 (27%)]\tLoss: 0.934824\n",
      "Train Epoch: 2 [97536/359035 (27%)]\tLoss: 0.924088\n",
      "Train Epoch: 2 [98816/359035 (28%)]\tLoss: 0.903377\n",
      "Train Epoch: 2 [100096/359035 (28%)]\tLoss: 0.856066\n",
      "Train Epoch: 2 [101376/359035 (28%)]\tLoss: 0.954198\n",
      "Train Epoch: 2 [102656/359035 (29%)]\tLoss: 0.910442\n",
      "Train Epoch: 2 [103936/359035 (29%)]\tLoss: 0.853419\n",
      "Train Epoch: 2 [105216/359035 (29%)]\tLoss: 0.849516\n",
      "Train Epoch: 2 [106496/359035 (30%)]\tLoss: 0.859641\n",
      "Train Epoch: 2 [107776/359035 (30%)]\tLoss: 0.887837\n",
      "Train Epoch: 2 [109056/359035 (30%)]\tLoss: 0.899491\n",
      "Train Epoch: 2 [110336/359035 (31%)]\tLoss: 0.861618\n",
      "Train Epoch: 2 [111616/359035 (31%)]\tLoss: 0.778032\n",
      "Train Epoch: 2 [112896/359035 (31%)]\tLoss: 0.799268\n",
      "Train Epoch: 2 [114176/359035 (32%)]\tLoss: 0.905517\n",
      "Train Epoch: 2 [115456/359035 (32%)]\tLoss: 0.887681\n",
      "Train Epoch: 2 [116736/359035 (33%)]\tLoss: 0.892128\n",
      "Train Epoch: 2 [118016/359035 (33%)]\tLoss: 0.870473\n",
      "Train Epoch: 2 [119296/359035 (33%)]\tLoss: 0.911040\n",
      "Train Epoch: 2 [120576/359035 (34%)]\tLoss: 0.883886\n",
      "Train Epoch: 2 [121856/359035 (34%)]\tLoss: 0.899636\n",
      "Train Epoch: 2 [123136/359035 (34%)]\tLoss: 0.832832\n",
      "Train Epoch: 2 [124416/359035 (35%)]\tLoss: 0.799201\n",
      "Train Epoch: 2 [125696/359035 (35%)]\tLoss: 0.860199\n",
      "Train Epoch: 2 [126976/359035 (35%)]\tLoss: 0.857857\n",
      "Train Epoch: 2 [128256/359035 (36%)]\tLoss: 0.931997\n",
      "Train Epoch: 2 [129536/359035 (36%)]\tLoss: 0.912793\n",
      "Train Epoch: 2 [130816/359035 (36%)]\tLoss: 0.836493\n",
      "Train Epoch: 2 [132096/359035 (37%)]\tLoss: 0.889385\n",
      "Train Epoch: 2 [133376/359035 (37%)]\tLoss: 0.817328\n",
      "Train Epoch: 2 [134656/359035 (38%)]\tLoss: 0.854361\n",
      "Train Epoch: 2 [135936/359035 (38%)]\tLoss: 0.929447\n",
      "Train Epoch: 2 [137216/359035 (38%)]\tLoss: 0.862130\n",
      "Train Epoch: 2 [138496/359035 (39%)]\tLoss: 0.872304\n",
      "Train Epoch: 2 [139776/359035 (39%)]\tLoss: 0.916900\n",
      "Train Epoch: 2 [141056/359035 (39%)]\tLoss: 0.802827\n",
      "Train Epoch: 2 [142336/359035 (40%)]\tLoss: 0.803950\n",
      "Train Epoch: 2 [143616/359035 (40%)]\tLoss: 0.815287\n",
      "Train Epoch: 2 [144896/359035 (40%)]\tLoss: 0.806718\n",
      "Train Epoch: 2 [146176/359035 (41%)]\tLoss: 0.851386\n",
      "Train Epoch: 2 [147456/359035 (41%)]\tLoss: 0.892145\n",
      "Train Epoch: 2 [148736/359035 (41%)]\tLoss: 0.935117\n",
      "Train Epoch: 2 [150016/359035 (42%)]\tLoss: 0.814981\n",
      "Train Epoch: 2 [151296/359035 (42%)]\tLoss: 0.960453\n",
      "Train Epoch: 2 [152576/359035 (42%)]\tLoss: 0.880438\n",
      "Train Epoch: 2 [153856/359035 (43%)]\tLoss: 0.887404\n",
      "Train Epoch: 2 [155136/359035 (43%)]\tLoss: 0.916017\n",
      "Train Epoch: 2 [156416/359035 (44%)]\tLoss: 0.874678\n",
      "Train Epoch: 2 [157696/359035 (44%)]\tLoss: 0.884787\n",
      "Train Epoch: 2 [158976/359035 (44%)]\tLoss: 0.918542\n",
      "Train Epoch: 2 [160256/359035 (45%)]\tLoss: 0.848787\n",
      "Train Epoch: 2 [161536/359035 (45%)]\tLoss: 0.831837\n",
      "Train Epoch: 2 [162816/359035 (45%)]\tLoss: 0.915045\n",
      "Train Epoch: 2 [164096/359035 (46%)]\tLoss: 0.861903\n",
      "Train Epoch: 2 [165376/359035 (46%)]\tLoss: 0.779861\n",
      "Train Epoch: 2 [166656/359035 (46%)]\tLoss: 0.839530\n",
      "Train Epoch: 2 [167936/359035 (47%)]\tLoss: 0.888019\n",
      "Train Epoch: 2 [169216/359035 (47%)]\tLoss: 0.848089\n",
      "Train Epoch: 2 [170496/359035 (47%)]\tLoss: 0.938962\n",
      "Train Epoch: 2 [171776/359035 (48%)]\tLoss: 0.849199\n",
      "Train Epoch: 2 [173056/359035 (48%)]\tLoss: 0.898999\n",
      "Train Epoch: 2 [174336/359035 (49%)]\tLoss: 0.908568\n",
      "Train Epoch: 2 [175616/359035 (49%)]\tLoss: 0.933980\n",
      "Train Epoch: 2 [176896/359035 (49%)]\tLoss: 0.972449\n",
      "Train Epoch: 2 [178176/359035 (50%)]\tLoss: 0.885087\n",
      "Train Epoch: 2 [179456/359035 (50%)]\tLoss: 0.874310\n",
      "Train Epoch: 2 [180736/359035 (50%)]\tLoss: 0.851944\n",
      "Train Epoch: 2 [182016/359035 (51%)]\tLoss: 0.888480\n",
      "Train Epoch: 2 [183296/359035 (51%)]\tLoss: 0.815893\n",
      "Train Epoch: 2 [184576/359035 (51%)]\tLoss: 0.990512\n",
      "Train Epoch: 2 [185856/359035 (52%)]\tLoss: 0.916745\n",
      "Train Epoch: 2 [187136/359035 (52%)]\tLoss: 0.922860\n",
      "Train Epoch: 2 [188416/359035 (52%)]\tLoss: 0.910450\n",
      "Train Epoch: 2 [189696/359035 (53%)]\tLoss: 0.853911\n",
      "Train Epoch: 2 [190976/359035 (53%)]\tLoss: 0.820211\n",
      "Train Epoch: 2 [192256/359035 (54%)]\tLoss: 0.856678\n",
      "Train Epoch: 2 [193536/359035 (54%)]\tLoss: 0.923093\n",
      "Train Epoch: 2 [194816/359035 (54%)]\tLoss: 0.841398\n",
      "Train Epoch: 2 [196096/359035 (55%)]\tLoss: 0.802854\n",
      "Train Epoch: 2 [197376/359035 (55%)]\tLoss: 0.846444\n",
      "Train Epoch: 2 [198656/359035 (55%)]\tLoss: 0.876515\n",
      "Train Epoch: 2 [199936/359035 (56%)]\tLoss: 0.825192\n",
      "Train Epoch: 2 [201216/359035 (56%)]\tLoss: 0.876864\n",
      "Train Epoch: 2 [202496/359035 (56%)]\tLoss: 0.862495\n",
      "Train Epoch: 2 [203776/359035 (57%)]\tLoss: 0.901675\n",
      "Train Epoch: 2 [205056/359035 (57%)]\tLoss: 0.843939\n",
      "Train Epoch: 2 [206336/359035 (57%)]\tLoss: 0.830046\n",
      "Train Epoch: 2 [207616/359035 (58%)]\tLoss: 0.828496\n",
      "Train Epoch: 2 [208896/359035 (58%)]\tLoss: 0.937115\n",
      "Train Epoch: 2 [210176/359035 (59%)]\tLoss: 0.800556\n",
      "Train Epoch: 2 [211456/359035 (59%)]\tLoss: 0.946726\n",
      "Train Epoch: 2 [212736/359035 (59%)]\tLoss: 0.786950\n",
      "Train Epoch: 2 [214016/359035 (60%)]\tLoss: 0.913392\n",
      "Train Epoch: 2 [215296/359035 (60%)]\tLoss: 0.972512\n",
      "Train Epoch: 2 [216576/359035 (60%)]\tLoss: 0.885612\n",
      "Train Epoch: 2 [217856/359035 (61%)]\tLoss: 0.953089\n",
      "Train Epoch: 2 [219136/359035 (61%)]\tLoss: 0.868848\n",
      "Train Epoch: 2 [220416/359035 (61%)]\tLoss: 0.906078\n",
      "Train Epoch: 2 [221696/359035 (62%)]\tLoss: 0.911710\n",
      "Train Epoch: 2 [222976/359035 (62%)]\tLoss: 0.846477\n",
      "Train Epoch: 2 [224256/359035 (62%)]\tLoss: 0.907461\n",
      "Train Epoch: 2 [225536/359035 (63%)]\tLoss: 0.869268\n",
      "Train Epoch: 2 [226816/359035 (63%)]\tLoss: 0.795102\n",
      "Train Epoch: 2 [228096/359035 (64%)]\tLoss: 0.936622\n",
      "Train Epoch: 2 [229376/359035 (64%)]\tLoss: 0.788753\n",
      "Train Epoch: 2 [230656/359035 (64%)]\tLoss: 0.800826\n",
      "Train Epoch: 2 [231936/359035 (65%)]\tLoss: 0.924428\n",
      "Train Epoch: 2 [233216/359035 (65%)]\tLoss: 0.897493\n",
      "Train Epoch: 2 [234496/359035 (65%)]\tLoss: 0.855108\n",
      "Train Epoch: 2 [235776/359035 (66%)]\tLoss: 0.839487\n",
      "Train Epoch: 2 [237056/359035 (66%)]\tLoss: 0.818073\n",
      "Train Epoch: 2 [238336/359035 (66%)]\tLoss: 0.925587\n",
      "Train Epoch: 2 [239616/359035 (67%)]\tLoss: 0.855927\n",
      "Train Epoch: 2 [240896/359035 (67%)]\tLoss: 0.929859\n",
      "Train Epoch: 2 [242176/359035 (67%)]\tLoss: 0.855842\n",
      "Train Epoch: 2 [243456/359035 (68%)]\tLoss: 0.955660\n",
      "Train Epoch: 2 [244736/359035 (68%)]\tLoss: 1.045808\n",
      "Train Epoch: 2 [246016/359035 (69%)]\tLoss: 0.750732\n",
      "Train Epoch: 2 [247296/359035 (69%)]\tLoss: 0.933782\n",
      "Train Epoch: 2 [248576/359035 (69%)]\tLoss: 0.886821\n",
      "Train Epoch: 2 [249856/359035 (70%)]\tLoss: 0.866161\n",
      "Train Epoch: 2 [251136/359035 (70%)]\tLoss: 0.907087\n",
      "Train Epoch: 2 [252416/359035 (70%)]\tLoss: 0.918449\n",
      "Train Epoch: 2 [253696/359035 (71%)]\tLoss: 0.857935\n",
      "Train Epoch: 2 [254976/359035 (71%)]\tLoss: 0.783386\n",
      "Train Epoch: 2 [256256/359035 (71%)]\tLoss: 0.938232\n",
      "Train Epoch: 2 [257536/359035 (72%)]\tLoss: 0.822136\n",
      "Train Epoch: 2 [258816/359035 (72%)]\tLoss: 0.835889\n",
      "Train Epoch: 2 [260096/359035 (72%)]\tLoss: 0.877077\n",
      "Train Epoch: 2 [261376/359035 (73%)]\tLoss: 0.828683\n",
      "Train Epoch: 2 [262656/359035 (73%)]\tLoss: 0.807699\n",
      "Train Epoch: 2 [263936/359035 (74%)]\tLoss: 0.895066\n",
      "Train Epoch: 2 [265216/359035 (74%)]\tLoss: 0.862100\n",
      "Train Epoch: 2 [266496/359035 (74%)]\tLoss: 0.927164\n",
      "Train Epoch: 2 [267776/359035 (75%)]\tLoss: 0.950906\n",
      "Train Epoch: 2 [269056/359035 (75%)]\tLoss: 0.916212\n",
      "Train Epoch: 2 [270336/359035 (75%)]\tLoss: 0.890195\n",
      "Train Epoch: 2 [271616/359035 (76%)]\tLoss: 0.887348\n",
      "Train Epoch: 2 [272896/359035 (76%)]\tLoss: 0.949561\n",
      "Train Epoch: 2 [274176/359035 (76%)]\tLoss: 0.942298\n",
      "Train Epoch: 2 [275456/359035 (77%)]\tLoss: 0.852722\n",
      "Train Epoch: 2 [276736/359035 (77%)]\tLoss: 0.894954\n",
      "Train Epoch: 2 [278016/359035 (77%)]\tLoss: 0.844317\n",
      "Train Epoch: 2 [279296/359035 (78%)]\tLoss: 0.841058\n",
      "Train Epoch: 2 [280576/359035 (78%)]\tLoss: 0.948087\n",
      "Train Epoch: 2 [281856/359035 (79%)]\tLoss: 0.907670\n",
      "Train Epoch: 2 [283136/359035 (79%)]\tLoss: 0.847936\n",
      "Train Epoch: 2 [284416/359035 (79%)]\tLoss: 0.839999\n",
      "Train Epoch: 2 [285696/359035 (80%)]\tLoss: 0.841270\n",
      "Train Epoch: 2 [286976/359035 (80%)]\tLoss: 0.900895\n",
      "Train Epoch: 2 [288256/359035 (80%)]\tLoss: 0.895306\n",
      "Train Epoch: 2 [289536/359035 (81%)]\tLoss: 0.862519\n",
      "Train Epoch: 2 [290816/359035 (81%)]\tLoss: 0.919995\n",
      "Train Epoch: 2 [292096/359035 (81%)]\tLoss: 0.873103\n",
      "Train Epoch: 2 [293376/359035 (82%)]\tLoss: 0.883917\n",
      "Train Epoch: 2 [294656/359035 (82%)]\tLoss: 0.850875\n",
      "Train Epoch: 2 [295936/359035 (82%)]\tLoss: 0.852301\n",
      "Train Epoch: 2 [297216/359035 (83%)]\tLoss: 0.928180\n",
      "Train Epoch: 2 [298496/359035 (83%)]\tLoss: 0.867653\n",
      "Train Epoch: 2 [299776/359035 (83%)]\tLoss: 0.917034\n",
      "Train Epoch: 2 [301056/359035 (84%)]\tLoss: 0.953478\n",
      "Train Epoch: 2 [302336/359035 (84%)]\tLoss: 0.859999\n",
      "Train Epoch: 2 [303616/359035 (85%)]\tLoss: 0.920361\n",
      "Train Epoch: 2 [304896/359035 (85%)]\tLoss: 0.882253\n",
      "Train Epoch: 2 [306176/359035 (85%)]\tLoss: 0.920362\n",
      "Train Epoch: 2 [307456/359035 (86%)]\tLoss: 0.921275\n",
      "Train Epoch: 2 [308736/359035 (86%)]\tLoss: 0.866825\n",
      "Train Epoch: 2 [310016/359035 (86%)]\tLoss: 0.918581\n",
      "Train Epoch: 2 [311296/359035 (87%)]\tLoss: 0.875315\n",
      "Train Epoch: 2 [312576/359035 (87%)]\tLoss: 0.880547\n",
      "Train Epoch: 2 [313856/359035 (87%)]\tLoss: 0.796343\n",
      "Train Epoch: 2 [315136/359035 (88%)]\tLoss: 0.859258\n",
      "Train Epoch: 2 [316416/359035 (88%)]\tLoss: 0.750890\n",
      "Train Epoch: 2 [317696/359035 (88%)]\tLoss: 0.950706\n",
      "Train Epoch: 2 [318976/359035 (89%)]\tLoss: 0.856070\n",
      "Train Epoch: 2 [320256/359035 (89%)]\tLoss: 0.836008\n",
      "Train Epoch: 2 [321536/359035 (90%)]\tLoss: 0.830629\n",
      "Train Epoch: 2 [322816/359035 (90%)]\tLoss: 0.830270\n",
      "Train Epoch: 2 [324096/359035 (90%)]\tLoss: 0.867405\n",
      "Train Epoch: 2 [325376/359035 (91%)]\tLoss: 0.845526\n",
      "Train Epoch: 2 [326656/359035 (91%)]\tLoss: 0.833095\n",
      "Train Epoch: 2 [327936/359035 (91%)]\tLoss: 0.867589\n",
      "Train Epoch: 2 [329216/359035 (92%)]\tLoss: 0.843958\n",
      "Train Epoch: 2 [330496/359035 (92%)]\tLoss: 0.894441\n",
      "Train Epoch: 2 [331776/359035 (92%)]\tLoss: 0.839640\n",
      "Train Epoch: 2 [333056/359035 (93%)]\tLoss: 0.793172\n",
      "Train Epoch: 2 [334336/359035 (93%)]\tLoss: 0.860994\n",
      "Train Epoch: 2 [335616/359035 (93%)]\tLoss: 0.905650\n",
      "Train Epoch: 2 [336896/359035 (94%)]\tLoss: 0.917273\n",
      "Train Epoch: 2 [338176/359035 (94%)]\tLoss: 0.777671\n",
      "Train Epoch: 2 [339456/359035 (95%)]\tLoss: 0.836641\n",
      "Train Epoch: 2 [340736/359035 (95%)]\tLoss: 0.820387\n",
      "Train Epoch: 2 [342016/359035 (95%)]\tLoss: 0.919681\n",
      "Train Epoch: 2 [343296/359035 (96%)]\tLoss: 0.886623\n",
      "Train Epoch: 2 [344576/359035 (96%)]\tLoss: 0.934481\n",
      "Train Epoch: 2 [345856/359035 (96%)]\tLoss: 0.847731\n",
      "Train Epoch: 2 [347136/359035 (97%)]\tLoss: 0.891829\n",
      "Train Epoch: 2 [348416/359035 (97%)]\tLoss: 0.962095\n",
      "Train Epoch: 2 [349696/359035 (97%)]\tLoss: 0.840292\n",
      "Train Epoch: 2 [350976/359035 (98%)]\tLoss: 0.921118\n",
      "Train Epoch: 2 [352256/359035 (98%)]\tLoss: 0.879361\n",
      "Train Epoch: 2 [353536/359035 (98%)]\tLoss: 0.899947\n",
      "Train Epoch: 2 [354816/359035 (99%)]\tLoss: 0.861749\n",
      "Train Epoch: 2 [356096/359035 (99%)]\tLoss: 0.818827\n",
      "Train Epoch: 2 [357376/359035 (100%)]\tLoss: 0.893197\n",
      "Train Epoch: 2 [358656/359035 (100%)]\tLoss: 0.888799\n",
      "Performance on test set: Average loss: 0.9341, Accuracy: 134/200 (67%)\n",
      "Epoch took: 2576.052s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.5164, 100to500: 0.1878, 20to50: 0.1069, 500to_above: 0.1889\n",
      "\n",
      "Epoch 3\n",
      "Train Epoch: 3 [256/359035 (0%)]\tLoss: 0.819266\n",
      "Train Epoch: 3 [1536/359035 (0%)]\tLoss: 0.917842\n",
      "Train Epoch: 3 [2816/359035 (1%)]\tLoss: 0.856159\n",
      "Train Epoch: 3 [4096/359035 (1%)]\tLoss: 0.955883\n",
      "Train Epoch: 3 [5376/359035 (1%)]\tLoss: 0.826579\n",
      "Train Epoch: 3 [6656/359035 (2%)]\tLoss: 0.863941\n",
      "Train Epoch: 3 [7936/359035 (2%)]\tLoss: 0.917267\n",
      "Train Epoch: 3 [9216/359035 (3%)]\tLoss: 0.849401\n",
      "Train Epoch: 3 [10496/359035 (3%)]\tLoss: 0.820099\n",
      "Train Epoch: 3 [11776/359035 (3%)]\tLoss: 0.857524\n",
      "Train Epoch: 3 [13056/359035 (4%)]\tLoss: 0.856574\n",
      "Train Epoch: 3 [14336/359035 (4%)]\tLoss: 0.909408\n",
      "Train Epoch: 3 [15616/359035 (4%)]\tLoss: 0.710530\n",
      "Train Epoch: 3 [16896/359035 (5%)]\tLoss: 0.853737\n",
      "Train Epoch: 3 [18176/359035 (5%)]\tLoss: 0.821878\n",
      "Train Epoch: 3 [19456/359035 (5%)]\tLoss: 0.952631\n",
      "Train Epoch: 3 [20736/359035 (6%)]\tLoss: 0.885574\n",
      "Train Epoch: 3 [22016/359035 (6%)]\tLoss: 0.861014\n",
      "Train Epoch: 3 [23296/359035 (6%)]\tLoss: 0.982991\n",
      "Train Epoch: 3 [24576/359035 (7%)]\tLoss: 0.878668\n",
      "Train Epoch: 3 [25856/359035 (7%)]\tLoss: 0.888139\n",
      "Train Epoch: 3 [27136/359035 (8%)]\tLoss: 0.834010\n",
      "Train Epoch: 3 [28416/359035 (8%)]\tLoss: 0.865441\n",
      "Train Epoch: 3 [29696/359035 (8%)]\tLoss: 0.844089\n",
      "Train Epoch: 3 [30976/359035 (9%)]\tLoss: 0.944035\n",
      "Train Epoch: 3 [32256/359035 (9%)]\tLoss: 0.872525\n",
      "Train Epoch: 3 [33536/359035 (9%)]\tLoss: 0.846371\n",
      "Train Epoch: 3 [34816/359035 (10%)]\tLoss: 0.813671\n",
      "Train Epoch: 3 [36096/359035 (10%)]\tLoss: 0.899369\n",
      "Train Epoch: 3 [37376/359035 (10%)]\tLoss: 0.912237\n",
      "Train Epoch: 3 [38656/359035 (11%)]\tLoss: 0.854931\n",
      "Train Epoch: 3 [39936/359035 (11%)]\tLoss: 0.828422\n",
      "Train Epoch: 3 [41216/359035 (11%)]\tLoss: 0.893855\n",
      "Train Epoch: 3 [42496/359035 (12%)]\tLoss: 0.883363\n",
      "Train Epoch: 3 [43776/359035 (12%)]\tLoss: 0.881546\n",
      "Train Epoch: 3 [45056/359035 (13%)]\tLoss: 0.797342\n",
      "Train Epoch: 3 [46336/359035 (13%)]\tLoss: 0.788385\n",
      "Train Epoch: 3 [47616/359035 (13%)]\tLoss: 0.728322\n",
      "Train Epoch: 3 [48896/359035 (14%)]\tLoss: 0.837685\n",
      "Train Epoch: 3 [50176/359035 (14%)]\tLoss: 0.805516\n",
      "Train Epoch: 3 [51456/359035 (14%)]\tLoss: 0.900483\n",
      "Train Epoch: 3 [52736/359035 (15%)]\tLoss: 0.767783\n",
      "Train Epoch: 3 [54016/359035 (15%)]\tLoss: 0.971961\n",
      "Train Epoch: 3 [55296/359035 (15%)]\tLoss: 0.809529\n",
      "Train Epoch: 3 [56576/359035 (16%)]\tLoss: 0.750239\n",
      "Train Epoch: 3 [57856/359035 (16%)]\tLoss: 0.888388\n",
      "Train Epoch: 3 [59136/359035 (16%)]\tLoss: 0.796849\n",
      "Train Epoch: 3 [60416/359035 (17%)]\tLoss: 0.882482\n",
      "Train Epoch: 3 [61696/359035 (17%)]\tLoss: 0.826715\n",
      "Train Epoch: 3 [62976/359035 (18%)]\tLoss: 0.900139\n",
      "Train Epoch: 3 [64256/359035 (18%)]\tLoss: 0.829970\n",
      "Train Epoch: 3 [65536/359035 (18%)]\tLoss: 1.021051\n",
      "Train Epoch: 3 [66816/359035 (19%)]\tLoss: 0.905788\n",
      "Train Epoch: 3 [68096/359035 (19%)]\tLoss: 0.850212\n",
      "Train Epoch: 3 [69376/359035 (19%)]\tLoss: 0.960960\n",
      "Train Epoch: 3 [70656/359035 (20%)]\tLoss: 0.853257\n",
      "Train Epoch: 3 [71936/359035 (20%)]\tLoss: 0.857960\n",
      "Train Epoch: 3 [73216/359035 (20%)]\tLoss: 0.913846\n",
      "Train Epoch: 3 [74496/359035 (21%)]\tLoss: 0.934723\n",
      "Train Epoch: 3 [75776/359035 (21%)]\tLoss: 0.867864\n",
      "Train Epoch: 3 [77056/359035 (21%)]\tLoss: 0.798984\n",
      "Train Epoch: 3 [78336/359035 (22%)]\tLoss: 0.918530\n",
      "Train Epoch: 3 [79616/359035 (22%)]\tLoss: 0.958446\n",
      "Train Epoch: 3 [80896/359035 (23%)]\tLoss: 0.949997\n",
      "Train Epoch: 3 [82176/359035 (23%)]\tLoss: 0.773857\n",
      "Train Epoch: 3 [83456/359035 (23%)]\tLoss: 0.922128\n",
      "Train Epoch: 3 [84736/359035 (24%)]\tLoss: 0.852803\n",
      "Train Epoch: 3 [86016/359035 (24%)]\tLoss: 0.870249\n",
      "Train Epoch: 3 [87296/359035 (24%)]\tLoss: 0.914249\n",
      "Train Epoch: 3 [88576/359035 (25%)]\tLoss: 0.814527\n",
      "Train Epoch: 3 [89856/359035 (25%)]\tLoss: 0.813988\n",
      "Train Epoch: 3 [91136/359035 (25%)]\tLoss: 0.960376\n",
      "Train Epoch: 3 [92416/359035 (26%)]\tLoss: 0.849709\n",
      "Train Epoch: 3 [93696/359035 (26%)]\tLoss: 0.907990\n",
      "Train Epoch: 3 [94976/359035 (26%)]\tLoss: 0.873825\n",
      "Train Epoch: 3 [96256/359035 (27%)]\tLoss: 0.815991\n",
      "Train Epoch: 3 [97536/359035 (27%)]\tLoss: 0.932821\n",
      "Train Epoch: 3 [98816/359035 (28%)]\tLoss: 0.865635\n",
      "Train Epoch: 3 [100096/359035 (28%)]\tLoss: 0.869304\n",
      "Train Epoch: 3 [101376/359035 (28%)]\tLoss: 0.857863\n",
      "Train Epoch: 3 [102656/359035 (29%)]\tLoss: 0.810571\n",
      "Train Epoch: 3 [103936/359035 (29%)]\tLoss: 0.859709\n",
      "Train Epoch: 3 [105216/359035 (29%)]\tLoss: 0.838506\n",
      "Train Epoch: 3 [106496/359035 (30%)]\tLoss: 0.928047\n",
      "Train Epoch: 3 [107776/359035 (30%)]\tLoss: 0.871785\n",
      "Train Epoch: 3 [109056/359035 (30%)]\tLoss: 0.796840\n",
      "Train Epoch: 3 [110336/359035 (31%)]\tLoss: 0.858566\n",
      "Train Epoch: 3 [111616/359035 (31%)]\tLoss: 0.867585\n",
      "Train Epoch: 3 [112896/359035 (31%)]\tLoss: 0.914975\n",
      "Train Epoch: 3 [114176/359035 (32%)]\tLoss: 0.816711\n",
      "Train Epoch: 3 [115456/359035 (32%)]\tLoss: 0.792104\n",
      "Train Epoch: 3 [116736/359035 (33%)]\tLoss: 0.905451\n",
      "Train Epoch: 3 [118016/359035 (33%)]\tLoss: 0.847066\n",
      "Train Epoch: 3 [119296/359035 (33%)]\tLoss: 0.840547\n",
      "Train Epoch: 3 [120576/359035 (34%)]\tLoss: 0.790381\n",
      "Train Epoch: 3 [121856/359035 (34%)]\tLoss: 0.838607\n",
      "Train Epoch: 3 [123136/359035 (34%)]\tLoss: 0.899228\n",
      "Train Epoch: 3 [124416/359035 (35%)]\tLoss: 0.842971\n",
      "Train Epoch: 3 [125696/359035 (35%)]\tLoss: 0.868478\n",
      "Train Epoch: 3 [126976/359035 (35%)]\tLoss: 0.900569\n",
      "Train Epoch: 3 [128256/359035 (36%)]\tLoss: 0.902569\n",
      "Train Epoch: 3 [129536/359035 (36%)]\tLoss: 0.933861\n",
      "Train Epoch: 3 [130816/359035 (36%)]\tLoss: 0.951857\n",
      "Train Epoch: 3 [132096/359035 (37%)]\tLoss: 0.922734\n",
      "Train Epoch: 3 [133376/359035 (37%)]\tLoss: 0.830469\n",
      "Train Epoch: 3 [134656/359035 (38%)]\tLoss: 0.969923\n",
      "Train Epoch: 3 [135936/359035 (38%)]\tLoss: 0.927228\n",
      "Train Epoch: 3 [137216/359035 (38%)]\tLoss: 0.823076\n",
      "Train Epoch: 3 [138496/359035 (39%)]\tLoss: 0.774622\n",
      "Train Epoch: 3 [139776/359035 (39%)]\tLoss: 0.812639\n",
      "Train Epoch: 3 [141056/359035 (39%)]\tLoss: 0.759173\n",
      "Train Epoch: 3 [142336/359035 (40%)]\tLoss: 0.933740\n",
      "Train Epoch: 3 [143616/359035 (40%)]\tLoss: 0.892150\n",
      "Train Epoch: 3 [144896/359035 (40%)]\tLoss: 0.928411\n",
      "Train Epoch: 3 [146176/359035 (41%)]\tLoss: 1.007972\n",
      "Train Epoch: 3 [147456/359035 (41%)]\tLoss: 0.879044\n",
      "Train Epoch: 3 [148736/359035 (41%)]\tLoss: 0.836656\n",
      "Train Epoch: 3 [150016/359035 (42%)]\tLoss: 0.802381\n",
      "Train Epoch: 3 [151296/359035 (42%)]\tLoss: 0.851428\n",
      "Train Epoch: 3 [152576/359035 (42%)]\tLoss: 0.852059\n",
      "Train Epoch: 3 [153856/359035 (43%)]\tLoss: 0.759174\n",
      "Train Epoch: 3 [155136/359035 (43%)]\tLoss: 0.957893\n",
      "Train Epoch: 3 [156416/359035 (44%)]\tLoss: 0.916781\n",
      "Train Epoch: 3 [157696/359035 (44%)]\tLoss: 0.876342\n",
      "Train Epoch: 3 [158976/359035 (44%)]\tLoss: 0.901881\n",
      "Train Epoch: 3 [160256/359035 (45%)]\tLoss: 0.932879\n",
      "Train Epoch: 3 [161536/359035 (45%)]\tLoss: 0.840869\n",
      "Train Epoch: 3 [162816/359035 (45%)]\tLoss: 0.780429\n",
      "Train Epoch: 3 [164096/359035 (46%)]\tLoss: 0.868851\n",
      "Train Epoch: 3 [165376/359035 (46%)]\tLoss: 0.921560\n",
      "Train Epoch: 3 [166656/359035 (46%)]\tLoss: 0.886791\n",
      "Train Epoch: 3 [167936/359035 (47%)]\tLoss: 0.858065\n",
      "Train Epoch: 3 [169216/359035 (47%)]\tLoss: 0.808355\n",
      "Train Epoch: 3 [170496/359035 (47%)]\tLoss: 0.821140\n",
      "Train Epoch: 3 [171776/359035 (48%)]\tLoss: 0.882533\n",
      "Train Epoch: 3 [173056/359035 (48%)]\tLoss: 0.986394\n",
      "Train Epoch: 3 [174336/359035 (49%)]\tLoss: 0.902671\n",
      "Train Epoch: 3 [175616/359035 (49%)]\tLoss: 0.869865\n",
      "Train Epoch: 3 [176896/359035 (49%)]\tLoss: 0.815281\n",
      "Train Epoch: 3 [178176/359035 (50%)]\tLoss: 0.778607\n",
      "Train Epoch: 3 [179456/359035 (50%)]\tLoss: 0.922671\n",
      "Train Epoch: 3 [180736/359035 (50%)]\tLoss: 0.779351\n",
      "Train Epoch: 3 [182016/359035 (51%)]\tLoss: 0.923775\n",
      "Train Epoch: 3 [183296/359035 (51%)]\tLoss: 0.882748\n",
      "Train Epoch: 3 [184576/359035 (51%)]\tLoss: 0.856545\n",
      "Train Epoch: 3 [185856/359035 (52%)]\tLoss: 0.847882\n",
      "Train Epoch: 3 [187136/359035 (52%)]\tLoss: 0.920403\n",
      "Train Epoch: 3 [188416/359035 (52%)]\tLoss: 0.881208\n",
      "Train Epoch: 3 [189696/359035 (53%)]\tLoss: 0.892896\n",
      "Train Epoch: 3 [190976/359035 (53%)]\tLoss: 0.851132\n",
      "Train Epoch: 3 [192256/359035 (54%)]\tLoss: 0.920708\n",
      "Train Epoch: 3 [193536/359035 (54%)]\tLoss: 0.869984\n",
      "Train Epoch: 3 [194816/359035 (54%)]\tLoss: 0.958715\n",
      "Train Epoch: 3 [196096/359035 (55%)]\tLoss: 0.839408\n",
      "Train Epoch: 3 [197376/359035 (55%)]\tLoss: 0.881132\n",
      "Train Epoch: 3 [198656/359035 (55%)]\tLoss: 0.961336\n",
      "Train Epoch: 3 [199936/359035 (56%)]\tLoss: 0.845494\n",
      "Train Epoch: 3 [201216/359035 (56%)]\tLoss: 0.879157\n",
      "Train Epoch: 3 [202496/359035 (56%)]\tLoss: 0.913258\n",
      "Train Epoch: 3 [203776/359035 (57%)]\tLoss: 0.976699\n",
      "Train Epoch: 3 [205056/359035 (57%)]\tLoss: 0.874980\n",
      "Train Epoch: 3 [206336/359035 (57%)]\tLoss: 0.842284\n",
      "Train Epoch: 3 [207616/359035 (58%)]\tLoss: 0.874151\n",
      "Train Epoch: 3 [208896/359035 (58%)]\tLoss: 0.947292\n",
      "Train Epoch: 3 [210176/359035 (59%)]\tLoss: 0.868220\n",
      "Train Epoch: 3 [211456/359035 (59%)]\tLoss: 0.950030\n",
      "Train Epoch: 3 [212736/359035 (59%)]\tLoss: 0.866949\n",
      "Train Epoch: 3 [214016/359035 (60%)]\tLoss: 0.853721\n",
      "Train Epoch: 3 [215296/359035 (60%)]\tLoss: 0.862917\n",
      "Train Epoch: 3 [216576/359035 (60%)]\tLoss: 0.933596\n",
      "Train Epoch: 3 [217856/359035 (61%)]\tLoss: 0.816784\n",
      "Train Epoch: 3 [219136/359035 (61%)]\tLoss: 0.856685\n",
      "Train Epoch: 3 [220416/359035 (61%)]\tLoss: 0.914044\n",
      "Train Epoch: 3 [221696/359035 (62%)]\tLoss: 0.843886\n",
      "Train Epoch: 3 [222976/359035 (62%)]\tLoss: 0.996563\n",
      "Train Epoch: 3 [224256/359035 (62%)]\tLoss: 0.911472\n",
      "Train Epoch: 3 [225536/359035 (63%)]\tLoss: 0.831875\n",
      "Train Epoch: 3 [226816/359035 (63%)]\tLoss: 0.853786\n",
      "Train Epoch: 3 [228096/359035 (64%)]\tLoss: 0.894803\n",
      "Train Epoch: 3 [229376/359035 (64%)]\tLoss: 0.967491\n",
      "Train Epoch: 3 [230656/359035 (64%)]\tLoss: 0.899765\n",
      "Train Epoch: 3 [231936/359035 (65%)]\tLoss: 0.885035\n",
      "Train Epoch: 3 [233216/359035 (65%)]\tLoss: 0.950018\n",
      "Train Epoch: 3 [234496/359035 (65%)]\tLoss: 0.876814\n",
      "Train Epoch: 3 [235776/359035 (66%)]\tLoss: 0.949211\n",
      "Train Epoch: 3 [237056/359035 (66%)]\tLoss: 0.896956\n",
      "Train Epoch: 3 [238336/359035 (66%)]\tLoss: 0.893217\n",
      "Train Epoch: 3 [239616/359035 (67%)]\tLoss: 0.854273\n",
      "Train Epoch: 3 [240896/359035 (67%)]\tLoss: 0.829878\n",
      "Train Epoch: 3 [242176/359035 (67%)]\tLoss: 0.862112\n",
      "Train Epoch: 3 [243456/359035 (68%)]\tLoss: 0.827192\n",
      "Train Epoch: 3 [244736/359035 (68%)]\tLoss: 0.952398\n",
      "Train Epoch: 3 [246016/359035 (69%)]\tLoss: 0.891094\n",
      "Train Epoch: 3 [247296/359035 (69%)]\tLoss: 0.866780\n",
      "Train Epoch: 3 [248576/359035 (69%)]\tLoss: 0.882172\n",
      "Train Epoch: 3 [249856/359035 (70%)]\tLoss: 1.045647\n",
      "Train Epoch: 3 [251136/359035 (70%)]\tLoss: 0.864180\n",
      "Train Epoch: 3 [252416/359035 (70%)]\tLoss: 0.884038\n",
      "Train Epoch: 3 [253696/359035 (71%)]\tLoss: 0.878620\n",
      "Train Epoch: 3 [254976/359035 (71%)]\tLoss: 0.945154\n",
      "Train Epoch: 3 [256256/359035 (71%)]\tLoss: 0.845801\n",
      "Train Epoch: 3 [257536/359035 (72%)]\tLoss: 0.954095\n",
      "Train Epoch: 3 [258816/359035 (72%)]\tLoss: 0.847314\n",
      "Train Epoch: 3 [260096/359035 (72%)]\tLoss: 0.790101\n",
      "Train Epoch: 3 [261376/359035 (73%)]\tLoss: 0.850235\n",
      "Train Epoch: 3 [262656/359035 (73%)]\tLoss: 0.837896\n",
      "Train Epoch: 3 [263936/359035 (74%)]\tLoss: 0.940348\n",
      "Train Epoch: 3 [265216/359035 (74%)]\tLoss: 0.885217\n",
      "Train Epoch: 3 [266496/359035 (74%)]\tLoss: 0.906473\n",
      "Train Epoch: 3 [267776/359035 (75%)]\tLoss: 0.864488\n",
      "Train Epoch: 3 [269056/359035 (75%)]\tLoss: 0.793718\n",
      "Train Epoch: 3 [270336/359035 (75%)]\tLoss: 0.816997\n",
      "Train Epoch: 3 [271616/359035 (76%)]\tLoss: 0.931307\n",
      "Train Epoch: 3 [272896/359035 (76%)]\tLoss: 0.879917\n",
      "Train Epoch: 3 [274176/359035 (76%)]\tLoss: 0.896726\n",
      "Train Epoch: 3 [275456/359035 (77%)]\tLoss: 0.807408\n",
      "Train Epoch: 3 [276736/359035 (77%)]\tLoss: 0.803975\n",
      "Train Epoch: 3 [278016/359035 (77%)]\tLoss: 0.937242\n",
      "Train Epoch: 3 [279296/359035 (78%)]\tLoss: 0.873771\n",
      "Train Epoch: 3 [280576/359035 (78%)]\tLoss: 0.924718\n",
      "Train Epoch: 3 [281856/359035 (79%)]\tLoss: 0.941875\n",
      "Train Epoch: 3 [283136/359035 (79%)]\tLoss: 0.899852\n",
      "Train Epoch: 3 [284416/359035 (79%)]\tLoss: 0.907470\n",
      "Train Epoch: 3 [285696/359035 (80%)]\tLoss: 0.929350\n",
      "Train Epoch: 3 [286976/359035 (80%)]\tLoss: 0.905867\n",
      "Train Epoch: 3 [288256/359035 (80%)]\tLoss: 0.824674\n",
      "Train Epoch: 3 [289536/359035 (81%)]\tLoss: 0.833025\n",
      "Train Epoch: 3 [290816/359035 (81%)]\tLoss: 0.827694\n",
      "Train Epoch: 3 [292096/359035 (81%)]\tLoss: 0.850355\n",
      "Train Epoch: 3 [293376/359035 (82%)]\tLoss: 0.945367\n",
      "Train Epoch: 3 [294656/359035 (82%)]\tLoss: 0.923884\n",
      "Train Epoch: 3 [295936/359035 (82%)]\tLoss: 0.899095\n",
      "Train Epoch: 3 [297216/359035 (83%)]\tLoss: 0.827654\n",
      "Train Epoch: 3 [298496/359035 (83%)]\tLoss: 0.852644\n",
      "Train Epoch: 3 [299776/359035 (83%)]\tLoss: 0.892987\n",
      "Train Epoch: 3 [301056/359035 (84%)]\tLoss: 1.006473\n",
      "Train Epoch: 3 [302336/359035 (84%)]\tLoss: 0.869534\n",
      "Train Epoch: 3 [303616/359035 (85%)]\tLoss: 0.925609\n",
      "Train Epoch: 3 [304896/359035 (85%)]\tLoss: 0.810990\n",
      "Train Epoch: 3 [306176/359035 (85%)]\tLoss: 0.823842\n",
      "Train Epoch: 3 [307456/359035 (86%)]\tLoss: 0.841392\n",
      "Train Epoch: 3 [308736/359035 (86%)]\tLoss: 0.749190\n",
      "Train Epoch: 3 [310016/359035 (86%)]\tLoss: 0.835815\n",
      "Train Epoch: 3 [311296/359035 (87%)]\tLoss: 0.856330\n",
      "Train Epoch: 3 [312576/359035 (87%)]\tLoss: 0.946256\n",
      "Train Epoch: 3 [313856/359035 (87%)]\tLoss: 0.891530\n",
      "Train Epoch: 3 [315136/359035 (88%)]\tLoss: 0.888375\n",
      "Train Epoch: 3 [316416/359035 (88%)]\tLoss: 0.930806\n",
      "Train Epoch: 3 [317696/359035 (88%)]\tLoss: 0.857801\n",
      "Train Epoch: 3 [318976/359035 (89%)]\tLoss: 0.942104\n",
      "Train Epoch: 3 [320256/359035 (89%)]\tLoss: 0.784622\n",
      "Train Epoch: 3 [321536/359035 (90%)]\tLoss: 0.907420\n",
      "Train Epoch: 3 [322816/359035 (90%)]\tLoss: 0.828563\n",
      "Train Epoch: 3 [324096/359035 (90%)]\tLoss: 0.811561\n",
      "Train Epoch: 3 [325376/359035 (91%)]\tLoss: 0.826551\n",
      "Train Epoch: 3 [326656/359035 (91%)]\tLoss: 0.812025\n",
      "Train Epoch: 3 [327936/359035 (91%)]\tLoss: 0.843932\n",
      "Train Epoch: 3 [329216/359035 (92%)]\tLoss: 0.868458\n",
      "Train Epoch: 3 [330496/359035 (92%)]\tLoss: 0.858905\n",
      "Train Epoch: 3 [331776/359035 (92%)]\tLoss: 0.893774\n",
      "Train Epoch: 3 [333056/359035 (93%)]\tLoss: 1.048574\n",
      "Train Epoch: 3 [334336/359035 (93%)]\tLoss: 0.869583\n",
      "Train Epoch: 3 [335616/359035 (93%)]\tLoss: 0.830411\n",
      "Train Epoch: 3 [336896/359035 (94%)]\tLoss: 0.825576\n",
      "Train Epoch: 3 [338176/359035 (94%)]\tLoss: 0.893785\n",
      "Train Epoch: 3 [339456/359035 (95%)]\tLoss: 0.827555\n",
      "Train Epoch: 3 [340736/359035 (95%)]\tLoss: 0.895029\n",
      "Train Epoch: 3 [342016/359035 (95%)]\tLoss: 0.907641\n",
      "Train Epoch: 3 [343296/359035 (96%)]\tLoss: 0.705967\n",
      "Train Epoch: 3 [344576/359035 (96%)]\tLoss: 0.930149\n",
      "Train Epoch: 3 [345856/359035 (96%)]\tLoss: 0.861632\n",
      "Train Epoch: 3 [347136/359035 (97%)]\tLoss: 0.843155\n",
      "Train Epoch: 3 [348416/359035 (97%)]\tLoss: 0.892933\n",
      "Train Epoch: 3 [349696/359035 (97%)]\tLoss: 0.935860\n",
      "Train Epoch: 3 [350976/359035 (98%)]\tLoss: 0.857034\n",
      "Train Epoch: 3 [352256/359035 (98%)]\tLoss: 0.883441\n",
      "Train Epoch: 3 [353536/359035 (98%)]\tLoss: 0.878498\n",
      "Train Epoch: 3 [354816/359035 (99%)]\tLoss: 0.839043\n",
      "Train Epoch: 3 [356096/359035 (99%)]\tLoss: 0.895603\n",
      "Train Epoch: 3 [357376/359035 (100%)]\tLoss: 0.829755\n",
      "Train Epoch: 3 [358656/359035 (100%)]\tLoss: 0.890634\n",
      "Performance on test set: Average loss: 0.9324, Accuracy: 134/200 (67%)\n",
      "Epoch took: 2572.248s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.5684, 100to500: 0.1705, 20to50: 0.0981, 500to_above: 0.1630\n",
      "\n",
      "Epoch 4\n",
      "Train Epoch: 4 [256/359035 (0%)]\tLoss: 0.942602\n",
      "Train Epoch: 4 [1536/359035 (0%)]\tLoss: 0.883207\n",
      "Train Epoch: 4 [2816/359035 (1%)]\tLoss: 0.806010\n",
      "Train Epoch: 4 [4096/359035 (1%)]\tLoss: 0.831487\n",
      "Train Epoch: 4 [5376/359035 (1%)]\tLoss: 0.797547\n",
      "Train Epoch: 4 [6656/359035 (2%)]\tLoss: 0.869421\n",
      "Train Epoch: 4 [7936/359035 (2%)]\tLoss: 0.818725\n",
      "Train Epoch: 4 [9216/359035 (3%)]\tLoss: 0.856634\n",
      "Train Epoch: 4 [10496/359035 (3%)]\tLoss: 0.888185\n",
      "Train Epoch: 4 [11776/359035 (3%)]\tLoss: 0.767330\n",
      "Train Epoch: 4 [13056/359035 (4%)]\tLoss: 0.891333\n",
      "Train Epoch: 4 [14336/359035 (4%)]\tLoss: 0.843966\n",
      "Train Epoch: 4 [15616/359035 (4%)]\tLoss: 0.896078\n",
      "Train Epoch: 4 [16896/359035 (5%)]\tLoss: 0.850468\n",
      "Train Epoch: 4 [18176/359035 (5%)]\tLoss: 0.836777\n",
      "Train Epoch: 4 [19456/359035 (5%)]\tLoss: 0.835209\n",
      "Train Epoch: 4 [20736/359035 (6%)]\tLoss: 0.926359\n",
      "Train Epoch: 4 [22016/359035 (6%)]\tLoss: 0.802915\n",
      "Train Epoch: 4 [23296/359035 (6%)]\tLoss: 0.942341\n",
      "Train Epoch: 4 [24576/359035 (7%)]\tLoss: 0.855537\n",
      "Train Epoch: 4 [25856/359035 (7%)]\tLoss: 0.790042\n",
      "Train Epoch: 4 [27136/359035 (8%)]\tLoss: 1.028623\n",
      "Train Epoch: 4 [28416/359035 (8%)]\tLoss: 0.805143\n",
      "Train Epoch: 4 [29696/359035 (8%)]\tLoss: 0.933718\n",
      "Train Epoch: 4 [30976/359035 (9%)]\tLoss: 0.849051\n",
      "Train Epoch: 4 [32256/359035 (9%)]\tLoss: 0.809732\n",
      "Train Epoch: 4 [33536/359035 (9%)]\tLoss: 0.818175\n",
      "Train Epoch: 4 [34816/359035 (10%)]\tLoss: 0.919676\n",
      "Train Epoch: 4 [36096/359035 (10%)]\tLoss: 0.845816\n",
      "Train Epoch: 4 [37376/359035 (10%)]\tLoss: 0.909881\n",
      "Train Epoch: 4 [38656/359035 (11%)]\tLoss: 0.825731\n",
      "Train Epoch: 4 [39936/359035 (11%)]\tLoss: 0.923494\n",
      "Train Epoch: 4 [41216/359035 (11%)]\tLoss: 0.914077\n",
      "Train Epoch: 4 [42496/359035 (12%)]\tLoss: 0.896199\n",
      "Train Epoch: 4 [43776/359035 (12%)]\tLoss: 0.848581\n",
      "Train Epoch: 4 [45056/359035 (13%)]\tLoss: 0.846591\n",
      "Train Epoch: 4 [46336/359035 (13%)]\tLoss: 0.902258\n",
      "Train Epoch: 4 [47616/359035 (13%)]\tLoss: 0.853471\n",
      "Train Epoch: 4 [48896/359035 (14%)]\tLoss: 1.007440\n",
      "Train Epoch: 4 [50176/359035 (14%)]\tLoss: 0.957342\n",
      "Train Epoch: 4 [51456/359035 (14%)]\tLoss: 0.913848\n",
      "Train Epoch: 4 [52736/359035 (15%)]\tLoss: 0.966011\n",
      "Train Epoch: 4 [54016/359035 (15%)]\tLoss: 0.755534\n",
      "Train Epoch: 4 [55296/359035 (15%)]\tLoss: 0.899786\n",
      "Train Epoch: 4 [56576/359035 (16%)]\tLoss: 0.886052\n",
      "Train Epoch: 4 [57856/359035 (16%)]\tLoss: 0.860308\n",
      "Train Epoch: 4 [59136/359035 (16%)]\tLoss: 0.793921\n",
      "Train Epoch: 4 [60416/359035 (17%)]\tLoss: 0.925540\n",
      "Train Epoch: 4 [61696/359035 (17%)]\tLoss: 0.878211\n",
      "Train Epoch: 4 [62976/359035 (18%)]\tLoss: 0.954691\n",
      "Train Epoch: 4 [64256/359035 (18%)]\tLoss: 0.840782\n",
      "Train Epoch: 4 [65536/359035 (18%)]\tLoss: 0.868880\n",
      "Train Epoch: 4 [66816/359035 (19%)]\tLoss: 0.827437\n",
      "Train Epoch: 4 [68096/359035 (19%)]\tLoss: 0.886089\n",
      "Train Epoch: 4 [69376/359035 (19%)]\tLoss: 0.904923\n",
      "Train Epoch: 4 [70656/359035 (20%)]\tLoss: 0.826597\n",
      "Train Epoch: 4 [71936/359035 (20%)]\tLoss: 0.854402\n",
      "Train Epoch: 4 [73216/359035 (20%)]\tLoss: 0.861435\n",
      "Train Epoch: 4 [74496/359035 (21%)]\tLoss: 0.936970\n",
      "Train Epoch: 4 [75776/359035 (21%)]\tLoss: 0.831326\n",
      "Train Epoch: 4 [77056/359035 (21%)]\tLoss: 0.891344\n",
      "Train Epoch: 4 [78336/359035 (22%)]\tLoss: 0.961333\n",
      "Train Epoch: 4 [79616/359035 (22%)]\tLoss: 0.913175\n",
      "Train Epoch: 4 [80896/359035 (23%)]\tLoss: 0.805331\n",
      "Train Epoch: 4 [82176/359035 (23%)]\tLoss: 0.782185\n",
      "Train Epoch: 4 [83456/359035 (23%)]\tLoss: 0.802146\n",
      "Train Epoch: 4 [84736/359035 (24%)]\tLoss: 0.801369\n",
      "Train Epoch: 4 [86016/359035 (24%)]\tLoss: 0.915519\n",
      "Train Epoch: 4 [87296/359035 (24%)]\tLoss: 0.864313\n",
      "Train Epoch: 4 [88576/359035 (25%)]\tLoss: 0.729917\n",
      "Train Epoch: 4 [89856/359035 (25%)]\tLoss: 0.911106\n",
      "Train Epoch: 4 [91136/359035 (25%)]\tLoss: 0.841666\n",
      "Train Epoch: 4 [92416/359035 (26%)]\tLoss: 0.837294\n",
      "Train Epoch: 4 [93696/359035 (26%)]\tLoss: 0.850068\n",
      "Train Epoch: 4 [94976/359035 (26%)]\tLoss: 0.859404\n",
      "Train Epoch: 4 [96256/359035 (27%)]\tLoss: 0.776800\n",
      "Train Epoch: 4 [97536/359035 (27%)]\tLoss: 0.935498\n",
      "Train Epoch: 4 [98816/359035 (28%)]\tLoss: 0.896766\n",
      "Train Epoch: 4 [100096/359035 (28%)]\tLoss: 0.881457\n",
      "Train Epoch: 4 [101376/359035 (28%)]\tLoss: 0.810691\n",
      "Train Epoch: 4 [102656/359035 (29%)]\tLoss: 0.841275\n",
      "Train Epoch: 4 [103936/359035 (29%)]\tLoss: 0.894714\n",
      "Train Epoch: 4 [105216/359035 (29%)]\tLoss: 0.912197\n",
      "Train Epoch: 4 [106496/359035 (30%)]\tLoss: 0.721532\n",
      "Train Epoch: 4 [107776/359035 (30%)]\tLoss: 0.746040\n",
      "Train Epoch: 4 [109056/359035 (30%)]\tLoss: 0.876195\n",
      "Train Epoch: 4 [110336/359035 (31%)]\tLoss: 0.784076\n",
      "Train Epoch: 4 [111616/359035 (31%)]\tLoss: 0.889081\n",
      "Train Epoch: 4 [112896/359035 (31%)]\tLoss: 0.826536\n",
      "Train Epoch: 4 [114176/359035 (32%)]\tLoss: 0.921176\n",
      "Train Epoch: 4 [115456/359035 (32%)]\tLoss: 0.881743\n",
      "Train Epoch: 4 [116736/359035 (33%)]\tLoss: 0.840329\n",
      "Train Epoch: 4 [118016/359035 (33%)]\tLoss: 0.907315\n",
      "Train Epoch: 4 [119296/359035 (33%)]\tLoss: 0.722961\n",
      "Train Epoch: 4 [120576/359035 (34%)]\tLoss: 0.903072\n",
      "Train Epoch: 4 [121856/359035 (34%)]\tLoss: 0.917752\n",
      "Train Epoch: 4 [123136/359035 (34%)]\tLoss: 0.946212\n",
      "Train Epoch: 4 [124416/359035 (35%)]\tLoss: 0.850778\n",
      "Train Epoch: 4 [125696/359035 (35%)]\tLoss: 0.940917\n",
      "Train Epoch: 4 [126976/359035 (35%)]\tLoss: 0.850384\n",
      "Train Epoch: 4 [128256/359035 (36%)]\tLoss: 0.906109\n",
      "Train Epoch: 4 [129536/359035 (36%)]\tLoss: 0.957735\n",
      "Train Epoch: 4 [130816/359035 (36%)]\tLoss: 0.911211\n",
      "Train Epoch: 4 [132096/359035 (37%)]\tLoss: 0.907867\n",
      "Train Epoch: 4 [133376/359035 (37%)]\tLoss: 0.783229\n",
      "Train Epoch: 4 [134656/359035 (38%)]\tLoss: 0.945249\n",
      "Train Epoch: 4 [135936/359035 (38%)]\tLoss: 0.868558\n",
      "Train Epoch: 4 [137216/359035 (38%)]\tLoss: 0.892802\n",
      "Train Epoch: 4 [138496/359035 (39%)]\tLoss: 0.839987\n",
      "Train Epoch: 4 [139776/359035 (39%)]\tLoss: 0.858069\n",
      "Train Epoch: 4 [141056/359035 (39%)]\tLoss: 0.826019\n",
      "Train Epoch: 4 [142336/359035 (40%)]\tLoss: 0.932549\n",
      "Train Epoch: 4 [143616/359035 (40%)]\tLoss: 0.800266\n",
      "Train Epoch: 4 [144896/359035 (40%)]\tLoss: 0.890839\n",
      "Train Epoch: 4 [146176/359035 (41%)]\tLoss: 0.874667\n",
      "Train Epoch: 4 [147456/359035 (41%)]\tLoss: 0.757715\n",
      "Train Epoch: 4 [148736/359035 (41%)]\tLoss: 0.767018\n",
      "Train Epoch: 4 [150016/359035 (42%)]\tLoss: 0.865746\n",
      "Train Epoch: 4 [151296/359035 (42%)]\tLoss: 0.832750\n",
      "Train Epoch: 4 [152576/359035 (42%)]\tLoss: 0.812029\n",
      "Train Epoch: 4 [153856/359035 (43%)]\tLoss: 0.791258\n",
      "Train Epoch: 4 [155136/359035 (43%)]\tLoss: 0.868257\n",
      "Train Epoch: 4 [156416/359035 (44%)]\tLoss: 0.823579\n",
      "Train Epoch: 4 [157696/359035 (44%)]\tLoss: 0.841181\n",
      "Train Epoch: 4 [158976/359035 (44%)]\tLoss: 0.855111\n",
      "Train Epoch: 4 [160256/359035 (45%)]\tLoss: 0.914895\n",
      "Train Epoch: 4 [161536/359035 (45%)]\tLoss: 0.883351\n",
      "Train Epoch: 4 [162816/359035 (45%)]\tLoss: 0.820328\n",
      "Train Epoch: 4 [164096/359035 (46%)]\tLoss: 0.839641\n",
      "Train Epoch: 4 [165376/359035 (46%)]\tLoss: 0.815227\n",
      "Train Epoch: 4 [166656/359035 (46%)]\tLoss: 0.928141\n",
      "Train Epoch: 4 [167936/359035 (47%)]\tLoss: 0.977973\n",
      "Train Epoch: 4 [169216/359035 (47%)]\tLoss: 0.902774\n",
      "Train Epoch: 4 [170496/359035 (47%)]\tLoss: 0.887437\n",
      "Train Epoch: 4 [171776/359035 (48%)]\tLoss: 0.862005\n",
      "Train Epoch: 4 [173056/359035 (48%)]\tLoss: 0.883332\n",
      "Train Epoch: 4 [174336/359035 (49%)]\tLoss: 0.880583\n",
      "Train Epoch: 4 [175616/359035 (49%)]\tLoss: 0.850471\n",
      "Train Epoch: 4 [176896/359035 (49%)]\tLoss: 0.949452\n",
      "Train Epoch: 4 [178176/359035 (50%)]\tLoss: 0.907644\n",
      "Train Epoch: 4 [179456/359035 (50%)]\tLoss: 0.859543\n",
      "Train Epoch: 4 [180736/359035 (50%)]\tLoss: 0.793855\n",
      "Train Epoch: 4 [182016/359035 (51%)]\tLoss: 0.844809\n",
      "Train Epoch: 4 [183296/359035 (51%)]\tLoss: 0.977053\n",
      "Train Epoch: 4 [184576/359035 (51%)]\tLoss: 0.864566\n",
      "Train Epoch: 4 [185856/359035 (52%)]\tLoss: 0.955846\n",
      "Train Epoch: 4 [187136/359035 (52%)]\tLoss: 0.785627\n",
      "Train Epoch: 4 [188416/359035 (52%)]\tLoss: 0.950873\n",
      "Train Epoch: 4 [189696/359035 (53%)]\tLoss: 0.956385\n",
      "Train Epoch: 4 [190976/359035 (53%)]\tLoss: 0.941089\n",
      "Train Epoch: 4 [192256/359035 (54%)]\tLoss: 0.872307\n",
      "Train Epoch: 4 [193536/359035 (54%)]\tLoss: 0.795307\n",
      "Train Epoch: 4 [194816/359035 (54%)]\tLoss: 0.901568\n",
      "Train Epoch: 4 [196096/359035 (55%)]\tLoss: 0.911567\n",
      "Train Epoch: 4 [197376/359035 (55%)]\tLoss: 0.900767\n",
      "Train Epoch: 4 [198656/359035 (55%)]\tLoss: 0.830182\n",
      "Train Epoch: 4 [199936/359035 (56%)]\tLoss: 0.910433\n",
      "Train Epoch: 4 [201216/359035 (56%)]\tLoss: 0.861422\n",
      "Train Epoch: 4 [202496/359035 (56%)]\tLoss: 0.857392\n",
      "Train Epoch: 4 [203776/359035 (57%)]\tLoss: 0.789354\n",
      "Train Epoch: 4 [205056/359035 (57%)]\tLoss: 0.893251\n",
      "Train Epoch: 4 [206336/359035 (57%)]\tLoss: 0.929326\n",
      "Train Epoch: 4 [207616/359035 (58%)]\tLoss: 0.928077\n",
      "Train Epoch: 4 [208896/359035 (58%)]\tLoss: 0.838809\n",
      "Train Epoch: 4 [210176/359035 (59%)]\tLoss: 0.835998\n",
      "Train Epoch: 4 [211456/359035 (59%)]\tLoss: 0.935425\n",
      "Train Epoch: 4 [212736/359035 (59%)]\tLoss: 0.858600\n",
      "Train Epoch: 4 [214016/359035 (60%)]\tLoss: 0.932588\n",
      "Train Epoch: 4 [215296/359035 (60%)]\tLoss: 0.800362\n",
      "Train Epoch: 4 [216576/359035 (60%)]\tLoss: 0.844827\n",
      "Train Epoch: 4 [217856/359035 (61%)]\tLoss: 0.826960\n",
      "Train Epoch: 4 [219136/359035 (61%)]\tLoss: 0.807592\n",
      "Train Epoch: 4 [220416/359035 (61%)]\tLoss: 0.825765\n",
      "Train Epoch: 4 [221696/359035 (62%)]\tLoss: 0.933128\n",
      "Train Epoch: 4 [222976/359035 (62%)]\tLoss: 0.869786\n",
      "Train Epoch: 4 [224256/359035 (62%)]\tLoss: 0.874968\n",
      "Train Epoch: 4 [225536/359035 (63%)]\tLoss: 0.911486\n",
      "Train Epoch: 4 [226816/359035 (63%)]\tLoss: 0.842307\n",
      "Train Epoch: 4 [228096/359035 (64%)]\tLoss: 0.868474\n",
      "Train Epoch: 4 [229376/359035 (64%)]\tLoss: 0.920144\n",
      "Train Epoch: 4 [230656/359035 (64%)]\tLoss: 0.834328\n",
      "Train Epoch: 4 [231936/359035 (65%)]\tLoss: 0.833482\n",
      "Train Epoch: 4 [233216/359035 (65%)]\tLoss: 0.897272\n",
      "Train Epoch: 4 [234496/359035 (65%)]\tLoss: 0.839591\n",
      "Train Epoch: 4 [235776/359035 (66%)]\tLoss: 0.909477\n",
      "Train Epoch: 4 [237056/359035 (66%)]\tLoss: 0.838848\n",
      "Train Epoch: 4 [238336/359035 (66%)]\tLoss: 0.796591\n",
      "Train Epoch: 4 [239616/359035 (67%)]\tLoss: 0.887456\n",
      "Train Epoch: 4 [240896/359035 (67%)]\tLoss: 0.858726\n",
      "Train Epoch: 4 [242176/359035 (67%)]\tLoss: 0.836031\n",
      "Train Epoch: 4 [243456/359035 (68%)]\tLoss: 0.866549\n",
      "Train Epoch: 4 [244736/359035 (68%)]\tLoss: 0.856114\n",
      "Train Epoch: 4 [246016/359035 (69%)]\tLoss: 0.877023\n",
      "Train Epoch: 4 [247296/359035 (69%)]\tLoss: 0.887574\n",
      "Train Epoch: 4 [248576/359035 (69%)]\tLoss: 0.943843\n",
      "Train Epoch: 4 [249856/359035 (70%)]\tLoss: 0.939466\n",
      "Train Epoch: 4 [251136/359035 (70%)]\tLoss: 0.771782\n",
      "Train Epoch: 4 [252416/359035 (70%)]\tLoss: 0.797794\n",
      "Train Epoch: 4 [253696/359035 (71%)]\tLoss: 0.845055\n",
      "Train Epoch: 4 [254976/359035 (71%)]\tLoss: 0.884977\n",
      "Train Epoch: 4 [256256/359035 (71%)]\tLoss: 0.949969\n",
      "Train Epoch: 4 [257536/359035 (72%)]\tLoss: 0.826484\n",
      "Train Epoch: 4 [258816/359035 (72%)]\tLoss: 0.855720\n",
      "Train Epoch: 4 [260096/359035 (72%)]\tLoss: 0.860079\n",
      "Train Epoch: 4 [261376/359035 (73%)]\tLoss: 0.938467\n",
      "Train Epoch: 4 [262656/359035 (73%)]\tLoss: 0.885912\n",
      "Train Epoch: 4 [263936/359035 (74%)]\tLoss: 0.926223\n",
      "Train Epoch: 4 [265216/359035 (74%)]\tLoss: 0.856008\n",
      "Train Epoch: 4 [266496/359035 (74%)]\tLoss: 0.915160\n",
      "Train Epoch: 4 [267776/359035 (75%)]\tLoss: 0.914861\n",
      "Train Epoch: 4 [269056/359035 (75%)]\tLoss: 0.764401\n",
      "Train Epoch: 4 [270336/359035 (75%)]\tLoss: 0.961410\n",
      "Train Epoch: 4 [271616/359035 (76%)]\tLoss: 0.887248\n",
      "Train Epoch: 4 [272896/359035 (76%)]\tLoss: 0.847719\n",
      "Train Epoch: 4 [274176/359035 (76%)]\tLoss: 0.861605\n",
      "Train Epoch: 4 [275456/359035 (77%)]\tLoss: 0.863612\n",
      "Train Epoch: 4 [276736/359035 (77%)]\tLoss: 0.858836\n",
      "Train Epoch: 4 [278016/359035 (77%)]\tLoss: 0.816627\n",
      "Train Epoch: 4 [279296/359035 (78%)]\tLoss: 0.863806\n",
      "Train Epoch: 4 [280576/359035 (78%)]\tLoss: 0.897915\n",
      "Train Epoch: 4 [281856/359035 (79%)]\tLoss: 0.839896\n",
      "Train Epoch: 4 [283136/359035 (79%)]\tLoss: 0.946684\n",
      "Train Epoch: 4 [284416/359035 (79%)]\tLoss: 0.851176\n",
      "Train Epoch: 4 [285696/359035 (80%)]\tLoss: 0.798476\n",
      "Train Epoch: 4 [286976/359035 (80%)]\tLoss: 0.916061\n",
      "Train Epoch: 4 [288256/359035 (80%)]\tLoss: 0.790683\n",
      "Train Epoch: 4 [289536/359035 (81%)]\tLoss: 0.838235\n",
      "Train Epoch: 4 [290816/359035 (81%)]\tLoss: 0.871249\n",
      "Train Epoch: 4 [292096/359035 (81%)]\tLoss: 0.980197\n",
      "Train Epoch: 4 [293376/359035 (82%)]\tLoss: 0.851903\n",
      "Train Epoch: 4 [294656/359035 (82%)]\tLoss: 0.852346\n",
      "Train Epoch: 4 [295936/359035 (82%)]\tLoss: 0.880948\n",
      "Train Epoch: 4 [297216/359035 (83%)]\tLoss: 0.869856\n",
      "Train Epoch: 4 [298496/359035 (83%)]\tLoss: 0.868274\n",
      "Train Epoch: 4 [299776/359035 (83%)]\tLoss: 0.910494\n",
      "Train Epoch: 4 [301056/359035 (84%)]\tLoss: 0.854820\n",
      "Train Epoch: 4 [302336/359035 (84%)]\tLoss: 0.916994\n",
      "Train Epoch: 4 [303616/359035 (85%)]\tLoss: 0.932784\n",
      "Train Epoch: 4 [304896/359035 (85%)]\tLoss: 0.841161\n",
      "Train Epoch: 4 [306176/359035 (85%)]\tLoss: 0.876789\n",
      "Train Epoch: 4 [307456/359035 (86%)]\tLoss: 0.876229\n",
      "Train Epoch: 4 [308736/359035 (86%)]\tLoss: 0.920835\n",
      "Train Epoch: 4 [310016/359035 (86%)]\tLoss: 0.852938\n",
      "Train Epoch: 4 [311296/359035 (87%)]\tLoss: 0.893544\n",
      "Train Epoch: 4 [312576/359035 (87%)]\tLoss: 0.867972\n",
      "Train Epoch: 4 [313856/359035 (87%)]\tLoss: 0.881385\n",
      "Train Epoch: 4 [315136/359035 (88%)]\tLoss: 0.913751\n",
      "Train Epoch: 4 [316416/359035 (88%)]\tLoss: 0.874358\n",
      "Train Epoch: 4 [317696/359035 (88%)]\tLoss: 0.896074\n",
      "Train Epoch: 4 [318976/359035 (89%)]\tLoss: 0.995406\n",
      "Train Epoch: 4 [320256/359035 (89%)]\tLoss: 0.894103\n",
      "Train Epoch: 4 [321536/359035 (90%)]\tLoss: 0.859926\n",
      "Train Epoch: 4 [322816/359035 (90%)]\tLoss: 0.936313\n",
      "Train Epoch: 4 [324096/359035 (90%)]\tLoss: 0.790107\n",
      "Train Epoch: 4 [325376/359035 (91%)]\tLoss: 0.926830\n",
      "Train Epoch: 4 [326656/359035 (91%)]\tLoss: 0.802812\n",
      "Train Epoch: 4 [327936/359035 (91%)]\tLoss: 0.848353\n",
      "Train Epoch: 4 [329216/359035 (92%)]\tLoss: 0.933771\n",
      "Train Epoch: 4 [330496/359035 (92%)]\tLoss: 0.905722\n",
      "Train Epoch: 4 [331776/359035 (92%)]\tLoss: 0.908813\n",
      "Train Epoch: 4 [333056/359035 (93%)]\tLoss: 0.824801\n",
      "Train Epoch: 4 [334336/359035 (93%)]\tLoss: 0.877919\n",
      "Train Epoch: 4 [335616/359035 (93%)]\tLoss: 0.873136\n",
      "Train Epoch: 4 [336896/359035 (94%)]\tLoss: 0.862559\n",
      "Train Epoch: 4 [338176/359035 (94%)]\tLoss: 0.913210\n",
      "Train Epoch: 4 [339456/359035 (95%)]\tLoss: 0.865547\n",
      "Train Epoch: 4 [340736/359035 (95%)]\tLoss: 0.882252\n",
      "Train Epoch: 4 [342016/359035 (95%)]\tLoss: 1.002051\n",
      "Train Epoch: 4 [343296/359035 (96%)]\tLoss: 0.954373\n",
      "Train Epoch: 4 [344576/359035 (96%)]\tLoss: 0.982901\n",
      "Train Epoch: 4 [345856/359035 (96%)]\tLoss: 0.847680\n",
      "Train Epoch: 4 [347136/359035 (97%)]\tLoss: 0.788152\n",
      "Train Epoch: 4 [348416/359035 (97%)]\tLoss: 0.959061\n",
      "Train Epoch: 4 [349696/359035 (97%)]\tLoss: 0.842790\n",
      "Train Epoch: 4 [350976/359035 (98%)]\tLoss: 0.865448\n",
      "Train Epoch: 4 [352256/359035 (98%)]\tLoss: 0.859980\n",
      "Train Epoch: 4 [353536/359035 (98%)]\tLoss: 0.904551\n",
      "Train Epoch: 4 [354816/359035 (99%)]\tLoss: 0.837869\n",
      "Train Epoch: 4 [356096/359035 (99%)]\tLoss: 0.868838\n",
      "Train Epoch: 4 [357376/359035 (100%)]\tLoss: 0.910284\n",
      "Train Epoch: 4 [358656/359035 (100%)]\tLoss: 0.876041\n",
      "Performance on test set: Average loss: 0.9391, Accuracy: 134/200 (67%)\n",
      "Epoch took: 2567.622s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.4880, 100to500: 0.2181, 20to50: 0.1548, 500to_above: 0.1391\n",
      "\n",
      "Epoch 5\n",
      "Train Epoch: 5 [256/359035 (0%)]\tLoss: 0.875195\n",
      "Train Epoch: 5 [1536/359035 (0%)]\tLoss: 0.823676\n",
      "Train Epoch: 5 [2816/359035 (1%)]\tLoss: 0.921183\n",
      "Train Epoch: 5 [4096/359035 (1%)]\tLoss: 0.905961\n",
      "Train Epoch: 5 [5376/359035 (1%)]\tLoss: 0.933485\n",
      "Train Epoch: 5 [6656/359035 (2%)]\tLoss: 0.901743\n",
      "Train Epoch: 5 [7936/359035 (2%)]\tLoss: 0.853320\n",
      "Train Epoch: 5 [9216/359035 (3%)]\tLoss: 0.783080\n",
      "Train Epoch: 5 [10496/359035 (3%)]\tLoss: 0.885939\n",
      "Train Epoch: 5 [11776/359035 (3%)]\tLoss: 0.879197\n",
      "Train Epoch: 5 [13056/359035 (4%)]\tLoss: 0.873901\n",
      "Train Epoch: 5 [14336/359035 (4%)]\tLoss: 0.808310\n",
      "Train Epoch: 5 [15616/359035 (4%)]\tLoss: 0.869225\n",
      "Train Epoch: 5 [16896/359035 (5%)]\tLoss: 0.786943\n",
      "Train Epoch: 5 [18176/359035 (5%)]\tLoss: 0.695362\n",
      "Train Epoch: 5 [19456/359035 (5%)]\tLoss: 0.919349\n",
      "Train Epoch: 5 [20736/359035 (6%)]\tLoss: 0.906106\n",
      "Train Epoch: 5 [22016/359035 (6%)]\tLoss: 0.856747\n",
      "Train Epoch: 5 [23296/359035 (6%)]\tLoss: 0.897269\n",
      "Train Epoch: 5 [24576/359035 (7%)]\tLoss: 0.914514\n",
      "Train Epoch: 5 [25856/359035 (7%)]\tLoss: 0.816524\n",
      "Train Epoch: 5 [27136/359035 (8%)]\tLoss: 0.846415\n",
      "Train Epoch: 5 [28416/359035 (8%)]\tLoss: 0.799709\n",
      "Train Epoch: 5 [29696/359035 (8%)]\tLoss: 0.781520\n",
      "Train Epoch: 5 [30976/359035 (9%)]\tLoss: 0.857157\n",
      "Train Epoch: 5 [32256/359035 (9%)]\tLoss: 0.818670\n",
      "Train Epoch: 5 [33536/359035 (9%)]\tLoss: 0.855311\n",
      "Train Epoch: 5 [34816/359035 (10%)]\tLoss: 0.808109\n",
      "Train Epoch: 5 [36096/359035 (10%)]\tLoss: 0.842745\n",
      "Train Epoch: 5 [37376/359035 (10%)]\tLoss: 0.921645\n",
      "Train Epoch: 5 [38656/359035 (11%)]\tLoss: 0.861784\n",
      "Train Epoch: 5 [39936/359035 (11%)]\tLoss: 0.918464\n",
      "Train Epoch: 5 [41216/359035 (11%)]\tLoss: 0.929397\n",
      "Train Epoch: 5 [42496/359035 (12%)]\tLoss: 0.824668\n",
      "Train Epoch: 5 [43776/359035 (12%)]\tLoss: 0.856312\n",
      "Train Epoch: 5 [45056/359035 (13%)]\tLoss: 0.878486\n",
      "Train Epoch: 5 [46336/359035 (13%)]\tLoss: 0.825940\n",
      "Train Epoch: 5 [47616/359035 (13%)]\tLoss: 0.937707\n",
      "Train Epoch: 5 [48896/359035 (14%)]\tLoss: 0.856209\n",
      "Train Epoch: 5 [50176/359035 (14%)]\tLoss: 0.848277\n",
      "Train Epoch: 5 [51456/359035 (14%)]\tLoss: 0.810543\n",
      "Train Epoch: 5 [52736/359035 (15%)]\tLoss: 0.791760\n",
      "Train Epoch: 5 [54016/359035 (15%)]\tLoss: 0.872857\n",
      "Train Epoch: 5 [55296/359035 (15%)]\tLoss: 0.899225\n",
      "Train Epoch: 5 [56576/359035 (16%)]\tLoss: 0.905231\n",
      "Train Epoch: 5 [57856/359035 (16%)]\tLoss: 0.781710\n",
      "Train Epoch: 5 [59136/359035 (16%)]\tLoss: 0.850898\n",
      "Train Epoch: 5 [60416/359035 (17%)]\tLoss: 0.849877\n",
      "Train Epoch: 5 [61696/359035 (17%)]\tLoss: 0.951712\n",
      "Train Epoch: 5 [62976/359035 (18%)]\tLoss: 0.838697\n",
      "Train Epoch: 5 [64256/359035 (18%)]\tLoss: 0.930011\n",
      "Train Epoch: 5 [65536/359035 (18%)]\tLoss: 0.877573\n",
      "Train Epoch: 5 [66816/359035 (19%)]\tLoss: 0.891500\n",
      "Train Epoch: 5 [68096/359035 (19%)]\tLoss: 0.782797\n",
      "Train Epoch: 5 [69376/359035 (19%)]\tLoss: 0.922949\n",
      "Train Epoch: 5 [70656/359035 (20%)]\tLoss: 0.788114\n",
      "Train Epoch: 5 [71936/359035 (20%)]\tLoss: 0.787954\n",
      "Train Epoch: 5 [73216/359035 (20%)]\tLoss: 0.865322\n",
      "Train Epoch: 5 [74496/359035 (21%)]\tLoss: 0.764565\n",
      "Train Epoch: 5 [75776/359035 (21%)]\tLoss: 0.801291\n",
      "Train Epoch: 5 [77056/359035 (21%)]\tLoss: 0.914318\n",
      "Train Epoch: 5 [78336/359035 (22%)]\tLoss: 0.927287\n",
      "Train Epoch: 5 [79616/359035 (22%)]\tLoss: 0.926015\n",
      "Train Epoch: 5 [80896/359035 (23%)]\tLoss: 0.867732\n",
      "Train Epoch: 5 [82176/359035 (23%)]\tLoss: 0.841907\n",
      "Train Epoch: 5 [83456/359035 (23%)]\tLoss: 0.903909\n",
      "Train Epoch: 5 [84736/359035 (24%)]\tLoss: 0.873386\n",
      "Train Epoch: 5 [86016/359035 (24%)]\tLoss: 0.817077\n",
      "Train Epoch: 5 [87296/359035 (24%)]\tLoss: 0.925225\n",
      "Train Epoch: 5 [88576/359035 (25%)]\tLoss: 0.938287\n",
      "Train Epoch: 5 [89856/359035 (25%)]\tLoss: 0.894272\n",
      "Train Epoch: 5 [91136/359035 (25%)]\tLoss: 0.851031\n",
      "Train Epoch: 5 [92416/359035 (26%)]\tLoss: 0.945143\n",
      "Train Epoch: 5 [93696/359035 (26%)]\tLoss: 0.961719\n",
      "Train Epoch: 5 [94976/359035 (26%)]\tLoss: 0.832502\n",
      "Train Epoch: 5 [96256/359035 (27%)]\tLoss: 0.821541\n",
      "Train Epoch: 5 [97536/359035 (27%)]\tLoss: 0.894911\n",
      "Train Epoch: 5 [98816/359035 (28%)]\tLoss: 0.797854\n",
      "Train Epoch: 5 [100096/359035 (28%)]\tLoss: 0.875796\n",
      "Train Epoch: 5 [101376/359035 (28%)]\tLoss: 0.934774\n",
      "Train Epoch: 5 [102656/359035 (29%)]\tLoss: 0.891719\n",
      "Train Epoch: 5 [103936/359035 (29%)]\tLoss: 0.864371\n",
      "Train Epoch: 5 [105216/359035 (29%)]\tLoss: 0.926465\n",
      "Train Epoch: 5 [106496/359035 (30%)]\tLoss: 0.791539\n",
      "Train Epoch: 5 [107776/359035 (30%)]\tLoss: 0.883623\n",
      "Train Epoch: 5 [109056/359035 (30%)]\tLoss: 0.868896\n",
      "Train Epoch: 5 [110336/359035 (31%)]\tLoss: 0.838965\n",
      "Train Epoch: 5 [111616/359035 (31%)]\tLoss: 0.906019\n",
      "Train Epoch: 5 [112896/359035 (31%)]\tLoss: 0.835211\n",
      "Train Epoch: 5 [114176/359035 (32%)]\tLoss: 0.870112\n",
      "Train Epoch: 5 [115456/359035 (32%)]\tLoss: 0.802958\n",
      "Train Epoch: 5 [116736/359035 (33%)]\tLoss: 0.824332\n",
      "Train Epoch: 5 [118016/359035 (33%)]\tLoss: 0.844500\n",
      "Train Epoch: 5 [119296/359035 (33%)]\tLoss: 0.834101\n",
      "Train Epoch: 5 [120576/359035 (34%)]\tLoss: 0.830075\n",
      "Train Epoch: 5 [121856/359035 (34%)]\tLoss: 0.836086\n",
      "Train Epoch: 5 [123136/359035 (34%)]\tLoss: 0.858668\n",
      "Train Epoch: 5 [124416/359035 (35%)]\tLoss: 0.854483\n",
      "Train Epoch: 5 [125696/359035 (35%)]\tLoss: 0.915547\n",
      "Train Epoch: 5 [126976/359035 (35%)]\tLoss: 0.865899\n",
      "Train Epoch: 5 [128256/359035 (36%)]\tLoss: 0.846710\n",
      "Train Epoch: 5 [129536/359035 (36%)]\tLoss: 0.883102\n",
      "Train Epoch: 5 [130816/359035 (36%)]\tLoss: 0.881832\n",
      "Train Epoch: 5 [132096/359035 (37%)]\tLoss: 0.760958\n",
      "Train Epoch: 5 [133376/359035 (37%)]\tLoss: 0.773063\n",
      "Train Epoch: 5 [134656/359035 (38%)]\tLoss: 0.879368\n",
      "Train Epoch: 5 [135936/359035 (38%)]\tLoss: 0.907190\n",
      "Train Epoch: 5 [137216/359035 (38%)]\tLoss: 0.969375\n",
      "Train Epoch: 5 [138496/359035 (39%)]\tLoss: 0.928946\n",
      "Train Epoch: 5 [139776/359035 (39%)]\tLoss: 0.925379\n",
      "Train Epoch: 5 [141056/359035 (39%)]\tLoss: 0.964252\n",
      "Train Epoch: 5 [142336/359035 (40%)]\tLoss: 0.787513\n",
      "Train Epoch: 5 [143616/359035 (40%)]\tLoss: 0.857149\n",
      "Train Epoch: 5 [144896/359035 (40%)]\tLoss: 0.821745\n",
      "Train Epoch: 5 [146176/359035 (41%)]\tLoss: 0.844915\n",
      "Train Epoch: 5 [147456/359035 (41%)]\tLoss: 0.865125\n",
      "Train Epoch: 5 [148736/359035 (41%)]\tLoss: 0.834405\n",
      "Train Epoch: 5 [150016/359035 (42%)]\tLoss: 0.891945\n",
      "Train Epoch: 5 [151296/359035 (42%)]\tLoss: 0.744438\n",
      "Train Epoch: 5 [152576/359035 (42%)]\tLoss: 0.809323\n",
      "Train Epoch: 5 [153856/359035 (43%)]\tLoss: 0.864204\n",
      "Train Epoch: 5 [155136/359035 (43%)]\tLoss: 0.805619\n",
      "Train Epoch: 5 [156416/359035 (44%)]\tLoss: 0.928235\n",
      "Train Epoch: 5 [157696/359035 (44%)]\tLoss: 0.935303\n",
      "Train Epoch: 5 [158976/359035 (44%)]\tLoss: 0.923602\n",
      "Train Epoch: 5 [160256/359035 (45%)]\tLoss: 0.794119\n",
      "Train Epoch: 5 [161536/359035 (45%)]\tLoss: 0.827275\n",
      "Train Epoch: 5 [162816/359035 (45%)]\tLoss: 0.837034\n",
      "Train Epoch: 5 [164096/359035 (46%)]\tLoss: 0.929317\n",
      "Train Epoch: 5 [165376/359035 (46%)]\tLoss: 0.845846\n",
      "Train Epoch: 5 [166656/359035 (46%)]\tLoss: 0.993773\n",
      "Train Epoch: 5 [167936/359035 (47%)]\tLoss: 0.888229\n",
      "Train Epoch: 5 [169216/359035 (47%)]\tLoss: 0.894716\n",
      "Train Epoch: 5 [170496/359035 (47%)]\tLoss: 0.880305\n",
      "Train Epoch: 5 [171776/359035 (48%)]\tLoss: 0.918503\n",
      "Train Epoch: 5 [173056/359035 (48%)]\tLoss: 0.912345\n",
      "Train Epoch: 5 [174336/359035 (49%)]\tLoss: 0.880252\n",
      "Train Epoch: 5 [175616/359035 (49%)]\tLoss: 0.959567\n",
      "Train Epoch: 5 [176896/359035 (49%)]\tLoss: 0.884154\n",
      "Train Epoch: 5 [178176/359035 (50%)]\tLoss: 0.891122\n",
      "Train Epoch: 5 [179456/359035 (50%)]\tLoss: 0.795572\n",
      "Train Epoch: 5 [180736/359035 (50%)]\tLoss: 0.825319\n",
      "Train Epoch: 5 [182016/359035 (51%)]\tLoss: 0.866913\n",
      "Train Epoch: 5 [183296/359035 (51%)]\tLoss: 0.835219\n",
      "Train Epoch: 5 [184576/359035 (51%)]\tLoss: 0.786238\n",
      "Train Epoch: 5 [185856/359035 (52%)]\tLoss: 0.867528\n",
      "Train Epoch: 5 [187136/359035 (52%)]\tLoss: 0.887432\n",
      "Train Epoch: 5 [188416/359035 (52%)]\tLoss: 0.829436\n",
      "Train Epoch: 5 [189696/359035 (53%)]\tLoss: 0.902283\n",
      "Train Epoch: 5 [190976/359035 (53%)]\tLoss: 0.928686\n",
      "Train Epoch: 5 [192256/359035 (54%)]\tLoss: 0.892770\n",
      "Train Epoch: 5 [193536/359035 (54%)]\tLoss: 0.893439\n",
      "Train Epoch: 5 [194816/359035 (54%)]\tLoss: 0.795650\n",
      "Train Epoch: 5 [196096/359035 (55%)]\tLoss: 0.902937\n",
      "Train Epoch: 5 [197376/359035 (55%)]\tLoss: 0.883900\n",
      "Train Epoch: 5 [198656/359035 (55%)]\tLoss: 0.800863\n",
      "Train Epoch: 5 [199936/359035 (56%)]\tLoss: 0.917965\n",
      "Train Epoch: 5 [201216/359035 (56%)]\tLoss: 0.945004\n",
      "Train Epoch: 5 [202496/359035 (56%)]\tLoss: 0.833629\n",
      "Train Epoch: 5 [203776/359035 (57%)]\tLoss: 0.820261\n",
      "Train Epoch: 5 [205056/359035 (57%)]\tLoss: 0.893833\n",
      "Train Epoch: 5 [206336/359035 (57%)]\tLoss: 0.821112\n",
      "Train Epoch: 5 [207616/359035 (58%)]\tLoss: 0.830550\n",
      "Train Epoch: 5 [208896/359035 (58%)]\tLoss: 1.040932\n",
      "Train Epoch: 5 [210176/359035 (59%)]\tLoss: 0.901329\n",
      "Train Epoch: 5 [211456/359035 (59%)]\tLoss: 0.834054\n",
      "Train Epoch: 5 [212736/359035 (59%)]\tLoss: 0.951287\n",
      "Train Epoch: 5 [214016/359035 (60%)]\tLoss: 0.910238\n",
      "Train Epoch: 5 [215296/359035 (60%)]\tLoss: 0.904679\n",
      "Train Epoch: 5 [216576/359035 (60%)]\tLoss: 0.833111\n",
      "Train Epoch: 5 [217856/359035 (61%)]\tLoss: 0.888761\n",
      "Train Epoch: 5 [219136/359035 (61%)]\tLoss: 0.898440\n",
      "Train Epoch: 5 [220416/359035 (61%)]\tLoss: 0.802564\n",
      "Train Epoch: 5 [221696/359035 (62%)]\tLoss: 0.957596\n",
      "Train Epoch: 5 [222976/359035 (62%)]\tLoss: 0.874059\n",
      "Train Epoch: 5 [224256/359035 (62%)]\tLoss: 0.752947\n",
      "Train Epoch: 5 [225536/359035 (63%)]\tLoss: 0.843773\n",
      "Train Epoch: 5 [226816/359035 (63%)]\tLoss: 0.785401\n",
      "Train Epoch: 5 [228096/359035 (64%)]\tLoss: 0.900932\n",
      "Train Epoch: 5 [229376/359035 (64%)]\tLoss: 0.847759\n",
      "Train Epoch: 5 [230656/359035 (64%)]\tLoss: 0.804228\n",
      "Train Epoch: 5 [231936/359035 (65%)]\tLoss: 0.947704\n",
      "Train Epoch: 5 [233216/359035 (65%)]\tLoss: 0.793718\n",
      "Train Epoch: 5 [234496/359035 (65%)]\tLoss: 0.834985\n",
      "Train Epoch: 5 [235776/359035 (66%)]\tLoss: 0.847375\n",
      "Train Epoch: 5 [237056/359035 (66%)]\tLoss: 0.841845\n",
      "Train Epoch: 5 [238336/359035 (66%)]\tLoss: 0.917430\n",
      "Train Epoch: 5 [239616/359035 (67%)]\tLoss: 0.935605\n",
      "Train Epoch: 5 [240896/359035 (67%)]\tLoss: 0.847103\n",
      "Train Epoch: 5 [242176/359035 (67%)]\tLoss: 0.881030\n",
      "Train Epoch: 5 [243456/359035 (68%)]\tLoss: 0.938710\n",
      "Train Epoch: 5 [244736/359035 (68%)]\tLoss: 0.899365\n",
      "Train Epoch: 5 [246016/359035 (69%)]\tLoss: 0.938862\n",
      "Train Epoch: 5 [247296/359035 (69%)]\tLoss: 0.805397\n",
      "Train Epoch: 5 [248576/359035 (69%)]\tLoss: 0.916393\n",
      "Train Epoch: 5 [249856/359035 (70%)]\tLoss: 0.851302\n",
      "Train Epoch: 5 [251136/359035 (70%)]\tLoss: 0.798217\n",
      "Train Epoch: 5 [252416/359035 (70%)]\tLoss: 0.878887\n",
      "Train Epoch: 5 [253696/359035 (71%)]\tLoss: 0.879999\n",
      "Train Epoch: 5 [254976/359035 (71%)]\tLoss: 0.879855\n",
      "Train Epoch: 5 [256256/359035 (71%)]\tLoss: 0.829845\n",
      "Train Epoch: 5 [257536/359035 (72%)]\tLoss: 0.759734\n",
      "Train Epoch: 5 [258816/359035 (72%)]\tLoss: 0.826155\n",
      "Train Epoch: 5 [260096/359035 (72%)]\tLoss: 0.792873\n",
      "Train Epoch: 5 [261376/359035 (73%)]\tLoss: 0.862350\n",
      "Train Epoch: 5 [262656/359035 (73%)]\tLoss: 0.833202\n",
      "Train Epoch: 5 [263936/359035 (74%)]\tLoss: 0.960084\n",
      "Train Epoch: 5 [265216/359035 (74%)]\tLoss: 0.864224\n",
      "Train Epoch: 5 [266496/359035 (74%)]\tLoss: 0.897882\n",
      "Train Epoch: 5 [267776/359035 (75%)]\tLoss: 0.876673\n",
      "Train Epoch: 5 [269056/359035 (75%)]\tLoss: 0.902999\n",
      "Train Epoch: 5 [270336/359035 (75%)]\tLoss: 0.882736\n",
      "Train Epoch: 5 [271616/359035 (76%)]\tLoss: 0.859348\n",
      "Train Epoch: 5 [272896/359035 (76%)]\tLoss: 0.824151\n",
      "Train Epoch: 5 [274176/359035 (76%)]\tLoss: 0.785777\n",
      "Train Epoch: 5 [275456/359035 (77%)]\tLoss: 0.954406\n",
      "Train Epoch: 5 [276736/359035 (77%)]\tLoss: 0.773003\n",
      "Train Epoch: 5 [278016/359035 (77%)]\tLoss: 0.760728\n",
      "Train Epoch: 5 [279296/359035 (78%)]\tLoss: 0.822219\n",
      "Train Epoch: 5 [280576/359035 (78%)]\tLoss: 0.871712\n",
      "Train Epoch: 5 [281856/359035 (79%)]\tLoss: 0.831194\n",
      "Train Epoch: 5 [283136/359035 (79%)]\tLoss: 0.885031\n",
      "Train Epoch: 5 [284416/359035 (79%)]\tLoss: 0.782966\n",
      "Train Epoch: 5 [285696/359035 (80%)]\tLoss: 0.833641\n",
      "Train Epoch: 5 [286976/359035 (80%)]\tLoss: 0.887019\n",
      "Train Epoch: 5 [288256/359035 (80%)]\tLoss: 0.935862\n",
      "Train Epoch: 5 [289536/359035 (81%)]\tLoss: 0.749731\n",
      "Train Epoch: 5 [290816/359035 (81%)]\tLoss: 0.813218\n",
      "Train Epoch: 5 [292096/359035 (81%)]\tLoss: 0.802270\n",
      "Train Epoch: 5 [293376/359035 (82%)]\tLoss: 0.791329\n",
      "Train Epoch: 5 [294656/359035 (82%)]\tLoss: 0.821664\n",
      "Train Epoch: 5 [295936/359035 (82%)]\tLoss: 0.897805\n",
      "Train Epoch: 5 [297216/359035 (83%)]\tLoss: 0.842023\n",
      "Train Epoch: 5 [298496/359035 (83%)]\tLoss: 0.818850\n",
      "Train Epoch: 5 [299776/359035 (83%)]\tLoss: 0.954622\n",
      "Train Epoch: 5 [301056/359035 (84%)]\tLoss: 0.822787\n",
      "Train Epoch: 5 [302336/359035 (84%)]\tLoss: 0.803030\n",
      "Train Epoch: 5 [303616/359035 (85%)]\tLoss: 0.869796\n",
      "Train Epoch: 5 [304896/359035 (85%)]\tLoss: 0.933197\n",
      "Train Epoch: 5 [306176/359035 (85%)]\tLoss: 0.847142\n",
      "Train Epoch: 5 [307456/359035 (86%)]\tLoss: 0.953995\n",
      "Train Epoch: 5 [308736/359035 (86%)]\tLoss: 0.892293\n",
      "Train Epoch: 5 [310016/359035 (86%)]\tLoss: 0.920711\n",
      "Train Epoch: 5 [311296/359035 (87%)]\tLoss: 0.940328\n",
      "Train Epoch: 5 [312576/359035 (87%)]\tLoss: 0.946768\n",
      "Train Epoch: 5 [313856/359035 (87%)]\tLoss: 0.847070\n",
      "Train Epoch: 5 [315136/359035 (88%)]\tLoss: 0.826533\n",
      "Train Epoch: 5 [316416/359035 (88%)]\tLoss: 1.020867\n",
      "Train Epoch: 5 [317696/359035 (88%)]\tLoss: 0.837768\n",
      "Train Epoch: 5 [318976/359035 (89%)]\tLoss: 0.866357\n",
      "Train Epoch: 5 [320256/359035 (89%)]\tLoss: 0.817498\n",
      "Train Epoch: 5 [321536/359035 (90%)]\tLoss: 0.854619\n",
      "Train Epoch: 5 [322816/359035 (90%)]\tLoss: 0.786233\n",
      "Train Epoch: 5 [324096/359035 (90%)]\tLoss: 0.913990\n",
      "Train Epoch: 5 [325376/359035 (91%)]\tLoss: 0.883553\n",
      "Train Epoch: 5 [326656/359035 (91%)]\tLoss: 0.903454\n",
      "Train Epoch: 5 [327936/359035 (91%)]\tLoss: 0.922794\n",
      "Train Epoch: 5 [329216/359035 (92%)]\tLoss: 0.830241\n",
      "Train Epoch: 5 [330496/359035 (92%)]\tLoss: 0.814156\n",
      "Train Epoch: 5 [331776/359035 (92%)]\tLoss: 0.924450\n",
      "Train Epoch: 5 [333056/359035 (93%)]\tLoss: 0.876686\n",
      "Train Epoch: 5 [334336/359035 (93%)]\tLoss: 0.880677\n",
      "Train Epoch: 5 [335616/359035 (93%)]\tLoss: 0.868769\n",
      "Train Epoch: 5 [336896/359035 (94%)]\tLoss: 0.812282\n",
      "Train Epoch: 5 [338176/359035 (94%)]\tLoss: 0.877482\n",
      "Train Epoch: 5 [339456/359035 (95%)]\tLoss: 0.954631\n",
      "Train Epoch: 5 [340736/359035 (95%)]\tLoss: 0.807915\n",
      "Train Epoch: 5 [342016/359035 (95%)]\tLoss: 0.830575\n",
      "Train Epoch: 5 [343296/359035 (96%)]\tLoss: 0.979297\n",
      "Train Epoch: 5 [344576/359035 (96%)]\tLoss: 0.839058\n",
      "Train Epoch: 5 [345856/359035 (96%)]\tLoss: 0.888424\n",
      "Train Epoch: 5 [347136/359035 (97%)]\tLoss: 0.896419\n",
      "Train Epoch: 5 [348416/359035 (97%)]\tLoss: 0.863299\n",
      "Train Epoch: 5 [349696/359035 (97%)]\tLoss: 0.828822\n",
      "Train Epoch: 5 [350976/359035 (98%)]\tLoss: 0.898467\n",
      "Train Epoch: 5 [352256/359035 (98%)]\tLoss: 0.852808\n",
      "Train Epoch: 5 [353536/359035 (98%)]\tLoss: 0.935580\n",
      "Train Epoch: 5 [354816/359035 (99%)]\tLoss: 0.807274\n",
      "Train Epoch: 5 [356096/359035 (99%)]\tLoss: 0.962710\n",
      "Train Epoch: 5 [357376/359035 (100%)]\tLoss: 0.934578\n",
      "Train Epoch: 5 [358656/359035 (100%)]\tLoss: 0.922063\n",
      "Performance on test set: Average loss: 0.9298, Accuracy: 133/200 (66%)\n",
      "Epoch took: 2570.376s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.5681, 100to500: 0.1956, 20to50: 0.1119, 500to_above: 0.1244\n",
      "\n",
      "Epoch 6\n",
      "Train Epoch: 6 [256/359035 (0%)]\tLoss: 0.907526\n",
      "Train Epoch: 6 [1536/359035 (0%)]\tLoss: 0.787308\n",
      "Train Epoch: 6 [2816/359035 (1%)]\tLoss: 0.785701\n",
      "Train Epoch: 6 [4096/359035 (1%)]\tLoss: 0.859556\n",
      "Train Epoch: 6 [5376/359035 (1%)]\tLoss: 0.839577\n",
      "Train Epoch: 6 [6656/359035 (2%)]\tLoss: 0.906299\n",
      "Train Epoch: 6 [7936/359035 (2%)]\tLoss: 0.797387\n",
      "Train Epoch: 6 [9216/359035 (3%)]\tLoss: 0.843002\n",
      "Train Epoch: 6 [10496/359035 (3%)]\tLoss: 0.879240\n",
      "Train Epoch: 6 [11776/359035 (3%)]\tLoss: 0.912263\n",
      "Train Epoch: 6 [13056/359035 (4%)]\tLoss: 0.928815\n",
      "Train Epoch: 6 [14336/359035 (4%)]\tLoss: 0.951969\n",
      "Train Epoch: 6 [15616/359035 (4%)]\tLoss: 0.788540\n",
      "Train Epoch: 6 [16896/359035 (5%)]\tLoss: 0.870242\n",
      "Train Epoch: 6 [18176/359035 (5%)]\tLoss: 0.843360\n",
      "Train Epoch: 6 [19456/359035 (5%)]\tLoss: 1.032193\n",
      "Train Epoch: 6 [20736/359035 (6%)]\tLoss: 0.926676\n",
      "Train Epoch: 6 [22016/359035 (6%)]\tLoss: 0.933338\n",
      "Train Epoch: 6 [23296/359035 (6%)]\tLoss: 0.926473\n",
      "Train Epoch: 6 [24576/359035 (7%)]\tLoss: 0.915131\n",
      "Train Epoch: 6 [25856/359035 (7%)]\tLoss: 0.992831\n",
      "Train Epoch: 6 [27136/359035 (8%)]\tLoss: 0.892131\n",
      "Train Epoch: 6 [28416/359035 (8%)]\tLoss: 0.904954\n",
      "Train Epoch: 6 [29696/359035 (8%)]\tLoss: 0.866144\n",
      "Train Epoch: 6 [30976/359035 (9%)]\tLoss: 0.835331\n",
      "Train Epoch: 6 [32256/359035 (9%)]\tLoss: 0.931678\n",
      "Train Epoch: 6 [33536/359035 (9%)]\tLoss: 0.844453\n",
      "Train Epoch: 6 [34816/359035 (10%)]\tLoss: 0.798067\n",
      "Train Epoch: 6 [36096/359035 (10%)]\tLoss: 0.817470\n",
      "Train Epoch: 6 [37376/359035 (10%)]\tLoss: 0.929896\n",
      "Train Epoch: 6 [38656/359035 (11%)]\tLoss: 0.868295\n",
      "Train Epoch: 6 [39936/359035 (11%)]\tLoss: 0.780849\n",
      "Train Epoch: 6 [41216/359035 (11%)]\tLoss: 0.883068\n",
      "Train Epoch: 6 [42496/359035 (12%)]\tLoss: 0.864568\n",
      "Train Epoch: 6 [43776/359035 (12%)]\tLoss: 0.911871\n",
      "Train Epoch: 6 [45056/359035 (13%)]\tLoss: 0.911826\n",
      "Train Epoch: 6 [46336/359035 (13%)]\tLoss: 0.890289\n",
      "Train Epoch: 6 [47616/359035 (13%)]\tLoss: 0.797823\n",
      "Train Epoch: 6 [48896/359035 (14%)]\tLoss: 0.762361\n",
      "Train Epoch: 6 [50176/359035 (14%)]\tLoss: 0.940118\n",
      "Train Epoch: 6 [51456/359035 (14%)]\tLoss: 0.832160\n",
      "Train Epoch: 6 [52736/359035 (15%)]\tLoss: 0.865782\n",
      "Train Epoch: 6 [54016/359035 (15%)]\tLoss: 0.974489\n",
      "Train Epoch: 6 [55296/359035 (15%)]\tLoss: 0.871412\n",
      "Train Epoch: 6 [56576/359035 (16%)]\tLoss: 0.845391\n",
      "Train Epoch: 6 [57856/359035 (16%)]\tLoss: 0.805607\n",
      "Train Epoch: 6 [59136/359035 (16%)]\tLoss: 0.835859\n",
      "Train Epoch: 6 [60416/359035 (17%)]\tLoss: 0.882482\n",
      "Train Epoch: 6 [61696/359035 (17%)]\tLoss: 0.930801\n",
      "Train Epoch: 6 [62976/359035 (18%)]\tLoss: 0.817613\n",
      "Train Epoch: 6 [64256/359035 (18%)]\tLoss: 0.821466\n",
      "Train Epoch: 6 [65536/359035 (18%)]\tLoss: 0.863715\n",
      "Train Epoch: 6 [66816/359035 (19%)]\tLoss: 0.832895\n",
      "Train Epoch: 6 [68096/359035 (19%)]\tLoss: 0.887106\n",
      "Train Epoch: 6 [69376/359035 (19%)]\tLoss: 0.811551\n",
      "Train Epoch: 6 [70656/359035 (20%)]\tLoss: 0.763212\n",
      "Train Epoch: 6 [71936/359035 (20%)]\tLoss: 0.905559\n",
      "Train Epoch: 6 [73216/359035 (20%)]\tLoss: 0.852504\n",
      "Train Epoch: 6 [74496/359035 (21%)]\tLoss: 0.816247\n",
      "Train Epoch: 6 [75776/359035 (21%)]\tLoss: 0.894583\n",
      "Train Epoch: 6 [77056/359035 (21%)]\tLoss: 0.912241\n",
      "Train Epoch: 6 [78336/359035 (22%)]\tLoss: 0.851526\n",
      "Train Epoch: 6 [79616/359035 (22%)]\tLoss: 0.852728\n",
      "Train Epoch: 6 [80896/359035 (23%)]\tLoss: 0.902095\n",
      "Train Epoch: 6 [82176/359035 (23%)]\tLoss: 0.868253\n",
      "Train Epoch: 6 [83456/359035 (23%)]\tLoss: 0.916137\n",
      "Train Epoch: 6 [84736/359035 (24%)]\tLoss: 0.983271\n",
      "Train Epoch: 6 [86016/359035 (24%)]\tLoss: 0.913577\n",
      "Train Epoch: 6 [87296/359035 (24%)]\tLoss: 0.936117\n",
      "Train Epoch: 6 [88576/359035 (25%)]\tLoss: 0.837710\n",
      "Train Epoch: 6 [89856/359035 (25%)]\tLoss: 0.826574\n",
      "Train Epoch: 6 [91136/359035 (25%)]\tLoss: 0.858289\n",
      "Train Epoch: 6 [92416/359035 (26%)]\tLoss: 0.879869\n",
      "Train Epoch: 6 [93696/359035 (26%)]\tLoss: 0.986167\n",
      "Train Epoch: 6 [94976/359035 (26%)]\tLoss: 0.886751\n",
      "Train Epoch: 6 [96256/359035 (27%)]\tLoss: 0.848324\n",
      "Train Epoch: 6 [97536/359035 (27%)]\tLoss: 0.813574\n",
      "Train Epoch: 6 [98816/359035 (28%)]\tLoss: 0.926268\n",
      "Train Epoch: 6 [100096/359035 (28%)]\tLoss: 0.803324\n",
      "Train Epoch: 6 [101376/359035 (28%)]\tLoss: 0.920866\n",
      "Train Epoch: 6 [102656/359035 (29%)]\tLoss: 0.840544\n",
      "Train Epoch: 6 [103936/359035 (29%)]\tLoss: 0.849390\n",
      "Train Epoch: 6 [105216/359035 (29%)]\tLoss: 0.835191\n",
      "Train Epoch: 6 [106496/359035 (30%)]\tLoss: 0.919564\n",
      "Train Epoch: 6 [107776/359035 (30%)]\tLoss: 0.849068\n",
      "Train Epoch: 6 [109056/359035 (30%)]\tLoss: 0.848697\n",
      "Train Epoch: 6 [110336/359035 (31%)]\tLoss: 0.851513\n",
      "Train Epoch: 6 [111616/359035 (31%)]\tLoss: 0.878944\n",
      "Train Epoch: 6 [112896/359035 (31%)]\tLoss: 0.836381\n",
      "Train Epoch: 6 [114176/359035 (32%)]\tLoss: 0.898811\n",
      "Train Epoch: 6 [115456/359035 (32%)]\tLoss: 0.819237\n",
      "Train Epoch: 6 [116736/359035 (33%)]\tLoss: 0.879148\n",
      "Train Epoch: 6 [118016/359035 (33%)]\tLoss: 0.884279\n",
      "Train Epoch: 6 [119296/359035 (33%)]\tLoss: 0.880609\n",
      "Train Epoch: 6 [120576/359035 (34%)]\tLoss: 0.857627\n",
      "Train Epoch: 6 [121856/359035 (34%)]\tLoss: 0.872922\n",
      "Train Epoch: 6 [123136/359035 (34%)]\tLoss: 0.830455\n",
      "Train Epoch: 6 [124416/359035 (35%)]\tLoss: 0.903110\n",
      "Train Epoch: 6 [125696/359035 (35%)]\tLoss: 0.905296\n",
      "Train Epoch: 6 [126976/359035 (35%)]\tLoss: 0.879032\n",
      "Train Epoch: 6 [128256/359035 (36%)]\tLoss: 0.806423\n",
      "Train Epoch: 6 [129536/359035 (36%)]\tLoss: 0.865657\n",
      "Train Epoch: 6 [130816/359035 (36%)]\tLoss: 0.839484\n",
      "Train Epoch: 6 [132096/359035 (37%)]\tLoss: 0.921307\n",
      "Train Epoch: 6 [133376/359035 (37%)]\tLoss: 0.894179\n",
      "Train Epoch: 6 [134656/359035 (38%)]\tLoss: 0.911829\n",
      "Train Epoch: 6 [135936/359035 (38%)]\tLoss: 0.751492\n",
      "Train Epoch: 6 [137216/359035 (38%)]\tLoss: 0.887254\n",
      "Train Epoch: 6 [138496/359035 (39%)]\tLoss: 0.907606\n",
      "Train Epoch: 6 [139776/359035 (39%)]\tLoss: 0.824189\n",
      "Train Epoch: 6 [141056/359035 (39%)]\tLoss: 0.859631\n",
      "Train Epoch: 6 [142336/359035 (40%)]\tLoss: 0.941883\n",
      "Train Epoch: 6 [143616/359035 (40%)]\tLoss: 0.903224\n",
      "Train Epoch: 6 [144896/359035 (40%)]\tLoss: 0.935275\n",
      "Train Epoch: 6 [146176/359035 (41%)]\tLoss: 0.902351\n",
      "Train Epoch: 6 [147456/359035 (41%)]\tLoss: 0.842779\n",
      "Train Epoch: 6 [148736/359035 (41%)]\tLoss: 0.892983\n",
      "Train Epoch: 6 [150016/359035 (42%)]\tLoss: 0.915132\n",
      "Train Epoch: 6 [151296/359035 (42%)]\tLoss: 0.907386\n",
      "Train Epoch: 6 [152576/359035 (42%)]\tLoss: 0.822743\n",
      "Train Epoch: 6 [153856/359035 (43%)]\tLoss: 0.834592\n",
      "Train Epoch: 6 [155136/359035 (43%)]\tLoss: 0.866071\n",
      "Train Epoch: 6 [156416/359035 (44%)]\tLoss: 0.843901\n",
      "Train Epoch: 6 [157696/359035 (44%)]\tLoss: 0.865899\n",
      "Train Epoch: 6 [158976/359035 (44%)]\tLoss: 0.850455\n",
      "Train Epoch: 6 [160256/359035 (45%)]\tLoss: 0.736895\n",
      "Train Epoch: 6 [161536/359035 (45%)]\tLoss: 0.887654\n",
      "Train Epoch: 6 [162816/359035 (45%)]\tLoss: 0.933809\n",
      "Train Epoch: 6 [164096/359035 (46%)]\tLoss: 0.922003\n",
      "Train Epoch: 6 [165376/359035 (46%)]\tLoss: 0.928017\n",
      "Train Epoch: 6 [166656/359035 (46%)]\tLoss: 0.776055\n",
      "Train Epoch: 6 [167936/359035 (47%)]\tLoss: 0.807443\n",
      "Train Epoch: 6 [169216/359035 (47%)]\tLoss: 0.863173\n",
      "Train Epoch: 6 [170496/359035 (47%)]\tLoss: 0.805432\n",
      "Train Epoch: 6 [171776/359035 (48%)]\tLoss: 0.894085\n",
      "Train Epoch: 6 [173056/359035 (48%)]\tLoss: 0.829084\n",
      "Train Epoch: 6 [174336/359035 (49%)]\tLoss: 0.812984\n",
      "Train Epoch: 6 [175616/359035 (49%)]\tLoss: 0.787070\n",
      "Train Epoch: 6 [176896/359035 (49%)]\tLoss: 0.910308\n",
      "Train Epoch: 6 [178176/359035 (50%)]\tLoss: 0.867532\n",
      "Train Epoch: 6 [179456/359035 (50%)]\tLoss: 0.923382\n",
      "Train Epoch: 6 [180736/359035 (50%)]\tLoss: 0.922047\n",
      "Train Epoch: 6 [182016/359035 (51%)]\tLoss: 0.862137\n",
      "Train Epoch: 6 [183296/359035 (51%)]\tLoss: 0.946541\n",
      "Train Epoch: 6 [184576/359035 (51%)]\tLoss: 0.897352\n",
      "Train Epoch: 6 [185856/359035 (52%)]\tLoss: 0.877086\n",
      "Train Epoch: 6 [187136/359035 (52%)]\tLoss: 0.912512\n",
      "Train Epoch: 6 [188416/359035 (52%)]\tLoss: 0.844985\n",
      "Train Epoch: 6 [189696/359035 (53%)]\tLoss: 0.770138\n",
      "Train Epoch: 6 [190976/359035 (53%)]\tLoss: 0.960956\n",
      "Train Epoch: 6 [192256/359035 (54%)]\tLoss: 0.835942\n",
      "Train Epoch: 6 [193536/359035 (54%)]\tLoss: 0.947528\n",
      "Train Epoch: 6 [194816/359035 (54%)]\tLoss: 0.917383\n",
      "Train Epoch: 6 [196096/359035 (55%)]\tLoss: 0.898995\n",
      "Train Epoch: 6 [197376/359035 (55%)]\tLoss: 0.830759\n",
      "Train Epoch: 6 [198656/359035 (55%)]\tLoss: 0.828329\n",
      "Train Epoch: 6 [199936/359035 (56%)]\tLoss: 0.919829\n",
      "Train Epoch: 6 [201216/359035 (56%)]\tLoss: 0.905110\n",
      "Train Epoch: 6 [202496/359035 (56%)]\tLoss: 0.905212\n",
      "Train Epoch: 6 [203776/359035 (57%)]\tLoss: 0.792885\n",
      "Train Epoch: 6 [205056/359035 (57%)]\tLoss: 0.925537\n",
      "Train Epoch: 6 [206336/359035 (57%)]\tLoss: 0.843589\n",
      "Train Epoch: 6 [207616/359035 (58%)]\tLoss: 0.878605\n",
      "Train Epoch: 6 [208896/359035 (58%)]\tLoss: 0.893447\n",
      "Train Epoch: 6 [210176/359035 (59%)]\tLoss: 0.847439\n",
      "Train Epoch: 6 [211456/359035 (59%)]\tLoss: 0.946290\n",
      "Train Epoch: 6 [212736/359035 (59%)]\tLoss: 0.922120\n",
      "Train Epoch: 6 [214016/359035 (60%)]\tLoss: 0.819562\n",
      "Train Epoch: 6 [215296/359035 (60%)]\tLoss: 0.825207\n",
      "Train Epoch: 6 [216576/359035 (60%)]\tLoss: 0.821908\n",
      "Train Epoch: 6 [217856/359035 (61%)]\tLoss: 0.893967\n",
      "Train Epoch: 6 [219136/359035 (61%)]\tLoss: 0.844088\n",
      "Train Epoch: 6 [220416/359035 (61%)]\tLoss: 0.841548\n",
      "Train Epoch: 6 [221696/359035 (62%)]\tLoss: 0.870954\n",
      "Train Epoch: 6 [222976/359035 (62%)]\tLoss: 0.876841\n",
      "Train Epoch: 6 [224256/359035 (62%)]\tLoss: 0.916708\n",
      "Train Epoch: 6 [225536/359035 (63%)]\tLoss: 0.803174\n",
      "Train Epoch: 6 [226816/359035 (63%)]\tLoss: 0.957751\n",
      "Train Epoch: 6 [228096/359035 (64%)]\tLoss: 0.809449\n",
      "Train Epoch: 6 [229376/359035 (64%)]\tLoss: 0.821326\n",
      "Train Epoch: 6 [230656/359035 (64%)]\tLoss: 0.837419\n",
      "Train Epoch: 6 [231936/359035 (65%)]\tLoss: 0.796085\n",
      "Train Epoch: 6 [233216/359035 (65%)]\tLoss: 0.860793\n",
      "Train Epoch: 6 [234496/359035 (65%)]\tLoss: 0.817151\n",
      "Train Epoch: 6 [235776/359035 (66%)]\tLoss: 0.954947\n",
      "Train Epoch: 6 [237056/359035 (66%)]\tLoss: 0.852555\n",
      "Train Epoch: 6 [238336/359035 (66%)]\tLoss: 0.832457\n",
      "Train Epoch: 6 [239616/359035 (67%)]\tLoss: 0.821817\n",
      "Train Epoch: 6 [240896/359035 (67%)]\tLoss: 0.821961\n",
      "Train Epoch: 6 [242176/359035 (67%)]\tLoss: 0.751222\n",
      "Train Epoch: 6 [243456/359035 (68%)]\tLoss: 0.793319\n",
      "Train Epoch: 6 [244736/359035 (68%)]\tLoss: 0.905838\n",
      "Train Epoch: 6 [246016/359035 (69%)]\tLoss: 0.857571\n",
      "Train Epoch: 6 [247296/359035 (69%)]\tLoss: 0.862537\n",
      "Train Epoch: 6 [248576/359035 (69%)]\tLoss: 0.807673\n",
      "Train Epoch: 6 [249856/359035 (70%)]\tLoss: 0.954703\n",
      "Train Epoch: 6 [251136/359035 (70%)]\tLoss: 0.912119\n",
      "Train Epoch: 6 [252416/359035 (70%)]\tLoss: 0.919005\n",
      "Train Epoch: 6 [253696/359035 (71%)]\tLoss: 0.806155\n",
      "Train Epoch: 6 [254976/359035 (71%)]\tLoss: 0.880104\n",
      "Train Epoch: 6 [256256/359035 (71%)]\tLoss: 0.903234\n",
      "Train Epoch: 6 [257536/359035 (72%)]\tLoss: 0.886934\n",
      "Train Epoch: 6 [258816/359035 (72%)]\tLoss: 0.908603\n",
      "Train Epoch: 6 [260096/359035 (72%)]\tLoss: 0.853286\n",
      "Train Epoch: 6 [261376/359035 (73%)]\tLoss: 0.936111\n",
      "Train Epoch: 6 [262656/359035 (73%)]\tLoss: 0.854034\n",
      "Train Epoch: 6 [263936/359035 (74%)]\tLoss: 0.866144\n",
      "Train Epoch: 6 [265216/359035 (74%)]\tLoss: 0.903404\n",
      "Train Epoch: 6 [266496/359035 (74%)]\tLoss: 0.822264\n",
      "Train Epoch: 6 [267776/359035 (75%)]\tLoss: 0.785056\n",
      "Train Epoch: 6 [269056/359035 (75%)]\tLoss: 0.812955\n",
      "Train Epoch: 6 [270336/359035 (75%)]\tLoss: 0.997141\n",
      "Train Epoch: 6 [271616/359035 (76%)]\tLoss: 0.928074\n",
      "Train Epoch: 6 [272896/359035 (76%)]\tLoss: 0.839553\n",
      "Train Epoch: 6 [274176/359035 (76%)]\tLoss: 0.890528\n",
      "Train Epoch: 6 [275456/359035 (77%)]\tLoss: 0.837192\n",
      "Train Epoch: 6 [276736/359035 (77%)]\tLoss: 0.885668\n",
      "Train Epoch: 6 [278016/359035 (77%)]\tLoss: 0.825872\n",
      "Train Epoch: 6 [279296/359035 (78%)]\tLoss: 0.954815\n",
      "Train Epoch: 6 [280576/359035 (78%)]\tLoss: 0.788740\n",
      "Train Epoch: 6 [281856/359035 (79%)]\tLoss: 0.886174\n",
      "Train Epoch: 6 [283136/359035 (79%)]\tLoss: 0.797266\n",
      "Train Epoch: 6 [284416/359035 (79%)]\tLoss: 0.906855\n",
      "Train Epoch: 6 [285696/359035 (80%)]\tLoss: 0.846037\n",
      "Train Epoch: 6 [286976/359035 (80%)]\tLoss: 0.980899\n",
      "Train Epoch: 6 [288256/359035 (80%)]\tLoss: 0.891153\n",
      "Train Epoch: 6 [289536/359035 (81%)]\tLoss: 0.866496\n",
      "Train Epoch: 6 [290816/359035 (81%)]\tLoss: 0.854784\n",
      "Train Epoch: 6 [292096/359035 (81%)]\tLoss: 0.814828\n",
      "Train Epoch: 6 [293376/359035 (82%)]\tLoss: 0.974998\n",
      "Train Epoch: 6 [294656/359035 (82%)]\tLoss: 0.967410\n",
      "Train Epoch: 6 [295936/359035 (82%)]\tLoss: 0.909874\n",
      "Train Epoch: 6 [297216/359035 (83%)]\tLoss: 0.903553\n",
      "Train Epoch: 6 [298496/359035 (83%)]\tLoss: 0.915396\n",
      "Train Epoch: 6 [299776/359035 (83%)]\tLoss: 0.799957\n",
      "Train Epoch: 6 [301056/359035 (84%)]\tLoss: 0.914151\n",
      "Train Epoch: 6 [302336/359035 (84%)]\tLoss: 0.830170\n",
      "Train Epoch: 6 [303616/359035 (85%)]\tLoss: 0.830972\n",
      "Train Epoch: 6 [304896/359035 (85%)]\tLoss: 0.824267\n",
      "Train Epoch: 6 [306176/359035 (85%)]\tLoss: 0.887977\n",
      "Train Epoch: 6 [307456/359035 (86%)]\tLoss: 0.918013\n",
      "Train Epoch: 6 [308736/359035 (86%)]\tLoss: 0.882690\n",
      "Train Epoch: 6 [310016/359035 (86%)]\tLoss: 0.949188\n",
      "Train Epoch: 6 [311296/359035 (87%)]\tLoss: 0.917003\n",
      "Train Epoch: 6 [312576/359035 (87%)]\tLoss: 0.791341\n",
      "Train Epoch: 6 [313856/359035 (87%)]\tLoss: 0.901723\n",
      "Train Epoch: 6 [315136/359035 (88%)]\tLoss: 0.882172\n",
      "Train Epoch: 6 [316416/359035 (88%)]\tLoss: 0.851909\n",
      "Train Epoch: 6 [317696/359035 (88%)]\tLoss: 0.925274\n",
      "Train Epoch: 6 [318976/359035 (89%)]\tLoss: 0.864571\n",
      "Train Epoch: 6 [320256/359035 (89%)]\tLoss: 0.794433\n",
      "Train Epoch: 6 [321536/359035 (90%)]\tLoss: 0.964534\n",
      "Train Epoch: 6 [322816/359035 (90%)]\tLoss: 0.911064\n",
      "Train Epoch: 6 [324096/359035 (90%)]\tLoss: 0.893416\n",
      "Train Epoch: 6 [325376/359035 (91%)]\tLoss: 0.793636\n",
      "Train Epoch: 6 [326656/359035 (91%)]\tLoss: 0.901423\n",
      "Train Epoch: 6 [327936/359035 (91%)]\tLoss: 0.895402\n",
      "Train Epoch: 6 [329216/359035 (92%)]\tLoss: 0.911563\n",
      "Train Epoch: 6 [330496/359035 (92%)]\tLoss: 0.967256\n",
      "Train Epoch: 6 [331776/359035 (92%)]\tLoss: 0.867927\n",
      "Train Epoch: 6 [333056/359035 (93%)]\tLoss: 0.806648\n",
      "Train Epoch: 6 [334336/359035 (93%)]\tLoss: 0.841982\n",
      "Train Epoch: 6 [335616/359035 (93%)]\tLoss: 0.816439\n",
      "Train Epoch: 6 [336896/359035 (94%)]\tLoss: 0.784694\n",
      "Train Epoch: 6 [338176/359035 (94%)]\tLoss: 0.921049\n",
      "Train Epoch: 6 [339456/359035 (95%)]\tLoss: 0.821378\n",
      "Train Epoch: 6 [340736/359035 (95%)]\tLoss: 0.916002\n",
      "Train Epoch: 6 [342016/359035 (95%)]\tLoss: 0.971727\n",
      "Train Epoch: 6 [343296/359035 (96%)]\tLoss: 0.882424\n",
      "Train Epoch: 6 [344576/359035 (96%)]\tLoss: 0.734911\n",
      "Train Epoch: 6 [345856/359035 (96%)]\tLoss: 0.918613\n",
      "Train Epoch: 6 [347136/359035 (97%)]\tLoss: 0.906511\n",
      "Train Epoch: 6 [348416/359035 (97%)]\tLoss: 0.952168\n",
      "Train Epoch: 6 [349696/359035 (97%)]\tLoss: 0.900769\n",
      "Train Epoch: 6 [350976/359035 (98%)]\tLoss: 0.862612\n",
      "Train Epoch: 6 [352256/359035 (98%)]\tLoss: 0.904425\n",
      "Train Epoch: 6 [353536/359035 (98%)]\tLoss: 0.783423\n",
      "Train Epoch: 6 [354816/359035 (99%)]\tLoss: 0.793835\n",
      "Train Epoch: 6 [356096/359035 (99%)]\tLoss: 0.956913\n",
      "Train Epoch: 6 [357376/359035 (100%)]\tLoss: 0.889827\n",
      "Train Epoch: 6 [358656/359035 (100%)]\tLoss: 0.744163\n",
      "Performance on test set: Average loss: 0.9242, Accuracy: 134/200 (67%)\n",
      "Epoch took: 2569.630s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.5663, 100to500: 0.1952, 20to50: 0.1157, 500to_above: 0.1229\n",
      "\n",
      "Epoch 7\n",
      "Train Epoch: 7 [256/359035 (0%)]\tLoss: 0.795562\n",
      "Train Epoch: 7 [1536/359035 (0%)]\tLoss: 0.877160\n",
      "Train Epoch: 7 [2816/359035 (1%)]\tLoss: 0.886955\n",
      "Train Epoch: 7 [4096/359035 (1%)]\tLoss: 0.955959\n",
      "Train Epoch: 7 [5376/359035 (1%)]\tLoss: 0.866962\n",
      "Train Epoch: 7 [6656/359035 (2%)]\tLoss: 0.911948\n",
      "Train Epoch: 7 [7936/359035 (2%)]\tLoss: 0.764365\n",
      "Train Epoch: 7 [9216/359035 (3%)]\tLoss: 0.862320\n",
      "Train Epoch: 7 [10496/359035 (3%)]\tLoss: 0.911644\n",
      "Train Epoch: 7 [11776/359035 (3%)]\tLoss: 0.967828\n",
      "Train Epoch: 7 [13056/359035 (4%)]\tLoss: 0.991308\n",
      "Train Epoch: 7 [14336/359035 (4%)]\tLoss: 0.867229\n",
      "Train Epoch: 7 [15616/359035 (4%)]\tLoss: 0.955352\n",
      "Train Epoch: 7 [16896/359035 (5%)]\tLoss: 0.871382\n",
      "Train Epoch: 7 [18176/359035 (5%)]\tLoss: 0.902815\n",
      "Train Epoch: 7 [19456/359035 (5%)]\tLoss: 0.827261\n",
      "Train Epoch: 7 [20736/359035 (6%)]\tLoss: 0.881235\n",
      "Train Epoch: 7 [22016/359035 (6%)]\tLoss: 0.899240\n",
      "Train Epoch: 7 [23296/359035 (6%)]\tLoss: 0.808195\n",
      "Train Epoch: 7 [24576/359035 (7%)]\tLoss: 0.800155\n",
      "Train Epoch: 7 [25856/359035 (7%)]\tLoss: 0.868326\n",
      "Train Epoch: 7 [27136/359035 (8%)]\tLoss: 0.831328\n",
      "Train Epoch: 7 [28416/359035 (8%)]\tLoss: 0.799219\n",
      "Train Epoch: 7 [29696/359035 (8%)]\tLoss: 0.864802\n",
      "Train Epoch: 7 [30976/359035 (9%)]\tLoss: 0.880911\n",
      "Train Epoch: 7 [32256/359035 (9%)]\tLoss: 0.827210\n",
      "Train Epoch: 7 [33536/359035 (9%)]\tLoss: 0.814156\n",
      "Train Epoch: 7 [34816/359035 (10%)]\tLoss: 0.918144\n",
      "Train Epoch: 7 [36096/359035 (10%)]\tLoss: 0.933422\n",
      "Train Epoch: 7 [37376/359035 (10%)]\tLoss: 0.874596\n",
      "Train Epoch: 7 [38656/359035 (11%)]\tLoss: 0.817862\n",
      "Train Epoch: 7 [39936/359035 (11%)]\tLoss: 0.836061\n",
      "Train Epoch: 7 [41216/359035 (11%)]\tLoss: 0.844964\n",
      "Train Epoch: 7 [42496/359035 (12%)]\tLoss: 0.812251\n",
      "Train Epoch: 7 [43776/359035 (12%)]\tLoss: 0.887458\n",
      "Train Epoch: 7 [45056/359035 (13%)]\tLoss: 0.816330\n",
      "Train Epoch: 7 [46336/359035 (13%)]\tLoss: 0.925206\n",
      "Train Epoch: 7 [47616/359035 (13%)]\tLoss: 0.761577\n",
      "Train Epoch: 7 [48896/359035 (14%)]\tLoss: 0.883866\n",
      "Train Epoch: 7 [50176/359035 (14%)]\tLoss: 0.888729\n",
      "Train Epoch: 7 [51456/359035 (14%)]\tLoss: 0.806005\n",
      "Train Epoch: 7 [52736/359035 (15%)]\tLoss: 0.866368\n",
      "Train Epoch: 7 [54016/359035 (15%)]\tLoss: 0.846976\n",
      "Train Epoch: 7 [55296/359035 (15%)]\tLoss: 0.797385\n",
      "Train Epoch: 7 [56576/359035 (16%)]\tLoss: 0.750051\n",
      "Train Epoch: 7 [57856/359035 (16%)]\tLoss: 1.010204\n",
      "Train Epoch: 7 [59136/359035 (16%)]\tLoss: 0.863608\n",
      "Train Epoch: 7 [60416/359035 (17%)]\tLoss: 0.731954\n",
      "Train Epoch: 7 [61696/359035 (17%)]\tLoss: 0.732361\n",
      "Train Epoch: 7 [62976/359035 (18%)]\tLoss: 0.816263\n",
      "Train Epoch: 7 [64256/359035 (18%)]\tLoss: 0.849019\n",
      "Train Epoch: 7 [65536/359035 (18%)]\tLoss: 0.975314\n",
      "Train Epoch: 7 [66816/359035 (19%)]\tLoss: 0.938825\n",
      "Train Epoch: 7 [68096/359035 (19%)]\tLoss: 0.769296\n",
      "Train Epoch: 7 [69376/359035 (19%)]\tLoss: 0.856925\n",
      "Train Epoch: 7 [70656/359035 (20%)]\tLoss: 0.796004\n",
      "Train Epoch: 7 [71936/359035 (20%)]\tLoss: 0.952309\n",
      "Train Epoch: 7 [73216/359035 (20%)]\tLoss: 0.943172\n",
      "Train Epoch: 7 [74496/359035 (21%)]\tLoss: 0.900180\n",
      "Train Epoch: 7 [75776/359035 (21%)]\tLoss: 0.837387\n",
      "Train Epoch: 7 [77056/359035 (21%)]\tLoss: 0.818127\n",
      "Train Epoch: 7 [78336/359035 (22%)]\tLoss: 0.865312\n",
      "Train Epoch: 7 [79616/359035 (22%)]\tLoss: 0.836375\n",
      "Train Epoch: 7 [80896/359035 (23%)]\tLoss: 0.834435\n",
      "Train Epoch: 7 [82176/359035 (23%)]\tLoss: 0.893032\n",
      "Train Epoch: 7 [83456/359035 (23%)]\tLoss: 0.904198\n",
      "Train Epoch: 7 [84736/359035 (24%)]\tLoss: 0.912952\n",
      "Train Epoch: 7 [86016/359035 (24%)]\tLoss: 0.789640\n",
      "Train Epoch: 7 [87296/359035 (24%)]\tLoss: 0.785955\n",
      "Train Epoch: 7 [88576/359035 (25%)]\tLoss: 0.857848\n",
      "Train Epoch: 7 [89856/359035 (25%)]\tLoss: 0.913547\n",
      "Train Epoch: 7 [91136/359035 (25%)]\tLoss: 0.818375\n",
      "Train Epoch: 7 [92416/359035 (26%)]\tLoss: 0.931427\n",
      "Train Epoch: 7 [93696/359035 (26%)]\tLoss: 0.915495\n",
      "Train Epoch: 7 [94976/359035 (26%)]\tLoss: 0.904896\n",
      "Train Epoch: 7 [96256/359035 (27%)]\tLoss: 0.918938\n",
      "Train Epoch: 7 [97536/359035 (27%)]\tLoss: 0.875186\n",
      "Train Epoch: 7 [98816/359035 (28%)]\tLoss: 0.974699\n",
      "Train Epoch: 7 [100096/359035 (28%)]\tLoss: 0.867070\n",
      "Train Epoch: 7 [101376/359035 (28%)]\tLoss: 0.876792\n",
      "Train Epoch: 7 [102656/359035 (29%)]\tLoss: 0.893386\n",
      "Train Epoch: 7 [103936/359035 (29%)]\tLoss: 0.884067\n",
      "Train Epoch: 7 [105216/359035 (29%)]\tLoss: 0.809050\n",
      "Train Epoch: 7 [106496/359035 (30%)]\tLoss: 0.862598\n",
      "Train Epoch: 7 [107776/359035 (30%)]\tLoss: 0.809040\n",
      "Train Epoch: 7 [109056/359035 (30%)]\tLoss: 0.804609\n",
      "Train Epoch: 7 [110336/359035 (31%)]\tLoss: 0.878046\n",
      "Train Epoch: 7 [111616/359035 (31%)]\tLoss: 0.821072\n",
      "Train Epoch: 7 [112896/359035 (31%)]\tLoss: 0.918719\n",
      "Train Epoch: 7 [114176/359035 (32%)]\tLoss: 0.915768\n",
      "Train Epoch: 7 [115456/359035 (32%)]\tLoss: 0.955868\n",
      "Train Epoch: 7 [116736/359035 (33%)]\tLoss: 0.905417\n",
      "Train Epoch: 7 [118016/359035 (33%)]\tLoss: 0.929000\n",
      "Train Epoch: 7 [119296/359035 (33%)]\tLoss: 0.818771\n",
      "Train Epoch: 7 [120576/359035 (34%)]\tLoss: 0.856551\n",
      "Train Epoch: 7 [121856/359035 (34%)]\tLoss: 0.935007\n",
      "Train Epoch: 7 [123136/359035 (34%)]\tLoss: 0.915607\n",
      "Train Epoch: 7 [124416/359035 (35%)]\tLoss: 0.821038\n",
      "Train Epoch: 7 [125696/359035 (35%)]\tLoss: 0.852280\n",
      "Train Epoch: 7 [126976/359035 (35%)]\tLoss: 0.815282\n",
      "Train Epoch: 7 [128256/359035 (36%)]\tLoss: 0.867638\n",
      "Train Epoch: 7 [129536/359035 (36%)]\tLoss: 0.826182\n",
      "Train Epoch: 7 [130816/359035 (36%)]\tLoss: 0.874168\n",
      "Train Epoch: 7 [132096/359035 (37%)]\tLoss: 0.951540\n",
      "Train Epoch: 7 [133376/359035 (37%)]\tLoss: 0.901118\n",
      "Train Epoch: 7 [134656/359035 (38%)]\tLoss: 0.848390\n",
      "Train Epoch: 7 [135936/359035 (38%)]\tLoss: 0.827217\n",
      "Train Epoch: 7 [137216/359035 (38%)]\tLoss: 0.957571\n",
      "Train Epoch: 7 [138496/359035 (39%)]\tLoss: 0.859819\n",
      "Train Epoch: 7 [139776/359035 (39%)]\tLoss: 0.885968\n",
      "Train Epoch: 7 [141056/359035 (39%)]\tLoss: 0.826313\n",
      "Train Epoch: 7 [142336/359035 (40%)]\tLoss: 0.914109\n",
      "Train Epoch: 7 [143616/359035 (40%)]\tLoss: 0.871432\n",
      "Train Epoch: 7 [144896/359035 (40%)]\tLoss: 0.791338\n",
      "Train Epoch: 7 [146176/359035 (41%)]\tLoss: 0.895427\n",
      "Train Epoch: 7 [147456/359035 (41%)]\tLoss: 0.933893\n",
      "Train Epoch: 7 [148736/359035 (41%)]\tLoss: 0.776776\n",
      "Train Epoch: 7 [150016/359035 (42%)]\tLoss: 0.843512\n",
      "Train Epoch: 7 [151296/359035 (42%)]\tLoss: 0.770029\n",
      "Train Epoch: 7 [152576/359035 (42%)]\tLoss: 0.853303\n",
      "Train Epoch: 7 [153856/359035 (43%)]\tLoss: 0.943656\n",
      "Train Epoch: 7 [155136/359035 (43%)]\tLoss: 0.793561\n",
      "Train Epoch: 7 [156416/359035 (44%)]\tLoss: 0.988916\n",
      "Train Epoch: 7 [157696/359035 (44%)]\tLoss: 1.016565\n",
      "Train Epoch: 7 [158976/359035 (44%)]\tLoss: 0.857656\n",
      "Train Epoch: 7 [160256/359035 (45%)]\tLoss: 0.931869\n",
      "Train Epoch: 7 [161536/359035 (45%)]\tLoss: 0.879637\n",
      "Train Epoch: 7 [162816/359035 (45%)]\tLoss: 0.765749\n",
      "Train Epoch: 7 [164096/359035 (46%)]\tLoss: 0.832221\n",
      "Train Epoch: 7 [165376/359035 (46%)]\tLoss: 0.911361\n",
      "Train Epoch: 7 [166656/359035 (46%)]\tLoss: 0.814404\n",
      "Train Epoch: 7 [167936/359035 (47%)]\tLoss: 0.864615\n",
      "Train Epoch: 7 [169216/359035 (47%)]\tLoss: 0.787112\n",
      "Train Epoch: 7 [170496/359035 (47%)]\tLoss: 0.797454\n",
      "Train Epoch: 7 [171776/359035 (48%)]\tLoss: 0.829690\n",
      "Train Epoch: 7 [173056/359035 (48%)]\tLoss: 0.813749\n",
      "Train Epoch: 7 [174336/359035 (49%)]\tLoss: 0.836266\n",
      "Train Epoch: 7 [175616/359035 (49%)]\tLoss: 0.870308\n",
      "Train Epoch: 7 [176896/359035 (49%)]\tLoss: 0.802057\n",
      "Train Epoch: 7 [178176/359035 (50%)]\tLoss: 0.851685\n",
      "Train Epoch: 7 [179456/359035 (50%)]\tLoss: 0.786944\n",
      "Train Epoch: 7 [180736/359035 (50%)]\tLoss: 0.848850\n",
      "Train Epoch: 7 [182016/359035 (51%)]\tLoss: 0.936944\n",
      "Train Epoch: 7 [183296/359035 (51%)]\tLoss: 0.811587\n",
      "Train Epoch: 7 [184576/359035 (51%)]\tLoss: 0.933033\n",
      "Train Epoch: 7 [185856/359035 (52%)]\tLoss: 0.891856\n",
      "Train Epoch: 7 [187136/359035 (52%)]\tLoss: 0.918500\n",
      "Train Epoch: 7 [188416/359035 (52%)]\tLoss: 0.899586\n",
      "Train Epoch: 7 [189696/359035 (53%)]\tLoss: 0.841513\n",
      "Train Epoch: 7 [190976/359035 (53%)]\tLoss: 0.921747\n",
      "Train Epoch: 7 [192256/359035 (54%)]\tLoss: 0.818441\n",
      "Train Epoch: 7 [193536/359035 (54%)]\tLoss: 0.886459\n",
      "Train Epoch: 7 [194816/359035 (54%)]\tLoss: 0.853845\n",
      "Train Epoch: 7 [196096/359035 (55%)]\tLoss: 0.897751\n",
      "Train Epoch: 7 [197376/359035 (55%)]\tLoss: 0.883534\n",
      "Train Epoch: 7 [198656/359035 (55%)]\tLoss: 0.823087\n",
      "Train Epoch: 7 [199936/359035 (56%)]\tLoss: 0.865541\n",
      "Train Epoch: 7 [201216/359035 (56%)]\tLoss: 0.919685\n",
      "Train Epoch: 7 [202496/359035 (56%)]\tLoss: 0.885295\n",
      "Train Epoch: 7 [203776/359035 (57%)]\tLoss: 0.967405\n",
      "Train Epoch: 7 [205056/359035 (57%)]\tLoss: 0.852361\n",
      "Train Epoch: 7 [206336/359035 (57%)]\tLoss: 0.942016\n",
      "Train Epoch: 7 [207616/359035 (58%)]\tLoss: 0.903638\n",
      "Train Epoch: 7 [208896/359035 (58%)]\tLoss: 0.884861\n",
      "Train Epoch: 7 [210176/359035 (59%)]\tLoss: 0.745797\n",
      "Train Epoch: 7 [211456/359035 (59%)]\tLoss: 0.851516\n",
      "Train Epoch: 7 [212736/359035 (59%)]\tLoss: 0.860463\n",
      "Train Epoch: 7 [214016/359035 (60%)]\tLoss: 0.872282\n",
      "Train Epoch: 7 [215296/359035 (60%)]\tLoss: 0.840549\n",
      "Train Epoch: 7 [216576/359035 (60%)]\tLoss: 0.711640\n",
      "Train Epoch: 7 [217856/359035 (61%)]\tLoss: 0.788237\n",
      "Train Epoch: 7 [219136/359035 (61%)]\tLoss: 0.898881\n",
      "Train Epoch: 7 [220416/359035 (61%)]\tLoss: 0.850134\n",
      "Train Epoch: 7 [221696/359035 (62%)]\tLoss: 0.694863\n",
      "Train Epoch: 7 [222976/359035 (62%)]\tLoss: 0.874407\n",
      "Train Epoch: 7 [224256/359035 (62%)]\tLoss: 0.786666\n",
      "Train Epoch: 7 [225536/359035 (63%)]\tLoss: 0.887726\n",
      "Train Epoch: 7 [226816/359035 (63%)]\tLoss: 0.913729\n",
      "Train Epoch: 7 [228096/359035 (64%)]\tLoss: 0.773006\n",
      "Train Epoch: 7 [229376/359035 (64%)]\tLoss: 0.993802\n",
      "Train Epoch: 7 [230656/359035 (64%)]\tLoss: 0.881890\n",
      "Train Epoch: 7 [231936/359035 (65%)]\tLoss: 0.778773\n",
      "Train Epoch: 7 [233216/359035 (65%)]\tLoss: 0.858194\n",
      "Train Epoch: 7 [234496/359035 (65%)]\tLoss: 0.891675\n",
      "Train Epoch: 7 [235776/359035 (66%)]\tLoss: 0.863524\n",
      "Train Epoch: 7 [237056/359035 (66%)]\tLoss: 0.890962\n",
      "Train Epoch: 7 [238336/359035 (66%)]\tLoss: 0.878689\n",
      "Train Epoch: 7 [239616/359035 (67%)]\tLoss: 0.930369\n",
      "Train Epoch: 7 [240896/359035 (67%)]\tLoss: 0.830593\n",
      "Train Epoch: 7 [242176/359035 (67%)]\tLoss: 0.835395\n",
      "Train Epoch: 7 [243456/359035 (68%)]\tLoss: 0.779367\n",
      "Train Epoch: 7 [244736/359035 (68%)]\tLoss: 0.870707\n",
      "Train Epoch: 7 [246016/359035 (69%)]\tLoss: 0.945700\n",
      "Train Epoch: 7 [247296/359035 (69%)]\tLoss: 0.861927\n",
      "Train Epoch: 7 [248576/359035 (69%)]\tLoss: 0.872626\n",
      "Train Epoch: 7 [249856/359035 (70%)]\tLoss: 0.861581\n",
      "Train Epoch: 7 [251136/359035 (70%)]\tLoss: 0.812887\n",
      "Train Epoch: 7 [252416/359035 (70%)]\tLoss: 0.883654\n",
      "Train Epoch: 7 [253696/359035 (71%)]\tLoss: 0.887208\n",
      "Train Epoch: 7 [254976/359035 (71%)]\tLoss: 0.929077\n",
      "Train Epoch: 7 [256256/359035 (71%)]\tLoss: 0.868293\n",
      "Train Epoch: 7 [257536/359035 (72%)]\tLoss: 0.849791\n",
      "Train Epoch: 7 [258816/359035 (72%)]\tLoss: 0.932886\n",
      "Train Epoch: 7 [260096/359035 (72%)]\tLoss: 0.975514\n",
      "Train Epoch: 7 [261376/359035 (73%)]\tLoss: 0.903412\n",
      "Train Epoch: 7 [262656/359035 (73%)]\tLoss: 0.936820\n",
      "Train Epoch: 7 [263936/359035 (74%)]\tLoss: 0.912649\n",
      "Train Epoch: 7 [265216/359035 (74%)]\tLoss: 0.806734\n",
      "Train Epoch: 7 [266496/359035 (74%)]\tLoss: 0.892432\n",
      "Train Epoch: 7 [267776/359035 (75%)]\tLoss: 0.937253\n",
      "Train Epoch: 7 [269056/359035 (75%)]\tLoss: 0.890542\n",
      "Train Epoch: 7 [270336/359035 (75%)]\tLoss: 0.939362\n",
      "Train Epoch: 7 [271616/359035 (76%)]\tLoss: 0.875116\n",
      "Train Epoch: 7 [272896/359035 (76%)]\tLoss: 0.843094\n",
      "Train Epoch: 7 [274176/359035 (76%)]\tLoss: 0.846252\n",
      "Train Epoch: 7 [275456/359035 (77%)]\tLoss: 0.819534\n",
      "Train Epoch: 7 [276736/359035 (77%)]\tLoss: 0.839452\n",
      "Train Epoch: 7 [278016/359035 (77%)]\tLoss: 0.798885\n",
      "Train Epoch: 7 [279296/359035 (78%)]\tLoss: 0.861591\n",
      "Train Epoch: 7 [280576/359035 (78%)]\tLoss: 0.803290\n",
      "Train Epoch: 7 [281856/359035 (79%)]\tLoss: 0.912233\n",
      "Train Epoch: 7 [283136/359035 (79%)]\tLoss: 0.904640\n",
      "Train Epoch: 7 [284416/359035 (79%)]\tLoss: 0.819438\n",
      "Train Epoch: 7 [285696/359035 (80%)]\tLoss: 0.797177\n",
      "Train Epoch: 7 [286976/359035 (80%)]\tLoss: 0.772258\n",
      "Train Epoch: 7 [288256/359035 (80%)]\tLoss: 0.859098\n",
      "Train Epoch: 7 [289536/359035 (81%)]\tLoss: 0.865583\n",
      "Train Epoch: 7 [290816/359035 (81%)]\tLoss: 0.925967\n",
      "Train Epoch: 7 [292096/359035 (81%)]\tLoss: 0.785150\n",
      "Train Epoch: 7 [293376/359035 (82%)]\tLoss: 0.850787\n",
      "Train Epoch: 7 [294656/359035 (82%)]\tLoss: 0.943506\n",
      "Train Epoch: 7 [295936/359035 (82%)]\tLoss: 0.838220\n",
      "Train Epoch: 7 [297216/359035 (83%)]\tLoss: 0.871972\n",
      "Train Epoch: 7 [298496/359035 (83%)]\tLoss: 0.948971\n",
      "Train Epoch: 7 [299776/359035 (83%)]\tLoss: 0.905350\n",
      "Train Epoch: 7 [301056/359035 (84%)]\tLoss: 0.887436\n",
      "Train Epoch: 7 [302336/359035 (84%)]\tLoss: 0.860493\n",
      "Train Epoch: 7 [303616/359035 (85%)]\tLoss: 0.935515\n",
      "Train Epoch: 7 [304896/359035 (85%)]\tLoss: 0.918679\n",
      "Train Epoch: 7 [306176/359035 (85%)]\tLoss: 0.879474\n",
      "Train Epoch: 7 [307456/359035 (86%)]\tLoss: 0.846272\n",
      "Train Epoch: 7 [308736/359035 (86%)]\tLoss: 0.799580\n",
      "Train Epoch: 7 [310016/359035 (86%)]\tLoss: 0.891823\n",
      "Train Epoch: 7 [311296/359035 (87%)]\tLoss: 0.791336\n",
      "Train Epoch: 7 [312576/359035 (87%)]\tLoss: 0.824965\n",
      "Train Epoch: 7 [313856/359035 (87%)]\tLoss: 0.921732\n",
      "Train Epoch: 7 [315136/359035 (88%)]\tLoss: 0.873002\n",
      "Train Epoch: 7 [316416/359035 (88%)]\tLoss: 0.936411\n",
      "Train Epoch: 7 [317696/359035 (88%)]\tLoss: 0.793390\n",
      "Train Epoch: 7 [318976/359035 (89%)]\tLoss: 0.810255\n",
      "Train Epoch: 7 [320256/359035 (89%)]\tLoss: 0.871754\n",
      "Train Epoch: 7 [321536/359035 (90%)]\tLoss: 0.882519\n",
      "Train Epoch: 7 [322816/359035 (90%)]\tLoss: 0.896349\n",
      "Train Epoch: 7 [324096/359035 (90%)]\tLoss: 0.971571\n",
      "Train Epoch: 7 [325376/359035 (91%)]\tLoss: 0.940572\n",
      "Train Epoch: 7 [326656/359035 (91%)]\tLoss: 0.893282\n",
      "Train Epoch: 7 [327936/359035 (91%)]\tLoss: 0.771749\n",
      "Train Epoch: 7 [329216/359035 (92%)]\tLoss: 0.872205\n",
      "Train Epoch: 7 [330496/359035 (92%)]\tLoss: 0.924604\n",
      "Train Epoch: 7 [331776/359035 (92%)]\tLoss: 0.881574\n",
      "Train Epoch: 7 [333056/359035 (93%)]\tLoss: 0.795326\n",
      "Train Epoch: 7 [334336/359035 (93%)]\tLoss: 0.883973\n",
      "Train Epoch: 7 [335616/359035 (93%)]\tLoss: 0.862083\n",
      "Train Epoch: 7 [336896/359035 (94%)]\tLoss: 0.872228\n",
      "Train Epoch: 7 [338176/359035 (94%)]\tLoss: 0.864982\n",
      "Train Epoch: 7 [339456/359035 (95%)]\tLoss: 0.806433\n",
      "Train Epoch: 7 [340736/359035 (95%)]\tLoss: 0.832409\n",
      "Train Epoch: 7 [342016/359035 (95%)]\tLoss: 0.789716\n",
      "Train Epoch: 7 [343296/359035 (96%)]\tLoss: 0.925304\n",
      "Train Epoch: 7 [344576/359035 (96%)]\tLoss: 0.884921\n",
      "Train Epoch: 7 [345856/359035 (96%)]\tLoss: 0.934554\n",
      "Train Epoch: 7 [347136/359035 (97%)]\tLoss: 0.794487\n",
      "Train Epoch: 7 [348416/359035 (97%)]\tLoss: 0.810150\n",
      "Train Epoch: 7 [349696/359035 (97%)]\tLoss: 0.951484\n",
      "Train Epoch: 7 [350976/359035 (98%)]\tLoss: 0.873814\n",
      "Train Epoch: 7 [352256/359035 (98%)]\tLoss: 0.855020\n",
      "Train Epoch: 7 [353536/359035 (98%)]\tLoss: 0.942460\n",
      "Train Epoch: 7 [354816/359035 (99%)]\tLoss: 0.936262\n",
      "Train Epoch: 7 [356096/359035 (99%)]\tLoss: 0.872189\n",
      "Train Epoch: 7 [357376/359035 (100%)]\tLoss: 0.907133\n",
      "Train Epoch: 7 [358656/359035 (100%)]\tLoss: 0.886015\n",
      "Performance on test set: Average loss: 0.9243, Accuracy: 134/200 (67%)\n",
      "Epoch took: 2569.980s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.5721, 100to500: 0.1849, 20to50: 0.1242, 500to_above: 0.1189\n",
      "\n",
      "Epoch 8\n",
      "Train Epoch: 8 [256/359035 (0%)]\tLoss: 0.860786\n",
      "Train Epoch: 8 [1536/359035 (0%)]\tLoss: 0.795993\n",
      "Train Epoch: 8 [2816/359035 (1%)]\tLoss: 0.834796\n",
      "Train Epoch: 8 [4096/359035 (1%)]\tLoss: 0.806362\n",
      "Train Epoch: 8 [5376/359035 (1%)]\tLoss: 0.923943\n",
      "Train Epoch: 8 [6656/359035 (2%)]\tLoss: 0.825238\n",
      "Train Epoch: 8 [7936/359035 (2%)]\tLoss: 0.870841\n",
      "Train Epoch: 8 [9216/359035 (3%)]\tLoss: 0.899606\n",
      "Train Epoch: 8 [10496/359035 (3%)]\tLoss: 0.791044\n",
      "Train Epoch: 8 [11776/359035 (3%)]\tLoss: 0.852953\n",
      "Train Epoch: 8 [13056/359035 (4%)]\tLoss: 0.845709\n",
      "Train Epoch: 8 [14336/359035 (4%)]\tLoss: 0.843109\n",
      "Train Epoch: 8 [15616/359035 (4%)]\tLoss: 0.990608\n",
      "Train Epoch: 8 [16896/359035 (5%)]\tLoss: 0.864540\n",
      "Train Epoch: 8 [18176/359035 (5%)]\tLoss: 0.888682\n",
      "Train Epoch: 8 [19456/359035 (5%)]\tLoss: 0.874390\n",
      "Train Epoch: 8 [20736/359035 (6%)]\tLoss: 0.860409\n",
      "Train Epoch: 8 [22016/359035 (6%)]\tLoss: 0.848197\n",
      "Train Epoch: 8 [23296/359035 (6%)]\tLoss: 0.890483\n",
      "Train Epoch: 8 [24576/359035 (7%)]\tLoss: 0.914486\n",
      "Train Epoch: 8 [25856/359035 (7%)]\tLoss: 0.811655\n",
      "Train Epoch: 8 [27136/359035 (8%)]\tLoss: 0.888915\n",
      "Train Epoch: 8 [28416/359035 (8%)]\tLoss: 0.808128\n",
      "Train Epoch: 8 [29696/359035 (8%)]\tLoss: 0.927588\n",
      "Train Epoch: 8 [30976/359035 (9%)]\tLoss: 0.951901\n",
      "Train Epoch: 8 [32256/359035 (9%)]\tLoss: 0.827051\n",
      "Train Epoch: 8 [33536/359035 (9%)]\tLoss: 0.803656\n",
      "Train Epoch: 8 [34816/359035 (10%)]\tLoss: 0.877145\n",
      "Train Epoch: 8 [36096/359035 (10%)]\tLoss: 0.784139\n",
      "Train Epoch: 8 [37376/359035 (10%)]\tLoss: 0.953536\n",
      "Train Epoch: 8 [38656/359035 (11%)]\tLoss: 0.831423\n",
      "Train Epoch: 8 [39936/359035 (11%)]\tLoss: 0.897582\n",
      "Train Epoch: 8 [41216/359035 (11%)]\tLoss: 0.854675\n",
      "Train Epoch: 8 [42496/359035 (12%)]\tLoss: 0.969600\n",
      "Train Epoch: 8 [43776/359035 (12%)]\tLoss: 0.829430\n",
      "Train Epoch: 8 [45056/359035 (13%)]\tLoss: 0.816024\n",
      "Train Epoch: 8 [46336/359035 (13%)]\tLoss: 0.874740\n",
      "Train Epoch: 8 [47616/359035 (13%)]\tLoss: 0.794519\n",
      "Train Epoch: 8 [48896/359035 (14%)]\tLoss: 0.919179\n",
      "Train Epoch: 8 [50176/359035 (14%)]\tLoss: 0.856210\n",
      "Train Epoch: 8 [51456/359035 (14%)]\tLoss: 0.861314\n",
      "Train Epoch: 8 [52736/359035 (15%)]\tLoss: 0.815759\n",
      "Train Epoch: 8 [54016/359035 (15%)]\tLoss: 0.927366\n",
      "Train Epoch: 8 [55296/359035 (15%)]\tLoss: 0.866392\n",
      "Train Epoch: 8 [56576/359035 (16%)]\tLoss: 0.847943\n",
      "Train Epoch: 8 [57856/359035 (16%)]\tLoss: 0.908094\n",
      "Train Epoch: 8 [59136/359035 (16%)]\tLoss: 0.880783\n",
      "Train Epoch: 8 [60416/359035 (17%)]\tLoss: 0.954890\n",
      "Train Epoch: 8 [61696/359035 (17%)]\tLoss: 0.911902\n",
      "Train Epoch: 8 [62976/359035 (18%)]\tLoss: 0.788220\n",
      "Train Epoch: 8 [64256/359035 (18%)]\tLoss: 0.828922\n",
      "Train Epoch: 8 [65536/359035 (18%)]\tLoss: 0.895990\n",
      "Train Epoch: 8 [66816/359035 (19%)]\tLoss: 0.823471\n",
      "Train Epoch: 8 [68096/359035 (19%)]\tLoss: 0.919243\n",
      "Train Epoch: 8 [69376/359035 (19%)]\tLoss: 0.926123\n",
      "Train Epoch: 8 [70656/359035 (20%)]\tLoss: 0.813774\n",
      "Train Epoch: 8 [71936/359035 (20%)]\tLoss: 0.834635\n",
      "Train Epoch: 8 [73216/359035 (20%)]\tLoss: 0.888938\n",
      "Train Epoch: 8 [74496/359035 (21%)]\tLoss: 0.813062\n",
      "Train Epoch: 8 [75776/359035 (21%)]\tLoss: 0.831081\n",
      "Train Epoch: 8 [77056/359035 (21%)]\tLoss: 0.784642\n",
      "Train Epoch: 8 [78336/359035 (22%)]\tLoss: 0.817595\n",
      "Train Epoch: 8 [79616/359035 (22%)]\tLoss: 0.840108\n",
      "Train Epoch: 8 [80896/359035 (23%)]\tLoss: 0.878367\n",
      "Train Epoch: 8 [82176/359035 (23%)]\tLoss: 0.898110\n",
      "Train Epoch: 8 [83456/359035 (23%)]\tLoss: 0.760042\n",
      "Train Epoch: 8 [84736/359035 (24%)]\tLoss: 0.911772\n",
      "Train Epoch: 8 [86016/359035 (24%)]\tLoss: 0.918166\n",
      "Train Epoch: 8 [87296/359035 (24%)]\tLoss: 0.877315\n",
      "Train Epoch: 8 [88576/359035 (25%)]\tLoss: 0.889781\n",
      "Train Epoch: 8 [89856/359035 (25%)]\tLoss: 0.766264\n",
      "Train Epoch: 8 [91136/359035 (25%)]\tLoss: 0.908335\n",
      "Train Epoch: 8 [92416/359035 (26%)]\tLoss: 0.865273\n",
      "Train Epoch: 8 [93696/359035 (26%)]\tLoss: 0.931440\n",
      "Train Epoch: 8 [94976/359035 (26%)]\tLoss: 0.845626\n",
      "Train Epoch: 8 [96256/359035 (27%)]\tLoss: 0.841565\n",
      "Train Epoch: 8 [97536/359035 (27%)]\tLoss: 0.838450\n",
      "Train Epoch: 8 [98816/359035 (28%)]\tLoss: 0.841026\n",
      "Train Epoch: 8 [100096/359035 (28%)]\tLoss: 0.944743\n",
      "Train Epoch: 8 [101376/359035 (28%)]\tLoss: 0.879871\n",
      "Train Epoch: 8 [102656/359035 (29%)]\tLoss: 0.885719\n",
      "Train Epoch: 8 [103936/359035 (29%)]\tLoss: 0.903794\n",
      "Train Epoch: 8 [105216/359035 (29%)]\tLoss: 0.685534\n",
      "Train Epoch: 8 [106496/359035 (30%)]\tLoss: 0.838984\n",
      "Train Epoch: 8 [107776/359035 (30%)]\tLoss: 0.805429\n",
      "Train Epoch: 8 [109056/359035 (30%)]\tLoss: 0.819885\n",
      "Train Epoch: 8 [110336/359035 (31%)]\tLoss: 0.915135\n",
      "Train Epoch: 8 [111616/359035 (31%)]\tLoss: 0.937512\n",
      "Train Epoch: 8 [112896/359035 (31%)]\tLoss: 0.922074\n",
      "Train Epoch: 8 [114176/359035 (32%)]\tLoss: 0.869549\n",
      "Train Epoch: 8 [115456/359035 (32%)]\tLoss: 0.843428\n",
      "Train Epoch: 8 [116736/359035 (33%)]\tLoss: 0.953040\n",
      "Train Epoch: 8 [118016/359035 (33%)]\tLoss: 0.906029\n",
      "Train Epoch: 8 [119296/359035 (33%)]\tLoss: 0.846244\n",
      "Train Epoch: 8 [120576/359035 (34%)]\tLoss: 0.932456\n",
      "Train Epoch: 8 [121856/359035 (34%)]\tLoss: 0.886901\n",
      "Train Epoch: 8 [123136/359035 (34%)]\tLoss: 0.825246\n",
      "Train Epoch: 8 [124416/359035 (35%)]\tLoss: 0.842677\n",
      "Train Epoch: 8 [125696/359035 (35%)]\tLoss: 0.852992\n",
      "Train Epoch: 8 [126976/359035 (35%)]\tLoss: 0.878135\n",
      "Train Epoch: 8 [128256/359035 (36%)]\tLoss: 0.851441\n",
      "Train Epoch: 8 [129536/359035 (36%)]\tLoss: 1.008945\n",
      "Train Epoch: 8 [130816/359035 (36%)]\tLoss: 0.813503\n",
      "Train Epoch: 8 [132096/359035 (37%)]\tLoss: 0.884347\n",
      "Train Epoch: 8 [133376/359035 (37%)]\tLoss: 0.858857\n",
      "Train Epoch: 8 [134656/359035 (38%)]\tLoss: 0.881895\n",
      "Train Epoch: 8 [135936/359035 (38%)]\tLoss: 0.846369\n",
      "Train Epoch: 8 [137216/359035 (38%)]\tLoss: 0.770960\n",
      "Train Epoch: 8 [138496/359035 (39%)]\tLoss: 0.869365\n",
      "Train Epoch: 8 [139776/359035 (39%)]\tLoss: 0.939094\n",
      "Train Epoch: 8 [141056/359035 (39%)]\tLoss: 0.841339\n",
      "Train Epoch: 8 [142336/359035 (40%)]\tLoss: 0.771180\n",
      "Train Epoch: 8 [143616/359035 (40%)]\tLoss: 0.797819\n",
      "Train Epoch: 8 [144896/359035 (40%)]\tLoss: 0.772951\n",
      "Train Epoch: 8 [146176/359035 (41%)]\tLoss: 0.941490\n",
      "Train Epoch: 8 [147456/359035 (41%)]\tLoss: 0.805907\n",
      "Train Epoch: 8 [148736/359035 (41%)]\tLoss: 0.802915\n",
      "Train Epoch: 8 [150016/359035 (42%)]\tLoss: 0.814943\n",
      "Train Epoch: 8 [151296/359035 (42%)]\tLoss: 0.903187\n",
      "Train Epoch: 8 [152576/359035 (42%)]\tLoss: 0.858364\n",
      "Train Epoch: 8 [153856/359035 (43%)]\tLoss: 0.904204\n",
      "Train Epoch: 8 [155136/359035 (43%)]\tLoss: 0.894009\n",
      "Train Epoch: 8 [156416/359035 (44%)]\tLoss: 0.917677\n",
      "Train Epoch: 8 [157696/359035 (44%)]\tLoss: 0.881011\n",
      "Train Epoch: 8 [158976/359035 (44%)]\tLoss: 0.893014\n",
      "Train Epoch: 8 [160256/359035 (45%)]\tLoss: 0.963854\n",
      "Train Epoch: 8 [161536/359035 (45%)]\tLoss: 0.985662\n",
      "Train Epoch: 8 [162816/359035 (45%)]\tLoss: 0.892195\n",
      "Train Epoch: 8 [164096/359035 (46%)]\tLoss: 0.797760\n",
      "Train Epoch: 8 [165376/359035 (46%)]\tLoss: 0.802014\n",
      "Train Epoch: 8 [166656/359035 (46%)]\tLoss: 0.867499\n",
      "Train Epoch: 8 [167936/359035 (47%)]\tLoss: 0.942418\n",
      "Train Epoch: 8 [169216/359035 (47%)]\tLoss: 0.831840\n",
      "Train Epoch: 8 [170496/359035 (47%)]\tLoss: 0.746698\n",
      "Train Epoch: 8 [171776/359035 (48%)]\tLoss: 0.877784\n",
      "Train Epoch: 8 [173056/359035 (48%)]\tLoss: 0.728464\n",
      "Train Epoch: 8 [174336/359035 (49%)]\tLoss: 0.914265\n",
      "Train Epoch: 8 [175616/359035 (49%)]\tLoss: 0.878135\n",
      "Train Epoch: 8 [176896/359035 (49%)]\tLoss: 0.718544\n",
      "Train Epoch: 8 [178176/359035 (50%)]\tLoss: 0.915685\n",
      "Train Epoch: 8 [179456/359035 (50%)]\tLoss: 0.917808\n",
      "Train Epoch: 8 [180736/359035 (50%)]\tLoss: 0.763046\n",
      "Train Epoch: 8 [182016/359035 (51%)]\tLoss: 0.838847\n",
      "Train Epoch: 8 [183296/359035 (51%)]\tLoss: 0.875309\n",
      "Train Epoch: 8 [184576/359035 (51%)]\tLoss: 0.935248\n",
      "Train Epoch: 8 [185856/359035 (52%)]\tLoss: 0.849633\n",
      "Train Epoch: 8 [187136/359035 (52%)]\tLoss: 0.924403\n",
      "Train Epoch: 8 [188416/359035 (52%)]\tLoss: 0.839622\n",
      "Train Epoch: 8 [189696/359035 (53%)]\tLoss: 0.778411\n",
      "Train Epoch: 8 [190976/359035 (53%)]\tLoss: 0.770142\n",
      "Train Epoch: 8 [192256/359035 (54%)]\tLoss: 0.893933\n",
      "Train Epoch: 8 [193536/359035 (54%)]\tLoss: 0.814049\n",
      "Train Epoch: 8 [194816/359035 (54%)]\tLoss: 0.750317\n",
      "Train Epoch: 8 [196096/359035 (55%)]\tLoss: 0.909850\n",
      "Train Epoch: 8 [197376/359035 (55%)]\tLoss: 0.907912\n",
      "Train Epoch: 8 [198656/359035 (55%)]\tLoss: 0.854545\n",
      "Train Epoch: 8 [199936/359035 (56%)]\tLoss: 0.924560\n",
      "Train Epoch: 8 [201216/359035 (56%)]\tLoss: 0.955744\n",
      "Train Epoch: 8 [202496/359035 (56%)]\tLoss: 0.858683\n",
      "Train Epoch: 8 [203776/359035 (57%)]\tLoss: 0.891216\n",
      "Train Epoch: 8 [205056/359035 (57%)]\tLoss: 0.836945\n",
      "Train Epoch: 8 [206336/359035 (57%)]\tLoss: 0.873725\n",
      "Train Epoch: 8 [207616/359035 (58%)]\tLoss: 0.891392\n",
      "Train Epoch: 8 [208896/359035 (58%)]\tLoss: 0.899734\n",
      "Train Epoch: 8 [210176/359035 (59%)]\tLoss: 0.867503\n",
      "Train Epoch: 8 [211456/359035 (59%)]\tLoss: 0.884172\n",
      "Train Epoch: 8 [212736/359035 (59%)]\tLoss: 0.790638\n",
      "Train Epoch: 8 [214016/359035 (60%)]\tLoss: 0.792501\n",
      "Train Epoch: 8 [215296/359035 (60%)]\tLoss: 0.858965\n",
      "Train Epoch: 8 [216576/359035 (60%)]\tLoss: 0.921993\n",
      "Train Epoch: 8 [217856/359035 (61%)]\tLoss: 0.889748\n",
      "Train Epoch: 8 [219136/359035 (61%)]\tLoss: 0.767962\n",
      "Train Epoch: 8 [220416/359035 (61%)]\tLoss: 0.856196\n",
      "Train Epoch: 8 [221696/359035 (62%)]\tLoss: 0.783535\n",
      "Train Epoch: 8 [222976/359035 (62%)]\tLoss: 1.002388\n",
      "Train Epoch: 8 [224256/359035 (62%)]\tLoss: 0.768875\n",
      "Train Epoch: 8 [225536/359035 (63%)]\tLoss: 0.835616\n",
      "Train Epoch: 8 [226816/359035 (63%)]\tLoss: 0.833085\n",
      "Train Epoch: 8 [228096/359035 (64%)]\tLoss: 0.838161\n",
      "Train Epoch: 8 [229376/359035 (64%)]\tLoss: 0.870148\n",
      "Train Epoch: 8 [230656/359035 (64%)]\tLoss: 0.919851\n",
      "Train Epoch: 8 [231936/359035 (65%)]\tLoss: 0.889198\n",
      "Train Epoch: 8 [233216/359035 (65%)]\tLoss: 0.928852\n",
      "Train Epoch: 8 [234496/359035 (65%)]\tLoss: 0.845607\n",
      "Train Epoch: 8 [235776/359035 (66%)]\tLoss: 0.732215\n",
      "Train Epoch: 8 [237056/359035 (66%)]\tLoss: 0.908103\n",
      "Train Epoch: 8 [238336/359035 (66%)]\tLoss: 0.931473\n",
      "Train Epoch: 8 [239616/359035 (67%)]\tLoss: 0.814527\n",
      "Train Epoch: 8 [240896/359035 (67%)]\tLoss: 0.765488\n",
      "Train Epoch: 8 [242176/359035 (67%)]\tLoss: 0.819973\n",
      "Train Epoch: 8 [243456/359035 (68%)]\tLoss: 0.850942\n",
      "Train Epoch: 8 [244736/359035 (68%)]\tLoss: 0.938899\n",
      "Train Epoch: 8 [246016/359035 (69%)]\tLoss: 0.870525\n",
      "Train Epoch: 8 [247296/359035 (69%)]\tLoss: 0.917295\n",
      "Train Epoch: 8 [248576/359035 (69%)]\tLoss: 0.872211\n",
      "Train Epoch: 8 [249856/359035 (70%)]\tLoss: 0.767975\n",
      "Train Epoch: 8 [251136/359035 (70%)]\tLoss: 0.857841\n",
      "Train Epoch: 8 [252416/359035 (70%)]\tLoss: 0.765099\n",
      "Train Epoch: 8 [253696/359035 (71%)]\tLoss: 0.870277\n",
      "Train Epoch: 8 [254976/359035 (71%)]\tLoss: 0.890701\n",
      "Train Epoch: 8 [256256/359035 (71%)]\tLoss: 0.962650\n",
      "Train Epoch: 8 [257536/359035 (72%)]\tLoss: 0.854835\n",
      "Train Epoch: 8 [258816/359035 (72%)]\tLoss: 0.866397\n",
      "Train Epoch: 8 [260096/359035 (72%)]\tLoss: 0.939853\n",
      "Train Epoch: 8 [261376/359035 (73%)]\tLoss: 0.924843\n",
      "Train Epoch: 8 [262656/359035 (73%)]\tLoss: 0.925915\n",
      "Train Epoch: 8 [263936/359035 (74%)]\tLoss: 0.912052\n",
      "Train Epoch: 8 [265216/359035 (74%)]\tLoss: 0.859798\n",
      "Train Epoch: 8 [266496/359035 (74%)]\tLoss: 0.875966\n",
      "Train Epoch: 8 [267776/359035 (75%)]\tLoss: 0.849567\n",
      "Train Epoch: 8 [269056/359035 (75%)]\tLoss: 0.906265\n",
      "Train Epoch: 8 [270336/359035 (75%)]\tLoss: 0.914202\n",
      "Train Epoch: 8 [271616/359035 (76%)]\tLoss: 0.818453\n",
      "Train Epoch: 8 [272896/359035 (76%)]\tLoss: 0.848310\n",
      "Train Epoch: 8 [274176/359035 (76%)]\tLoss: 0.834867\n",
      "Train Epoch: 8 [275456/359035 (77%)]\tLoss: 0.896067\n",
      "Train Epoch: 8 [276736/359035 (77%)]\tLoss: 0.837631\n",
      "Train Epoch: 8 [278016/359035 (77%)]\tLoss: 0.792572\n",
      "Train Epoch: 8 [279296/359035 (78%)]\tLoss: 0.732448\n",
      "Train Epoch: 8 [280576/359035 (78%)]\tLoss: 0.973598\n",
      "Train Epoch: 8 [281856/359035 (79%)]\tLoss: 0.867928\n",
      "Train Epoch: 8 [283136/359035 (79%)]\tLoss: 0.903534\n",
      "Train Epoch: 8 [284416/359035 (79%)]\tLoss: 0.895754\n",
      "Train Epoch: 8 [285696/359035 (80%)]\tLoss: 0.766391\n",
      "Train Epoch: 8 [286976/359035 (80%)]\tLoss: 0.853437\n",
      "Train Epoch: 8 [288256/359035 (80%)]\tLoss: 0.767948\n",
      "Train Epoch: 8 [289536/359035 (81%)]\tLoss: 0.761093\n",
      "Train Epoch: 8 [290816/359035 (81%)]\tLoss: 0.778458\n",
      "Train Epoch: 8 [292096/359035 (81%)]\tLoss: 0.903037\n",
      "Train Epoch: 8 [293376/359035 (82%)]\tLoss: 0.802132\n",
      "Train Epoch: 8 [294656/359035 (82%)]\tLoss: 0.867947\n",
      "Train Epoch: 8 [295936/359035 (82%)]\tLoss: 0.872973\n",
      "Train Epoch: 8 [297216/359035 (83%)]\tLoss: 0.820068\n",
      "Train Epoch: 8 [298496/359035 (83%)]\tLoss: 0.830381\n",
      "Train Epoch: 8 [299776/359035 (83%)]\tLoss: 0.776705\n",
      "Train Epoch: 8 [301056/359035 (84%)]\tLoss: 0.835614\n",
      "Train Epoch: 8 [302336/359035 (84%)]\tLoss: 0.932707\n",
      "Train Epoch: 8 [303616/359035 (85%)]\tLoss: 0.963858\n",
      "Train Epoch: 8 [304896/359035 (85%)]\tLoss: 0.824367\n",
      "Train Epoch: 8 [306176/359035 (85%)]\tLoss: 0.843692\n",
      "Train Epoch: 8 [307456/359035 (86%)]\tLoss: 0.819897\n",
      "Train Epoch: 8 [308736/359035 (86%)]\tLoss: 0.841527\n",
      "Train Epoch: 8 [310016/359035 (86%)]\tLoss: 0.931288\n",
      "Train Epoch: 8 [311296/359035 (87%)]\tLoss: 0.741457\n",
      "Train Epoch: 8 [312576/359035 (87%)]\tLoss: 0.870037\n",
      "Train Epoch: 8 [313856/359035 (87%)]\tLoss: 0.898063\n",
      "Train Epoch: 8 [315136/359035 (88%)]\tLoss: 0.801194\n",
      "Train Epoch: 8 [316416/359035 (88%)]\tLoss: 0.786153\n",
      "Train Epoch: 8 [317696/359035 (88%)]\tLoss: 0.778123\n",
      "Train Epoch: 8 [318976/359035 (89%)]\tLoss: 0.839742\n",
      "Train Epoch: 8 [320256/359035 (89%)]\tLoss: 0.845567\n",
      "Train Epoch: 8 [321536/359035 (90%)]\tLoss: 0.860048\n",
      "Train Epoch: 8 [322816/359035 (90%)]\tLoss: 0.896752\n",
      "Train Epoch: 8 [324096/359035 (90%)]\tLoss: 0.799559\n",
      "Train Epoch: 8 [325376/359035 (91%)]\tLoss: 0.858511\n",
      "Train Epoch: 8 [326656/359035 (91%)]\tLoss: 0.913873\n",
      "Train Epoch: 8 [327936/359035 (91%)]\tLoss: 0.963676\n",
      "Train Epoch: 8 [329216/359035 (92%)]\tLoss: 0.865567\n",
      "Train Epoch: 8 [330496/359035 (92%)]\tLoss: 0.863124\n",
      "Train Epoch: 8 [331776/359035 (92%)]\tLoss: 0.976231\n",
      "Train Epoch: 8 [333056/359035 (93%)]\tLoss: 0.910885\n",
      "Train Epoch: 8 [334336/359035 (93%)]\tLoss: 0.938222\n",
      "Train Epoch: 8 [335616/359035 (93%)]\tLoss: 0.786266\n",
      "Train Epoch: 8 [336896/359035 (94%)]\tLoss: 0.809632\n",
      "Train Epoch: 8 [338176/359035 (94%)]\tLoss: 0.843283\n",
      "Train Epoch: 8 [339456/359035 (95%)]\tLoss: 0.835789\n",
      "Train Epoch: 8 [340736/359035 (95%)]\tLoss: 0.765035\n",
      "Train Epoch: 8 [342016/359035 (95%)]\tLoss: 0.843534\n",
      "Train Epoch: 8 [343296/359035 (96%)]\tLoss: 0.898786\n",
      "Train Epoch: 8 [344576/359035 (96%)]\tLoss: 0.801554\n",
      "Train Epoch: 8 [345856/359035 (96%)]\tLoss: 0.914194\n",
      "Train Epoch: 8 [347136/359035 (97%)]\tLoss: 0.845823\n",
      "Train Epoch: 8 [348416/359035 (97%)]\tLoss: 0.881067\n",
      "Train Epoch: 8 [349696/359035 (97%)]\tLoss: 0.781908\n",
      "Train Epoch: 8 [350976/359035 (98%)]\tLoss: 0.739662\n",
      "Train Epoch: 8 [352256/359035 (98%)]\tLoss: 0.901913\n",
      "Train Epoch: 8 [353536/359035 (98%)]\tLoss: 0.864876\n",
      "Train Epoch: 8 [354816/359035 (99%)]\tLoss: 0.877803\n",
      "Train Epoch: 8 [356096/359035 (99%)]\tLoss: 0.909098\n",
      "Train Epoch: 8 [357376/359035 (100%)]\tLoss: 0.825521\n",
      "Train Epoch: 8 [358656/359035 (100%)]\tLoss: 0.763409\n",
      "Performance on test set: Average loss: 0.9150, Accuracy: 134/200 (67%)\n",
      "Epoch took: 2571.874s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.6081, 100to500: 0.1722, 20to50: 0.0920, 500to_above: 0.1278\n",
      "\n",
      "Epoch 9\n",
      "Train Epoch: 9 [256/359035 (0%)]\tLoss: 0.928726\n",
      "Train Epoch: 9 [1536/359035 (0%)]\tLoss: 0.824249\n",
      "Train Epoch: 9 [2816/359035 (1%)]\tLoss: 0.784702\n",
      "Train Epoch: 9 [4096/359035 (1%)]\tLoss: 1.001811\n",
      "Train Epoch: 9 [5376/359035 (1%)]\tLoss: 0.891097\n",
      "Train Epoch: 9 [6656/359035 (2%)]\tLoss: 0.960184\n",
      "Train Epoch: 9 [7936/359035 (2%)]\tLoss: 0.861398\n",
      "Train Epoch: 9 [9216/359035 (3%)]\tLoss: 0.885690\n",
      "Train Epoch: 9 [10496/359035 (3%)]\tLoss: 0.808490\n",
      "Train Epoch: 9 [11776/359035 (3%)]\tLoss: 0.866792\n",
      "Train Epoch: 9 [13056/359035 (4%)]\tLoss: 0.947663\n",
      "Train Epoch: 9 [14336/359035 (4%)]\tLoss: 0.800583\n",
      "Train Epoch: 9 [15616/359035 (4%)]\tLoss: 0.800655\n",
      "Train Epoch: 9 [16896/359035 (5%)]\tLoss: 0.886097\n",
      "Train Epoch: 9 [18176/359035 (5%)]\tLoss: 0.944333\n",
      "Train Epoch: 9 [19456/359035 (5%)]\tLoss: 0.886705\n",
      "Train Epoch: 9 [20736/359035 (6%)]\tLoss: 0.866198\n",
      "Train Epoch: 9 [22016/359035 (6%)]\tLoss: 0.870016\n",
      "Train Epoch: 9 [23296/359035 (6%)]\tLoss: 0.882921\n",
      "Train Epoch: 9 [24576/359035 (7%)]\tLoss: 0.841010\n",
      "Train Epoch: 9 [25856/359035 (7%)]\tLoss: 0.896843\n",
      "Train Epoch: 9 [27136/359035 (8%)]\tLoss: 0.834113\n",
      "Train Epoch: 9 [28416/359035 (8%)]\tLoss: 0.820258\n",
      "Train Epoch: 9 [29696/359035 (8%)]\tLoss: 0.833754\n",
      "Train Epoch: 9 [30976/359035 (9%)]\tLoss: 0.926602\n",
      "Train Epoch: 9 [32256/359035 (9%)]\tLoss: 0.894324\n",
      "Train Epoch: 9 [33536/359035 (9%)]\tLoss: 0.842533\n",
      "Train Epoch: 9 [34816/359035 (10%)]\tLoss: 0.966637\n",
      "Train Epoch: 9 [36096/359035 (10%)]\tLoss: 0.893960\n",
      "Train Epoch: 9 [37376/359035 (10%)]\tLoss: 0.886139\n",
      "Train Epoch: 9 [38656/359035 (11%)]\tLoss: 0.752256\n",
      "Train Epoch: 9 [39936/359035 (11%)]\tLoss: 0.804148\n",
      "Train Epoch: 9 [41216/359035 (11%)]\tLoss: 0.820491\n",
      "Train Epoch: 9 [42496/359035 (12%)]\tLoss: 0.840577\n",
      "Train Epoch: 9 [43776/359035 (12%)]\tLoss: 0.829727\n",
      "Train Epoch: 9 [45056/359035 (13%)]\tLoss: 0.935725\n",
      "Train Epoch: 9 [46336/359035 (13%)]\tLoss: 0.863363\n",
      "Train Epoch: 9 [47616/359035 (13%)]\tLoss: 0.874005\n",
      "Train Epoch: 9 [48896/359035 (14%)]\tLoss: 0.906331\n",
      "Train Epoch: 9 [50176/359035 (14%)]\tLoss: 0.947511\n",
      "Train Epoch: 9 [51456/359035 (14%)]\tLoss: 0.838794\n",
      "Train Epoch: 9 [52736/359035 (15%)]\tLoss: 0.913213\n",
      "Train Epoch: 9 [54016/359035 (15%)]\tLoss: 0.795235\n",
      "Train Epoch: 9 [55296/359035 (15%)]\tLoss: 0.846060\n",
      "Train Epoch: 9 [56576/359035 (16%)]\tLoss: 0.817935\n",
      "Train Epoch: 9 [57856/359035 (16%)]\tLoss: 0.823930\n",
      "Train Epoch: 9 [59136/359035 (16%)]\tLoss: 0.915056\n",
      "Train Epoch: 9 [60416/359035 (17%)]\tLoss: 0.857622\n",
      "Train Epoch: 9 [61696/359035 (17%)]\tLoss: 0.929714\n",
      "Train Epoch: 9 [62976/359035 (18%)]\tLoss: 0.872272\n",
      "Train Epoch: 9 [64256/359035 (18%)]\tLoss: 0.960612\n",
      "Train Epoch: 9 [65536/359035 (18%)]\tLoss: 0.922744\n",
      "Train Epoch: 9 [66816/359035 (19%)]\tLoss: 0.781254\n",
      "Train Epoch: 9 [68096/359035 (19%)]\tLoss: 0.932789\n",
      "Train Epoch: 9 [69376/359035 (19%)]\tLoss: 0.816565\n",
      "Train Epoch: 9 [70656/359035 (20%)]\tLoss: 0.878751\n",
      "Train Epoch: 9 [71936/359035 (20%)]\tLoss: 0.887127\n",
      "Train Epoch: 9 [73216/359035 (20%)]\tLoss: 0.904052\n",
      "Train Epoch: 9 [74496/359035 (21%)]\tLoss: 0.914593\n",
      "Train Epoch: 9 [75776/359035 (21%)]\tLoss: 0.873240\n",
      "Train Epoch: 9 [77056/359035 (21%)]\tLoss: 0.876243\n",
      "Train Epoch: 9 [78336/359035 (22%)]\tLoss: 0.780443\n",
      "Train Epoch: 9 [79616/359035 (22%)]\tLoss: 0.865122\n",
      "Train Epoch: 9 [80896/359035 (23%)]\tLoss: 0.847263\n",
      "Train Epoch: 9 [82176/359035 (23%)]\tLoss: 0.818283\n",
      "Train Epoch: 9 [83456/359035 (23%)]\tLoss: 0.888206\n",
      "Train Epoch: 9 [84736/359035 (24%)]\tLoss: 0.891795\n",
      "Train Epoch: 9 [86016/359035 (24%)]\tLoss: 0.880512\n",
      "Train Epoch: 9 [87296/359035 (24%)]\tLoss: 0.916807\n",
      "Train Epoch: 9 [88576/359035 (25%)]\tLoss: 0.886757\n",
      "Train Epoch: 9 [89856/359035 (25%)]\tLoss: 0.847791\n",
      "Train Epoch: 9 [91136/359035 (25%)]\tLoss: 0.942305\n",
      "Train Epoch: 9 [92416/359035 (26%)]\tLoss: 0.855316\n",
      "Train Epoch: 9 [93696/359035 (26%)]\tLoss: 0.866607\n",
      "Train Epoch: 9 [94976/359035 (26%)]\tLoss: 0.842393\n",
      "Train Epoch: 9 [96256/359035 (27%)]\tLoss: 0.889303\n",
      "Train Epoch: 9 [97536/359035 (27%)]\tLoss: 0.846825\n",
      "Train Epoch: 9 [98816/359035 (28%)]\tLoss: 0.870182\n",
      "Train Epoch: 9 [100096/359035 (28%)]\tLoss: 0.930574\n",
      "Train Epoch: 9 [101376/359035 (28%)]\tLoss: 1.012671\n",
      "Train Epoch: 9 [102656/359035 (29%)]\tLoss: 0.877803\n",
      "Train Epoch: 9 [103936/359035 (29%)]\tLoss: 0.845090\n",
      "Train Epoch: 9 [105216/359035 (29%)]\tLoss: 0.812149\n",
      "Train Epoch: 9 [106496/359035 (30%)]\tLoss: 0.822297\n",
      "Train Epoch: 9 [107776/359035 (30%)]\tLoss: 0.890670\n",
      "Train Epoch: 9 [109056/359035 (30%)]\tLoss: 0.915319\n",
      "Train Epoch: 9 [110336/359035 (31%)]\tLoss: 0.871550\n",
      "Train Epoch: 9 [111616/359035 (31%)]\tLoss: 0.791060\n",
      "Train Epoch: 9 [112896/359035 (31%)]\tLoss: 0.870644\n",
      "Train Epoch: 9 [114176/359035 (32%)]\tLoss: 0.785732\n",
      "Train Epoch: 9 [115456/359035 (32%)]\tLoss: 0.918853\n",
      "Train Epoch: 9 [116736/359035 (33%)]\tLoss: 0.899742\n",
      "Train Epoch: 9 [118016/359035 (33%)]\tLoss: 0.866390\n",
      "Train Epoch: 9 [119296/359035 (33%)]\tLoss: 0.989187\n",
      "Train Epoch: 9 [120576/359035 (34%)]\tLoss: 0.820909\n",
      "Train Epoch: 9 [121856/359035 (34%)]\tLoss: 0.821573\n",
      "Train Epoch: 9 [123136/359035 (34%)]\tLoss: 0.853117\n",
      "Train Epoch: 9 [124416/359035 (35%)]\tLoss: 0.889208\n",
      "Train Epoch: 9 [125696/359035 (35%)]\tLoss: 0.831222\n",
      "Train Epoch: 9 [126976/359035 (35%)]\tLoss: 0.922148\n",
      "Train Epoch: 9 [128256/359035 (36%)]\tLoss: 0.904880\n",
      "Train Epoch: 9 [129536/359035 (36%)]\tLoss: 0.869669\n",
      "Train Epoch: 9 [130816/359035 (36%)]\tLoss: 0.916237\n",
      "Train Epoch: 9 [132096/359035 (37%)]\tLoss: 0.825076\n",
      "Train Epoch: 9 [133376/359035 (37%)]\tLoss: 0.900270\n",
      "Train Epoch: 9 [134656/359035 (38%)]\tLoss: 0.755824\n",
      "Train Epoch: 9 [135936/359035 (38%)]\tLoss: 0.805563\n",
      "Train Epoch: 9 [137216/359035 (38%)]\tLoss: 0.874766\n",
      "Train Epoch: 9 [138496/359035 (39%)]\tLoss: 0.808983\n",
      "Train Epoch: 9 [139776/359035 (39%)]\tLoss: 0.871858\n",
      "Train Epoch: 9 [141056/359035 (39%)]\tLoss: 0.815011\n",
      "Train Epoch: 9 [142336/359035 (40%)]\tLoss: 0.921945\n",
      "Train Epoch: 9 [143616/359035 (40%)]\tLoss: 0.957868\n",
      "Train Epoch: 9 [144896/359035 (40%)]\tLoss: 0.789629\n",
      "Train Epoch: 9 [146176/359035 (41%)]\tLoss: 0.946088\n",
      "Train Epoch: 9 [147456/359035 (41%)]\tLoss: 0.777201\n",
      "Train Epoch: 9 [148736/359035 (41%)]\tLoss: 0.892406\n",
      "Train Epoch: 9 [150016/359035 (42%)]\tLoss: 0.892529\n",
      "Train Epoch: 9 [151296/359035 (42%)]\tLoss: 0.860710\n",
      "Train Epoch: 9 [152576/359035 (42%)]\tLoss: 0.825212\n",
      "Train Epoch: 9 [153856/359035 (43%)]\tLoss: 0.796570\n",
      "Train Epoch: 9 [155136/359035 (43%)]\tLoss: 0.864976\n",
      "Train Epoch: 9 [156416/359035 (44%)]\tLoss: 1.022812\n",
      "Train Epoch: 9 [157696/359035 (44%)]\tLoss: 0.814564\n",
      "Train Epoch: 9 [158976/359035 (44%)]\tLoss: 0.941873\n",
      "Train Epoch: 9 [160256/359035 (45%)]\tLoss: 0.888091\n",
      "Train Epoch: 9 [161536/359035 (45%)]\tLoss: 0.821536\n",
      "Train Epoch: 9 [162816/359035 (45%)]\tLoss: 0.739271\n",
      "Train Epoch: 9 [164096/359035 (46%)]\tLoss: 0.939707\n",
      "Train Epoch: 9 [165376/359035 (46%)]\tLoss: 0.820756\n",
      "Train Epoch: 9 [166656/359035 (46%)]\tLoss: 1.013442\n",
      "Train Epoch: 9 [167936/359035 (47%)]\tLoss: 0.833180\n",
      "Train Epoch: 9 [169216/359035 (47%)]\tLoss: 0.887868\n",
      "Train Epoch: 9 [170496/359035 (47%)]\tLoss: 0.875201\n",
      "Train Epoch: 9 [171776/359035 (48%)]\tLoss: 0.801404\n",
      "Train Epoch: 9 [173056/359035 (48%)]\tLoss: 0.890906\n",
      "Train Epoch: 9 [174336/359035 (49%)]\tLoss: 0.773045\n",
      "Train Epoch: 9 [175616/359035 (49%)]\tLoss: 0.871402\n",
      "Train Epoch: 9 [176896/359035 (49%)]\tLoss: 0.909869\n",
      "Train Epoch: 9 [178176/359035 (50%)]\tLoss: 0.862021\n",
      "Train Epoch: 9 [179456/359035 (50%)]\tLoss: 0.842368\n",
      "Train Epoch: 9 [180736/359035 (50%)]\tLoss: 0.827686\n",
      "Train Epoch: 9 [182016/359035 (51%)]\tLoss: 0.834971\n",
      "Train Epoch: 9 [183296/359035 (51%)]\tLoss: 1.013562\n",
      "Train Epoch: 9 [184576/359035 (51%)]\tLoss: 0.904372\n",
      "Train Epoch: 9 [185856/359035 (52%)]\tLoss: 0.831628\n",
      "Train Epoch: 9 [187136/359035 (52%)]\tLoss: 0.854197\n",
      "Train Epoch: 9 [188416/359035 (52%)]\tLoss: 0.837322\n",
      "Train Epoch: 9 [189696/359035 (53%)]\tLoss: 0.849558\n",
      "Train Epoch: 9 [190976/359035 (53%)]\tLoss: 0.860697\n",
      "Train Epoch: 9 [192256/359035 (54%)]\tLoss: 0.835885\n",
      "Train Epoch: 9 [193536/359035 (54%)]\tLoss: 0.926003\n",
      "Train Epoch: 9 [194816/359035 (54%)]\tLoss: 0.823060\n",
      "Train Epoch: 9 [196096/359035 (55%)]\tLoss: 0.787541\n",
      "Train Epoch: 9 [197376/359035 (55%)]\tLoss: 0.779628\n",
      "Train Epoch: 9 [198656/359035 (55%)]\tLoss: 0.864622\n",
      "Train Epoch: 9 [199936/359035 (56%)]\tLoss: 0.837697\n",
      "Train Epoch: 9 [201216/359035 (56%)]\tLoss: 0.944773\n",
      "Train Epoch: 9 [202496/359035 (56%)]\tLoss: 0.890323\n",
      "Train Epoch: 9 [203776/359035 (57%)]\tLoss: 0.848813\n",
      "Train Epoch: 9 [205056/359035 (57%)]\tLoss: 0.823168\n",
      "Train Epoch: 9 [206336/359035 (57%)]\tLoss: 0.882644\n",
      "Train Epoch: 9 [207616/359035 (58%)]\tLoss: 0.783451\n",
      "Train Epoch: 9 [208896/359035 (58%)]\tLoss: 0.952087\n",
      "Train Epoch: 9 [210176/359035 (59%)]\tLoss: 0.937128\n",
      "Train Epoch: 9 [211456/359035 (59%)]\tLoss: 0.735313\n",
      "Train Epoch: 9 [212736/359035 (59%)]\tLoss: 0.915127\n",
      "Train Epoch: 9 [214016/359035 (60%)]\tLoss: 0.897473\n",
      "Train Epoch: 9 [215296/359035 (60%)]\tLoss: 0.832995\n",
      "Train Epoch: 9 [216576/359035 (60%)]\tLoss: 0.779517\n",
      "Train Epoch: 9 [217856/359035 (61%)]\tLoss: 0.775867\n",
      "Train Epoch: 9 [219136/359035 (61%)]\tLoss: 0.963681\n",
      "Train Epoch: 9 [220416/359035 (61%)]\tLoss: 0.790440\n",
      "Train Epoch: 9 [221696/359035 (62%)]\tLoss: 0.824103\n",
      "Train Epoch: 9 [222976/359035 (62%)]\tLoss: 0.783982\n",
      "Train Epoch: 9 [224256/359035 (62%)]\tLoss: 0.791604\n",
      "Train Epoch: 9 [225536/359035 (63%)]\tLoss: 0.874728\n",
      "Train Epoch: 9 [226816/359035 (63%)]\tLoss: 0.872376\n",
      "Train Epoch: 9 [228096/359035 (64%)]\tLoss: 0.739479\n",
      "Train Epoch: 9 [229376/359035 (64%)]\tLoss: 0.830084\n",
      "Train Epoch: 9 [230656/359035 (64%)]\tLoss: 0.926218\n",
      "Train Epoch: 9 [231936/359035 (65%)]\tLoss: 0.871813\n",
      "Train Epoch: 9 [233216/359035 (65%)]\tLoss: 0.865915\n",
      "Train Epoch: 9 [234496/359035 (65%)]\tLoss: 0.872865\n",
      "Train Epoch: 9 [235776/359035 (66%)]\tLoss: 0.904162\n",
      "Train Epoch: 9 [237056/359035 (66%)]\tLoss: 0.825453\n",
      "Train Epoch: 9 [238336/359035 (66%)]\tLoss: 0.799647\n",
      "Train Epoch: 9 [239616/359035 (67%)]\tLoss: 0.909331\n",
      "Train Epoch: 9 [240896/359035 (67%)]\tLoss: 0.895231\n",
      "Train Epoch: 9 [242176/359035 (67%)]\tLoss: 0.828242\n",
      "Train Epoch: 9 [243456/359035 (68%)]\tLoss: 0.841870\n",
      "Train Epoch: 9 [244736/359035 (68%)]\tLoss: 0.918851\n",
      "Train Epoch: 9 [246016/359035 (69%)]\tLoss: 0.833211\n",
      "Train Epoch: 9 [247296/359035 (69%)]\tLoss: 0.782160\n",
      "Train Epoch: 9 [248576/359035 (69%)]\tLoss: 0.875234\n",
      "Train Epoch: 9 [249856/359035 (70%)]\tLoss: 0.803499\n",
      "Train Epoch: 9 [251136/359035 (70%)]\tLoss: 0.898554\n",
      "Train Epoch: 9 [252416/359035 (70%)]\tLoss: 0.871955\n",
      "Train Epoch: 9 [253696/359035 (71%)]\tLoss: 0.844445\n",
      "Train Epoch: 9 [254976/359035 (71%)]\tLoss: 0.821990\n",
      "Train Epoch: 9 [256256/359035 (71%)]\tLoss: 0.878905\n",
      "Train Epoch: 9 [257536/359035 (72%)]\tLoss: 0.780388\n",
      "Train Epoch: 9 [258816/359035 (72%)]\tLoss: 0.811645\n",
      "Train Epoch: 9 [260096/359035 (72%)]\tLoss: 0.861668\n",
      "Train Epoch: 9 [261376/359035 (73%)]\tLoss: 0.749477\n",
      "Train Epoch: 9 [262656/359035 (73%)]\tLoss: 0.838070\n",
      "Train Epoch: 9 [263936/359035 (74%)]\tLoss: 0.867076\n",
      "Train Epoch: 9 [265216/359035 (74%)]\tLoss: 0.884806\n",
      "Train Epoch: 9 [266496/359035 (74%)]\tLoss: 0.800840\n",
      "Train Epoch: 9 [267776/359035 (75%)]\tLoss: 0.823265\n",
      "Train Epoch: 9 [269056/359035 (75%)]\tLoss: 0.773611\n",
      "Train Epoch: 9 [270336/359035 (75%)]\tLoss: 0.830431\n",
      "Train Epoch: 9 [271616/359035 (76%)]\tLoss: 0.881262\n",
      "Train Epoch: 9 [272896/359035 (76%)]\tLoss: 0.807769\n",
      "Train Epoch: 9 [274176/359035 (76%)]\tLoss: 0.845611\n",
      "Train Epoch: 9 [275456/359035 (77%)]\tLoss: 0.934553\n",
      "Train Epoch: 9 [276736/359035 (77%)]\tLoss: 0.811737\n",
      "Train Epoch: 9 [278016/359035 (77%)]\tLoss: 0.834475\n",
      "Train Epoch: 9 [279296/359035 (78%)]\tLoss: 0.888202\n",
      "Train Epoch: 9 [280576/359035 (78%)]\tLoss: 0.870052\n",
      "Train Epoch: 9 [281856/359035 (79%)]\tLoss: 0.843695\n",
      "Train Epoch: 9 [283136/359035 (79%)]\tLoss: 0.848610\n",
      "Train Epoch: 9 [284416/359035 (79%)]\tLoss: 0.875977\n",
      "Train Epoch: 9 [285696/359035 (80%)]\tLoss: 0.865273\n",
      "Train Epoch: 9 [286976/359035 (80%)]\tLoss: 0.777325\n",
      "Train Epoch: 9 [288256/359035 (80%)]\tLoss: 0.881261\n",
      "Train Epoch: 9 [289536/359035 (81%)]\tLoss: 0.914862\n",
      "Train Epoch: 9 [290816/359035 (81%)]\tLoss: 0.883881\n",
      "Train Epoch: 9 [292096/359035 (81%)]\tLoss: 0.884487\n",
      "Train Epoch: 9 [293376/359035 (82%)]\tLoss: 0.848637\n",
      "Train Epoch: 9 [294656/359035 (82%)]\tLoss: 0.872793\n",
      "Train Epoch: 9 [295936/359035 (82%)]\tLoss: 0.811938\n",
      "Train Epoch: 9 [297216/359035 (83%)]\tLoss: 0.926787\n",
      "Train Epoch: 9 [298496/359035 (83%)]\tLoss: 0.849743\n",
      "Train Epoch: 9 [299776/359035 (83%)]\tLoss: 0.769335\n",
      "Train Epoch: 9 [301056/359035 (84%)]\tLoss: 0.845410\n",
      "Train Epoch: 9 [302336/359035 (84%)]\tLoss: 0.830651\n",
      "Train Epoch: 9 [303616/359035 (85%)]\tLoss: 0.847547\n",
      "Train Epoch: 9 [304896/359035 (85%)]\tLoss: 0.813789\n",
      "Train Epoch: 9 [306176/359035 (85%)]\tLoss: 0.845940\n",
      "Train Epoch: 9 [307456/359035 (86%)]\tLoss: 0.901328\n",
      "Train Epoch: 9 [308736/359035 (86%)]\tLoss: 0.761069\n",
      "Train Epoch: 9 [310016/359035 (86%)]\tLoss: 0.800937\n",
      "Train Epoch: 9 [311296/359035 (87%)]\tLoss: 0.755913\n",
      "Train Epoch: 9 [312576/359035 (87%)]\tLoss: 0.855012\n",
      "Train Epoch: 9 [313856/359035 (87%)]\tLoss: 0.825192\n",
      "Train Epoch: 9 [315136/359035 (88%)]\tLoss: 0.819484\n",
      "Train Epoch: 9 [316416/359035 (88%)]\tLoss: 0.806409\n",
      "Train Epoch: 9 [317696/359035 (88%)]\tLoss: 0.820322\n",
      "Train Epoch: 9 [318976/359035 (89%)]\tLoss: 0.745040\n",
      "Train Epoch: 9 [320256/359035 (89%)]\tLoss: 0.840213\n",
      "Train Epoch: 9 [321536/359035 (90%)]\tLoss: 0.850020\n",
      "Train Epoch: 9 [322816/359035 (90%)]\tLoss: 0.976199\n",
      "Train Epoch: 9 [324096/359035 (90%)]\tLoss: 0.820918\n",
      "Train Epoch: 9 [325376/359035 (91%)]\tLoss: 0.797042\n",
      "Train Epoch: 9 [326656/359035 (91%)]\tLoss: 0.821373\n",
      "Train Epoch: 9 [327936/359035 (91%)]\tLoss: 0.927420\n",
      "Train Epoch: 9 [329216/359035 (92%)]\tLoss: 0.814987\n",
      "Train Epoch: 9 [330496/359035 (92%)]\tLoss: 0.896287\n",
      "Train Epoch: 9 [331776/359035 (92%)]\tLoss: 0.805547\n",
      "Train Epoch: 9 [333056/359035 (93%)]\tLoss: 0.860147\n",
      "Train Epoch: 9 [334336/359035 (93%)]\tLoss: 0.916843\n",
      "Train Epoch: 9 [335616/359035 (93%)]\tLoss: 0.846245\n",
      "Train Epoch: 9 [336896/359035 (94%)]\tLoss: 0.799417\n",
      "Train Epoch: 9 [338176/359035 (94%)]\tLoss: 0.835551\n",
      "Train Epoch: 9 [339456/359035 (95%)]\tLoss: 0.889886\n",
      "Train Epoch: 9 [340736/359035 (95%)]\tLoss: 0.968542\n",
      "Train Epoch: 9 [342016/359035 (95%)]\tLoss: 0.903372\n",
      "Train Epoch: 9 [343296/359035 (96%)]\tLoss: 0.832926\n",
      "Train Epoch: 9 [344576/359035 (96%)]\tLoss: 0.835431\n",
      "Train Epoch: 9 [345856/359035 (96%)]\tLoss: 0.821765\n",
      "Train Epoch: 9 [347136/359035 (97%)]\tLoss: 0.908526\n",
      "Train Epoch: 9 [348416/359035 (97%)]\tLoss: 0.868750\n",
      "Train Epoch: 9 [349696/359035 (97%)]\tLoss: 0.891285\n",
      "Train Epoch: 9 [350976/359035 (98%)]\tLoss: 0.812414\n",
      "Train Epoch: 9 [352256/359035 (98%)]\tLoss: 0.816553\n",
      "Train Epoch: 9 [353536/359035 (98%)]\tLoss: 0.742029\n",
      "Train Epoch: 9 [354816/359035 (99%)]\tLoss: 0.875178\n",
      "Train Epoch: 9 [356096/359035 (99%)]\tLoss: 0.784654\n",
      "Train Epoch: 9 [357376/359035 (100%)]\tLoss: 0.906443\n",
      "Train Epoch: 9 [358656/359035 (100%)]\tLoss: 0.845796\n",
      "Performance on test set: Average loss: 0.9282, Accuracy: 133/200 (66%)\n",
      "Epoch took: 2572.635s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.6387, 100to500: 0.1637, 20to50: 0.1298, 500to_above: 0.0678\n",
      "\n",
      "Epoch 10\n",
      "Train Epoch: 10 [256/359035 (0%)]\tLoss: 0.903269\n",
      "Train Epoch: 10 [1536/359035 (0%)]\tLoss: 0.862047\n",
      "Train Epoch: 10 [2816/359035 (1%)]\tLoss: 0.835390\n",
      "Train Epoch: 10 [4096/359035 (1%)]\tLoss: 0.900732\n",
      "Train Epoch: 10 [5376/359035 (1%)]\tLoss: 0.935126\n",
      "Train Epoch: 10 [6656/359035 (2%)]\tLoss: 0.840137\n",
      "Train Epoch: 10 [7936/359035 (2%)]\tLoss: 0.944545\n",
      "Train Epoch: 10 [9216/359035 (3%)]\tLoss: 0.837395\n",
      "Train Epoch: 10 [10496/359035 (3%)]\tLoss: 0.890025\n",
      "Train Epoch: 10 [11776/359035 (3%)]\tLoss: 0.867628\n",
      "Train Epoch: 10 [13056/359035 (4%)]\tLoss: 0.831194\n",
      "Train Epoch: 10 [14336/359035 (4%)]\tLoss: 0.794634\n",
      "Train Epoch: 10 [15616/359035 (4%)]\tLoss: 0.818945\n",
      "Train Epoch: 10 [16896/359035 (5%)]\tLoss: 0.817790\n",
      "Train Epoch: 10 [18176/359035 (5%)]\tLoss: 0.804148\n",
      "Train Epoch: 10 [19456/359035 (5%)]\tLoss: 0.795131\n",
      "Train Epoch: 10 [20736/359035 (6%)]\tLoss: 0.945648\n",
      "Train Epoch: 10 [22016/359035 (6%)]\tLoss: 0.921363\n",
      "Train Epoch: 10 [23296/359035 (6%)]\tLoss: 0.951478\n",
      "Train Epoch: 10 [24576/359035 (7%)]\tLoss: 0.837572\n",
      "Train Epoch: 10 [25856/359035 (7%)]\tLoss: 0.825402\n",
      "Train Epoch: 10 [27136/359035 (8%)]\tLoss: 0.808477\n",
      "Train Epoch: 10 [28416/359035 (8%)]\tLoss: 0.986431\n",
      "Train Epoch: 10 [29696/359035 (8%)]\tLoss: 0.796330\n",
      "Train Epoch: 10 [30976/359035 (9%)]\tLoss: 0.879349\n",
      "Train Epoch: 10 [32256/359035 (9%)]\tLoss: 0.895031\n",
      "Train Epoch: 10 [33536/359035 (9%)]\tLoss: 0.859182\n",
      "Train Epoch: 10 [34816/359035 (10%)]\tLoss: 0.824879\n",
      "Train Epoch: 10 [36096/359035 (10%)]\tLoss: 0.769542\n",
      "Train Epoch: 10 [37376/359035 (10%)]\tLoss: 0.861988\n",
      "Train Epoch: 10 [38656/359035 (11%)]\tLoss: 0.851957\n",
      "Train Epoch: 10 [39936/359035 (11%)]\tLoss: 0.780480\n",
      "Train Epoch: 10 [41216/359035 (11%)]\tLoss: 0.816430\n",
      "Train Epoch: 10 [42496/359035 (12%)]\tLoss: 0.838509\n",
      "Train Epoch: 10 [43776/359035 (12%)]\tLoss: 0.868611\n",
      "Train Epoch: 10 [45056/359035 (13%)]\tLoss: 0.872725\n",
      "Train Epoch: 10 [46336/359035 (13%)]\tLoss: 0.847606\n",
      "Train Epoch: 10 [47616/359035 (13%)]\tLoss: 0.918672\n",
      "Train Epoch: 10 [48896/359035 (14%)]\tLoss: 0.802074\n",
      "Train Epoch: 10 [50176/359035 (14%)]\tLoss: 0.807747\n",
      "Train Epoch: 10 [51456/359035 (14%)]\tLoss: 0.871535\n",
      "Train Epoch: 10 [52736/359035 (15%)]\tLoss: 0.815048\n",
      "Train Epoch: 10 [54016/359035 (15%)]\tLoss: 0.838473\n",
      "Train Epoch: 10 [55296/359035 (15%)]\tLoss: 0.867412\n",
      "Train Epoch: 10 [56576/359035 (16%)]\tLoss: 0.854982\n",
      "Train Epoch: 10 [57856/359035 (16%)]\tLoss: 0.696781\n",
      "Train Epoch: 10 [59136/359035 (16%)]\tLoss: 0.901119\n",
      "Train Epoch: 10 [60416/359035 (17%)]\tLoss: 0.932236\n",
      "Train Epoch: 10 [61696/359035 (17%)]\tLoss: 0.858824\n",
      "Train Epoch: 10 [62976/359035 (18%)]\tLoss: 0.747329\n",
      "Train Epoch: 10 [64256/359035 (18%)]\tLoss: 0.904338\n",
      "Train Epoch: 10 [65536/359035 (18%)]\tLoss: 0.873915\n",
      "Train Epoch: 10 [66816/359035 (19%)]\tLoss: 0.821237\n",
      "Train Epoch: 10 [68096/359035 (19%)]\tLoss: 0.846303\n",
      "Train Epoch: 10 [69376/359035 (19%)]\tLoss: 0.975432\n",
      "Train Epoch: 10 [70656/359035 (20%)]\tLoss: 0.797494\n",
      "Train Epoch: 10 [71936/359035 (20%)]\tLoss: 0.773445\n",
      "Train Epoch: 10 [73216/359035 (20%)]\tLoss: 0.831681\n",
      "Train Epoch: 10 [74496/359035 (21%)]\tLoss: 0.831426\n",
      "Train Epoch: 10 [75776/359035 (21%)]\tLoss: 0.820415\n",
      "Train Epoch: 10 [77056/359035 (21%)]\tLoss: 0.883168\n",
      "Train Epoch: 10 [78336/359035 (22%)]\tLoss: 0.832145\n",
      "Train Epoch: 10 [79616/359035 (22%)]\tLoss: 0.901942\n",
      "Train Epoch: 10 [80896/359035 (23%)]\tLoss: 0.910217\n",
      "Train Epoch: 10 [82176/359035 (23%)]\tLoss: 0.827903\n",
      "Train Epoch: 10 [83456/359035 (23%)]\tLoss: 0.940321\n",
      "Train Epoch: 10 [84736/359035 (24%)]\tLoss: 0.885164\n",
      "Train Epoch: 10 [86016/359035 (24%)]\tLoss: 0.944145\n",
      "Train Epoch: 10 [87296/359035 (24%)]\tLoss: 0.720651\n",
      "Train Epoch: 10 [88576/359035 (25%)]\tLoss: 0.952733\n",
      "Train Epoch: 10 [89856/359035 (25%)]\tLoss: 0.880249\n",
      "Train Epoch: 10 [91136/359035 (25%)]\tLoss: 0.827254\n",
      "Train Epoch: 10 [92416/359035 (26%)]\tLoss: 0.866605\n",
      "Train Epoch: 10 [93696/359035 (26%)]\tLoss: 0.904305\n",
      "Train Epoch: 10 [94976/359035 (26%)]\tLoss: 0.787866\n",
      "Train Epoch: 10 [96256/359035 (27%)]\tLoss: 0.889788\n",
      "Train Epoch: 10 [97536/359035 (27%)]\tLoss: 0.950387\n",
      "Train Epoch: 10 [98816/359035 (28%)]\tLoss: 0.879664\n",
      "Train Epoch: 10 [100096/359035 (28%)]\tLoss: 0.864686\n",
      "Train Epoch: 10 [101376/359035 (28%)]\tLoss: 0.863717\n",
      "Train Epoch: 10 [102656/359035 (29%)]\tLoss: 0.892444\n",
      "Train Epoch: 10 [103936/359035 (29%)]\tLoss: 0.828203\n",
      "Train Epoch: 10 [105216/359035 (29%)]\tLoss: 0.737096\n",
      "Train Epoch: 10 [106496/359035 (30%)]\tLoss: 0.878701\n",
      "Train Epoch: 10 [107776/359035 (30%)]\tLoss: 0.879474\n",
      "Train Epoch: 10 [109056/359035 (30%)]\tLoss: 0.873250\n",
      "Train Epoch: 10 [110336/359035 (31%)]\tLoss: 0.837913\n",
      "Train Epoch: 10 [111616/359035 (31%)]\tLoss: 0.942543\n",
      "Train Epoch: 10 [112896/359035 (31%)]\tLoss: 0.881493\n",
      "Train Epoch: 10 [114176/359035 (32%)]\tLoss: 0.875898\n",
      "Train Epoch: 10 [115456/359035 (32%)]\tLoss: 0.859563\n",
      "Train Epoch: 10 [116736/359035 (33%)]\tLoss: 0.909351\n",
      "Train Epoch: 10 [118016/359035 (33%)]\tLoss: 0.882320\n",
      "Train Epoch: 10 [119296/359035 (33%)]\tLoss: 0.850162\n",
      "Train Epoch: 10 [120576/359035 (34%)]\tLoss: 0.854948\n",
      "Train Epoch: 10 [121856/359035 (34%)]\tLoss: 0.874350\n",
      "Train Epoch: 10 [123136/359035 (34%)]\tLoss: 0.866621\n",
      "Train Epoch: 10 [124416/359035 (35%)]\tLoss: 0.866877\n",
      "Train Epoch: 10 [125696/359035 (35%)]\tLoss: 0.844589\n",
      "Train Epoch: 10 [126976/359035 (35%)]\tLoss: 0.828851\n",
      "Train Epoch: 10 [128256/359035 (36%)]\tLoss: 0.928112\n",
      "Train Epoch: 10 [129536/359035 (36%)]\tLoss: 0.872142\n",
      "Train Epoch: 10 [130816/359035 (36%)]\tLoss: 0.753103\n",
      "Train Epoch: 10 [132096/359035 (37%)]\tLoss: 0.843941\n",
      "Train Epoch: 10 [133376/359035 (37%)]\tLoss: 0.834653\n",
      "Train Epoch: 10 [134656/359035 (38%)]\tLoss: 0.878138\n",
      "Train Epoch: 10 [135936/359035 (38%)]\tLoss: 0.793516\n",
      "Train Epoch: 10 [137216/359035 (38%)]\tLoss: 0.795714\n",
      "Train Epoch: 10 [138496/359035 (39%)]\tLoss: 0.814919\n",
      "Train Epoch: 10 [139776/359035 (39%)]\tLoss: 0.935698\n",
      "Train Epoch: 10 [141056/359035 (39%)]\tLoss: 0.786869\n",
      "Train Epoch: 10 [142336/359035 (40%)]\tLoss: 0.972386\n",
      "Train Epoch: 10 [143616/359035 (40%)]\tLoss: 0.805652\n",
      "Train Epoch: 10 [144896/359035 (40%)]\tLoss: 0.810767\n",
      "Train Epoch: 10 [146176/359035 (41%)]\tLoss: 0.806086\n",
      "Train Epoch: 10 [147456/359035 (41%)]\tLoss: 0.874529\n",
      "Train Epoch: 10 [148736/359035 (41%)]\tLoss: 0.796469\n",
      "Train Epoch: 10 [150016/359035 (42%)]\tLoss: 0.836299\n",
      "Train Epoch: 10 [151296/359035 (42%)]\tLoss: 0.871181\n",
      "Train Epoch: 10 [152576/359035 (42%)]\tLoss: 0.854620\n",
      "Train Epoch: 10 [153856/359035 (43%)]\tLoss: 0.760508\n",
      "Train Epoch: 10 [155136/359035 (43%)]\tLoss: 0.855042\n",
      "Train Epoch: 10 [156416/359035 (44%)]\tLoss: 0.923466\n",
      "Train Epoch: 10 [157696/359035 (44%)]\tLoss: 0.847278\n",
      "Train Epoch: 10 [158976/359035 (44%)]\tLoss: 0.880158\n",
      "Train Epoch: 10 [160256/359035 (45%)]\tLoss: 0.799751\n",
      "Train Epoch: 10 [161536/359035 (45%)]\tLoss: 0.863791\n",
      "Train Epoch: 10 [162816/359035 (45%)]\tLoss: 0.885215\n",
      "Train Epoch: 10 [164096/359035 (46%)]\tLoss: 0.834746\n",
      "Train Epoch: 10 [165376/359035 (46%)]\tLoss: 0.868989\n",
      "Train Epoch: 10 [166656/359035 (46%)]\tLoss: 0.880608\n",
      "Train Epoch: 10 [167936/359035 (47%)]\tLoss: 0.897129\n",
      "Train Epoch: 10 [169216/359035 (47%)]\tLoss: 0.911409\n",
      "Train Epoch: 10 [170496/359035 (47%)]\tLoss: 0.850424\n",
      "Train Epoch: 10 [171776/359035 (48%)]\tLoss: 0.806183\n",
      "Train Epoch: 10 [173056/359035 (48%)]\tLoss: 0.874078\n",
      "Train Epoch: 10 [174336/359035 (49%)]\tLoss: 0.932326\n",
      "Train Epoch: 10 [175616/359035 (49%)]\tLoss: 0.880472\n",
      "Train Epoch: 10 [176896/359035 (49%)]\tLoss: 0.814727\n",
      "Train Epoch: 10 [178176/359035 (50%)]\tLoss: 0.894443\n",
      "Train Epoch: 10 [179456/359035 (50%)]\tLoss: 0.811427\n",
      "Train Epoch: 10 [180736/359035 (50%)]\tLoss: 0.845720\n",
      "Train Epoch: 10 [182016/359035 (51%)]\tLoss: 0.873976\n",
      "Train Epoch: 10 [183296/359035 (51%)]\tLoss: 0.893608\n",
      "Train Epoch: 10 [184576/359035 (51%)]\tLoss: 0.913651\n",
      "Train Epoch: 10 [185856/359035 (52%)]\tLoss: 0.909408\n",
      "Train Epoch: 10 [187136/359035 (52%)]\tLoss: 0.816314\n",
      "Train Epoch: 10 [188416/359035 (52%)]\tLoss: 0.846767\n",
      "Train Epoch: 10 [189696/359035 (53%)]\tLoss: 0.818009\n",
      "Train Epoch: 10 [190976/359035 (53%)]\tLoss: 0.888676\n",
      "Train Epoch: 10 [192256/359035 (54%)]\tLoss: 0.975300\n",
      "Train Epoch: 10 [193536/359035 (54%)]\tLoss: 0.895480\n",
      "Train Epoch: 10 [194816/359035 (54%)]\tLoss: 0.883741\n",
      "Train Epoch: 10 [196096/359035 (55%)]\tLoss: 0.851813\n",
      "Train Epoch: 10 [197376/359035 (55%)]\tLoss: 0.790232\n",
      "Train Epoch: 10 [198656/359035 (55%)]\tLoss: 0.923579\n",
      "Train Epoch: 10 [199936/359035 (56%)]\tLoss: 0.851138\n",
      "Train Epoch: 10 [201216/359035 (56%)]\tLoss: 0.861032\n",
      "Train Epoch: 10 [202496/359035 (56%)]\tLoss: 0.853999\n",
      "Train Epoch: 10 [203776/359035 (57%)]\tLoss: 0.816331\n",
      "Train Epoch: 10 [205056/359035 (57%)]\tLoss: 0.881774\n",
      "Train Epoch: 10 [206336/359035 (57%)]\tLoss: 0.829533\n",
      "Train Epoch: 10 [207616/359035 (58%)]\tLoss: 0.827236\n",
      "Train Epoch: 10 [208896/359035 (58%)]\tLoss: 0.899913\n",
      "Train Epoch: 10 [210176/359035 (59%)]\tLoss: 0.835577\n",
      "Train Epoch: 10 [211456/359035 (59%)]\tLoss: 0.952005\n",
      "Train Epoch: 10 [212736/359035 (59%)]\tLoss: 0.903145\n",
      "Train Epoch: 10 [214016/359035 (60%)]\tLoss: 0.835940\n",
      "Train Epoch: 10 [215296/359035 (60%)]\tLoss: 0.850318\n",
      "Train Epoch: 10 [216576/359035 (60%)]\tLoss: 0.856106\n",
      "Train Epoch: 10 [217856/359035 (61%)]\tLoss: 0.921037\n",
      "Train Epoch: 10 [219136/359035 (61%)]\tLoss: 0.820093\n",
      "Train Epoch: 10 [220416/359035 (61%)]\tLoss: 0.907990\n",
      "Train Epoch: 10 [221696/359035 (62%)]\tLoss: 0.914447\n",
      "Train Epoch: 10 [222976/359035 (62%)]\tLoss: 0.740841\n",
      "Train Epoch: 10 [224256/359035 (62%)]\tLoss: 0.870891\n",
      "Train Epoch: 10 [225536/359035 (63%)]\tLoss: 0.850384\n",
      "Train Epoch: 10 [226816/359035 (63%)]\tLoss: 0.915110\n",
      "Train Epoch: 10 [228096/359035 (64%)]\tLoss: 0.808429\n",
      "Train Epoch: 10 [229376/359035 (64%)]\tLoss: 0.912836\n",
      "Train Epoch: 10 [230656/359035 (64%)]\tLoss: 0.785324\n",
      "Train Epoch: 10 [231936/359035 (65%)]\tLoss: 0.977680\n",
      "Train Epoch: 10 [233216/359035 (65%)]\tLoss: 0.891076\n",
      "Train Epoch: 10 [234496/359035 (65%)]\tLoss: 0.978373\n",
      "Train Epoch: 10 [235776/359035 (66%)]\tLoss: 0.900594\n",
      "Train Epoch: 10 [237056/359035 (66%)]\tLoss: 0.815712\n",
      "Train Epoch: 10 [238336/359035 (66%)]\tLoss: 0.853047\n",
      "Train Epoch: 10 [239616/359035 (67%)]\tLoss: 0.785708\n",
      "Train Epoch: 10 [240896/359035 (67%)]\tLoss: 0.858639\n",
      "Train Epoch: 10 [242176/359035 (67%)]\tLoss: 0.830990\n",
      "Train Epoch: 10 [243456/359035 (68%)]\tLoss: 0.852397\n",
      "Train Epoch: 10 [244736/359035 (68%)]\tLoss: 0.859423\n",
      "Train Epoch: 10 [246016/359035 (69%)]\tLoss: 0.873512\n",
      "Train Epoch: 10 [247296/359035 (69%)]\tLoss: 0.847873\n",
      "Train Epoch: 10 [248576/359035 (69%)]\tLoss: 0.841578\n",
      "Train Epoch: 10 [249856/359035 (70%)]\tLoss: 0.826487\n",
      "Train Epoch: 10 [251136/359035 (70%)]\tLoss: 0.818115\n",
      "Train Epoch: 10 [252416/359035 (70%)]\tLoss: 0.849387\n",
      "Train Epoch: 10 [253696/359035 (71%)]\tLoss: 0.883577\n",
      "Train Epoch: 10 [254976/359035 (71%)]\tLoss: 0.818288\n",
      "Train Epoch: 10 [256256/359035 (71%)]\tLoss: 0.785910\n",
      "Train Epoch: 10 [257536/359035 (72%)]\tLoss: 0.890173\n",
      "Train Epoch: 10 [258816/359035 (72%)]\tLoss: 0.834261\n",
      "Train Epoch: 10 [260096/359035 (72%)]\tLoss: 0.866053\n",
      "Train Epoch: 10 [261376/359035 (73%)]\tLoss: 0.979092\n",
      "Train Epoch: 10 [262656/359035 (73%)]\tLoss: 0.863376\n",
      "Train Epoch: 10 [263936/359035 (74%)]\tLoss: 0.869279\n",
      "Train Epoch: 10 [265216/359035 (74%)]\tLoss: 0.776894\n",
      "Train Epoch: 10 [266496/359035 (74%)]\tLoss: 0.916382\n",
      "Train Epoch: 10 [267776/359035 (75%)]\tLoss: 0.835685\n",
      "Train Epoch: 10 [269056/359035 (75%)]\tLoss: 0.828980\n",
      "Train Epoch: 10 [270336/359035 (75%)]\tLoss: 0.771820\n",
      "Train Epoch: 10 [271616/359035 (76%)]\tLoss: 0.846396\n",
      "Train Epoch: 10 [272896/359035 (76%)]\tLoss: 0.872143\n",
      "Train Epoch: 10 [274176/359035 (76%)]\tLoss: 0.870050\n",
      "Train Epoch: 10 [275456/359035 (77%)]\tLoss: 0.856298\n",
      "Train Epoch: 10 [276736/359035 (77%)]\tLoss: 0.884389\n",
      "Train Epoch: 10 [278016/359035 (77%)]\tLoss: 0.911199\n",
      "Train Epoch: 10 [279296/359035 (78%)]\tLoss: 0.770897\n",
      "Train Epoch: 10 [280576/359035 (78%)]\tLoss: 0.825242\n",
      "Train Epoch: 10 [281856/359035 (79%)]\tLoss: 0.874967\n",
      "Train Epoch: 10 [283136/359035 (79%)]\tLoss: 0.842511\n",
      "Train Epoch: 10 [284416/359035 (79%)]\tLoss: 0.935183\n",
      "Train Epoch: 10 [285696/359035 (80%)]\tLoss: 0.776013\n",
      "Train Epoch: 10 [286976/359035 (80%)]\tLoss: 0.806190\n",
      "Train Epoch: 10 [288256/359035 (80%)]\tLoss: 0.981049\n",
      "Train Epoch: 10 [289536/359035 (81%)]\tLoss: 1.001184\n",
      "Train Epoch: 10 [290816/359035 (81%)]\tLoss: 0.865113\n",
      "Train Epoch: 10 [292096/359035 (81%)]\tLoss: 0.902777\n",
      "Train Epoch: 10 [293376/359035 (82%)]\tLoss: 0.806502\n",
      "Train Epoch: 10 [294656/359035 (82%)]\tLoss: 0.867473\n",
      "Train Epoch: 10 [295936/359035 (82%)]\tLoss: 0.855774\n",
      "Train Epoch: 10 [297216/359035 (83%)]\tLoss: 0.850545\n",
      "Train Epoch: 10 [298496/359035 (83%)]\tLoss: 0.882501\n",
      "Train Epoch: 10 [299776/359035 (83%)]\tLoss: 0.906961\n",
      "Train Epoch: 10 [301056/359035 (84%)]\tLoss: 0.967633\n",
      "Train Epoch: 10 [302336/359035 (84%)]\tLoss: 0.787727\n",
      "Train Epoch: 10 [303616/359035 (85%)]\tLoss: 0.938087\n",
      "Train Epoch: 10 [304896/359035 (85%)]\tLoss: 0.817270\n",
      "Train Epoch: 10 [306176/359035 (85%)]\tLoss: 0.882286\n",
      "Train Epoch: 10 [307456/359035 (86%)]\tLoss: 0.825116\n",
      "Train Epoch: 10 [308736/359035 (86%)]\tLoss: 0.801963\n",
      "Train Epoch: 10 [310016/359035 (86%)]\tLoss: 0.822672\n",
      "Train Epoch: 10 [311296/359035 (87%)]\tLoss: 0.912975\n",
      "Train Epoch: 10 [312576/359035 (87%)]\tLoss: 0.741919\n",
      "Train Epoch: 10 [313856/359035 (87%)]\tLoss: 0.793019\n",
      "Train Epoch: 10 [315136/359035 (88%)]\tLoss: 0.816126\n",
      "Train Epoch: 10 [316416/359035 (88%)]\tLoss: 0.760905\n",
      "Train Epoch: 10 [317696/359035 (88%)]\tLoss: 0.893084\n",
      "Train Epoch: 10 [318976/359035 (89%)]\tLoss: 0.860978\n",
      "Train Epoch: 10 [320256/359035 (89%)]\tLoss: 0.787916\n",
      "Train Epoch: 10 [321536/359035 (90%)]\tLoss: 0.952109\n",
      "Train Epoch: 10 [322816/359035 (90%)]\tLoss: 0.873589\n",
      "Train Epoch: 10 [324096/359035 (90%)]\tLoss: 0.919559\n",
      "Train Epoch: 10 [325376/359035 (91%)]\tLoss: 0.808318\n",
      "Train Epoch: 10 [326656/359035 (91%)]\tLoss: 0.868724\n",
      "Train Epoch: 10 [327936/359035 (91%)]\tLoss: 0.871983\n",
      "Train Epoch: 10 [329216/359035 (92%)]\tLoss: 0.940155\n",
      "Train Epoch: 10 [330496/359035 (92%)]\tLoss: 0.826734\n",
      "Train Epoch: 10 [331776/359035 (92%)]\tLoss: 0.797284\n",
      "Train Epoch: 10 [333056/359035 (93%)]\tLoss: 0.863496\n",
      "Train Epoch: 10 [334336/359035 (93%)]\tLoss: 0.849950\n",
      "Train Epoch: 10 [335616/359035 (93%)]\tLoss: 0.991877\n",
      "Train Epoch: 10 [336896/359035 (94%)]\tLoss: 0.856717\n",
      "Train Epoch: 10 [338176/359035 (94%)]\tLoss: 0.936224\n",
      "Train Epoch: 10 [339456/359035 (95%)]\tLoss: 0.810436\n",
      "Train Epoch: 10 [340736/359035 (95%)]\tLoss: 0.853977\n",
      "Train Epoch: 10 [342016/359035 (95%)]\tLoss: 0.887172\n",
      "Train Epoch: 10 [343296/359035 (96%)]\tLoss: 0.918155\n",
      "Train Epoch: 10 [344576/359035 (96%)]\tLoss: 0.868189\n",
      "Train Epoch: 10 [345856/359035 (96%)]\tLoss: 0.864279\n",
      "Train Epoch: 10 [347136/359035 (97%)]\tLoss: 0.832327\n",
      "Train Epoch: 10 [348416/359035 (97%)]\tLoss: 0.819362\n",
      "Train Epoch: 10 [349696/359035 (97%)]\tLoss: 0.880901\n",
      "Train Epoch: 10 [350976/359035 (98%)]\tLoss: 0.836275\n",
      "Train Epoch: 10 [352256/359035 (98%)]\tLoss: 0.938725\n",
      "Train Epoch: 10 [353536/359035 (98%)]\tLoss: 0.861124\n",
      "Train Epoch: 10 [354816/359035 (99%)]\tLoss: 0.879073\n",
      "Train Epoch: 10 [356096/359035 (99%)]\tLoss: 0.848329\n",
      "Train Epoch: 10 [357376/359035 (100%)]\tLoss: 0.826288\n",
      "Train Epoch: 10 [358656/359035 (100%)]\tLoss: 0.860368\n",
      "Performance on test set: Average loss: 0.9263, Accuracy: 135/200 (68%)\n",
      "Epoch took: 2572.836s\n",
      "\n",
      "Example prediction\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: 0to20: 0.5502, 100to500: 0.1832, 20to50: 0.1449, 500to_above: 0.1217\n",
      "Test performance per epoch\n",
      "epoch\tloss\tacc\n",
      "1\t0.946376953125\t0.665\n",
      "2\t0.9340777587890625\t0.67\n",
      "3\t0.932445297241211\t0.67\n",
      "4\t0.9390990447998047\t0.67\n",
      "5\t0.929759521484375\t0.665\n",
      "6\t0.9242205810546875\t0.67\n",
      "7\t0.924305419921875\t0.67\n",
      "8\t0.9149957275390626\t0.67\n",
      "9\t0.9281902313232422\t0.665\n",
      "10\t0.92627197265625\t0.675\n",
      "Min loss: 0.9149957275390626 - Epoch: 8\n",
      "Max acc: 0.675 - Epoch: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Discriminator(\n",
       "   (encoder): GPT2LMHeadModel(\n",
       "     (transformer): GPT2Model(\n",
       "       (wte): Embedding(50257, 1024)\n",
       "       (wpe): Embedding(1024, 1024)\n",
       "       (drop): Dropout(p=0.1, inplace=False)\n",
       "       (h): ModuleList(\n",
       "         (0): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (1): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (2): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (3): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (4): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (5): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (6): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (7): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (8): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (9): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (10): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (11): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (12): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (13): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (14): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (15): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (16): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (17): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (18): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (19): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (20): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (21): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (22): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "         (23): Block(\n",
       "           (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "   )\n",
       "   (classifier_head): ClassificationHead(\n",
       "     (hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "     (mlp): Linear(in_features=512, out_features=4, bias=True)\n",
       "   )\n",
       " ),\n",
       " {'class_size': 4,\n",
       "  'embed_size': 1024,\n",
       "  'pretrained_model': 'gpt2-medium',\n",
       "  'class_vocab': {'0to20': 0, '100to500': 1, '20to50': 2, '500to_above': 3},\n",
       "  'default_class': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from torchtext import data as torchtext_data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "EPSILON = 1e-10\n",
    "example_sentence = \"This is incredible! I love it, this is the best chicken I have ever had.\"\n",
    "max_length_seq = 128\n",
    "df = pd.read_csv('../input/eluvio-dataset/Eluvio_DS_Challenge.csv')\n",
    "df = df.sort_values(by='up_votes').reset_index(drop=True)\n",
    "df['labels'] = '0to20'\n",
    "df['labels'][400000:440000] = '20to50'\n",
    "df['labels'][440000:490000] = '100to500'\n",
    "df['labels'][490000:] = '500to_above'\n",
    "df = df[150000:]\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df[['labels','title']].to_csv('dataset.csv',sep='\\t',index=False,header=False)\n",
    "\n",
    "\n",
    "class ClassificationHead(torch.nn.Module):\n",
    "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
    "\n",
    "    def __init__(self, class_size, embed_size):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.class_size = class_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden = torch.nn.Linear(embed_size, 512)\n",
    "        self.mlp = torch.nn.Linear(512, class_size)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        hidden_state = F.relu(self.hidden(hidden_state))\n",
    "        logits = self.mlp(hidden_state)\n",
    "        return logits\n",
    "    \n",
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\"Transformer encoder followed by a Classification Head\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            class_size=None,\n",
    "            pretrained_model=\"gpt2-medium\",\n",
    "            classifier_head=None,\n",
    "            cached_mode=False,\n",
    "            device='cpu'\n",
    "    ):\n",
    "        super(Discriminator, self).__init__()\n",
    "        if pretrained_model.startswith(\"gpt2\"):\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
    "            self.encoder = GPT2LMHeadModel.from_pretrained(pretrained_model)\n",
    "            self.embed_size = self.encoder.transformer.config.hidden_size\n",
    "        elif pretrained_model.startswith(\"bert\"):\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "            self.encoder = BertModel.from_pretrained(pretrained_model)\n",
    "            self.embed_size = self.encoder.config.hidden_size\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"{} model not yet supported\".format(pretrained_model)\n",
    "            )\n",
    "        if classifier_head:\n",
    "            self.classifier_head = classifier_head\n",
    "        else:\n",
    "            if not class_size:\n",
    "                raise ValueError(\"must specify class_size\")\n",
    "            self.classifier_head = ClassificationHead(\n",
    "                class_size=class_size,\n",
    "                embed_size=self.embed_size\n",
    "            )\n",
    "        self.cached_mode = cached_mode\n",
    "        self.device = device\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.classifier_head\n",
    "\n",
    "    def train_custom(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier_head.train()\n",
    "\n",
    "    def avg_representation(self, x):\n",
    "        mask = x.ne(0).unsqueeze(2).repeat(\n",
    "            1, 1, self.embed_size\n",
    "        ).float().to(self.device).detach()\n",
    "        if hasattr(self.encoder, 'transformer'):\n",
    "            # for gpt2\n",
    "            hidden, _ = self.encoder.transformer(x)\n",
    "        else:\n",
    "            # for bert\n",
    "            hidden, _ = self.encoder(x)\n",
    "        masked_hidden = hidden * mask\n",
    "        avg_hidden = torch.sum(masked_hidden, dim=1) / (\n",
    "                torch.sum(mask, dim=1).detach() + EPSILON\n",
    "        )\n",
    "        return avg_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cached_mode:\n",
    "            avg_hidden = x.to(self.device)\n",
    "        else:\n",
    "            avg_hidden = self.avg_representation(x.to(self.device))\n",
    "\n",
    "        logits = self.classifier_head(avg_hidden)\n",
    "        probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def predict(self, input_sentence):\n",
    "        input_t = self.tokenizer.encode(input_sentence)\n",
    "        input_t = torch.tensor([input_t], dtype=torch.long, device=self.device)\n",
    "        if self.cached_mode:\n",
    "            input_t = self.avg_representation(input_t)\n",
    "\n",
    "        log_probs = self(input_t).data.cpu().numpy().flatten().tolist()\n",
    "        prob = [math.exp(log_prob) for log_prob in log_probs]\n",
    "        return prob\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"Reads source and target sequences from txt files.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (source and target).\"\"\"\n",
    "        data = {}\n",
    "        data[\"X\"] = self.X[index]\n",
    "        data[\"y\"] = self.y[index]\n",
    "        return data\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    def pad_sequences(sequences):\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "        padded_sequences = torch.zeros(\n",
    "            len(sequences),\n",
    "            max(lengths)\n",
    "        ).long()  # padding value = 0\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_sequences[i, :end] = seq[:end]\n",
    "\n",
    "        return padded_sequences, lengths\n",
    "\n",
    "    item_info = {}\n",
    "    for key in data[0].keys():\n",
    "        item_info[key] = [d[key] for d in data]\n",
    "\n",
    "    x_batch, _ = pad_sequences(item_info[\"X\"])\n",
    "    y_batch = torch.tensor(item_info[\"y\"], dtype=torch.long)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "\n",
    "def cached_collate_fn(data):\n",
    "    item_info = {}\n",
    "    for key in data[0].keys():\n",
    "        item_info[key] = [d[key] for d in data]\n",
    "\n",
    "    x_batch = torch.cat(item_info[\"X\"], 0)\n",
    "    y_batch = torch.tensor(item_info[\"y\"], dtype=torch.long)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "\n",
    "def train_epoch(data_loader, discriminator, optimizer,\n",
    "                epoch=0, log_interval=10, device='cpu'):\n",
    "    samples_so_far = 0\n",
    "    discriminator.train_custom()\n",
    "    for batch_idx, (input_t, target_t) in enumerate(data_loader):\n",
    "        input_t, target_t = input_t.to(device), target_t.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_t = discriminator(input_t)\n",
    "        loss = F.nll_loss(output_t, target_t)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        samples_so_far += len(input_t)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch + 1,\n",
    "                    samples_so_far, len(data_loader.dataset),\n",
    "                    100 * samples_so_far / len(data_loader.dataset), loss.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def evaluate_performance(data_loader, discriminator, device='cpu'):\n",
    "    discriminator.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_t, target_t in data_loader:\n",
    "            input_t, target_t = input_t.to(device), target_t.to(device)\n",
    "            output_t = discriminator(input_t)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output_t, target_t, reduction=\"sum\").item()\n",
    "            # get the index of the max log-probability\n",
    "            pred_t = output_t.argmax(dim=1, keepdim=True)\n",
    "            correct += pred_t.eq(target_t.view_as(pred_t)).sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"Performance on test set: \"\n",
    "        \"Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss, correct, len(data_loader.dataset),\n",
    "            100. * accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def predict(input_sentence, model, classes, cached=False, device='cpu'):\n",
    "    input_t = model.tokenizer.encode(input_sentence)\n",
    "    input_t = torch.tensor([input_t], dtype=torch.long, device=device)\n",
    "    if cached:\n",
    "        input_t = model.avg_representation(input_t)\n",
    "\n",
    "    log_probs = model(input_t).data.cpu().numpy().flatten().tolist()\n",
    "    print(\"Input sentence:\", input_sentence)\n",
    "    print(\"Predictions:\", \", \".join(\n",
    "        \"{}: {:.4f}\".format(c, math.exp(log_prob)) for c, log_prob in\n",
    "        zip(classes, log_probs)\n",
    "    ))\n",
    "\n",
    "\n",
    "def get_cached_data_loader(dataset, batch_size, discriminator,\n",
    "                           shuffle=False, device='cpu'):\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              collate_fn=collate_fn)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(data_loader, ascii=True)):\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            avg_rep = discriminator.avg_representation(x).cpu().detach()\n",
    "            avg_rep_list = torch.unbind(avg_rep.unsqueeze(1))\n",
    "            xs += avg_rep_list\n",
    "            ys += y.cpu().numpy().tolist()\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=Dataset(xs, ys),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=cached_collate_fn)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def get_idx2class(dataset_fp):\n",
    "    classes = set()\n",
    "    with open(dataset_fp) as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in tqdm(csv_reader, ascii=True):\n",
    "            if row:\n",
    "                classes.add(row[0])\n",
    "\n",
    "    return sorted(classes)\n",
    "\n",
    "\n",
    "def get_generic_dataset(dataset_fp, tokenizer, device,\n",
    "                        idx2class=None, add_eos_token=False):\n",
    "    if not idx2class:\n",
    "        idx2class = get_idx2class(dataset_fp)\n",
    "    class2idx = {c: i for i, c in enumerate(idx2class)}\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    df = pd.read_csv(dataset_fp,sep='\\t')\n",
    "    for row in df.values:\n",
    "        label = row[0]\n",
    "        text = row[1]\n",
    "        try:\n",
    "            seq = tokenizer.encode(text)\n",
    "            if (len(seq) < max_length_seq):\n",
    "                if add_eos_token:\n",
    "                    seq = [50256] + seq\n",
    "                seq = torch.tensor(seq,device=device,dtype=torch.long)\n",
    "            else:\n",
    "                print(\"Line {} is longer than maximum length {}\".format(i, max_length_seq))\n",
    "                continue\n",
    "            x.append(seq)\n",
    "            y.append(class2idx[label])\n",
    "\n",
    "        except:\n",
    "            print(\"Error tokenizing line {}, skipping it\".format(row[1]))\n",
    "            pass\n",
    "    return Dataset(x, y)\n",
    "\n",
    "\n",
    "def train_discriminator(\n",
    "        dataset='generic',\n",
    "        dataset_fp='dataset.csv',\n",
    "        pretrained_model=\"gpt2-medium\",\n",
    "        epochs=10,\n",
    "        learning_rate=0.0001,\n",
    "        batch_size=256,\n",
    "        log_interval=5,\n",
    "        save_model=True,\n",
    "        cached=False,\n",
    "        no_cuda=False,\n",
    "        output_fp='.'\n",
    "):\n",
    "    device = \"cuda\"\n",
    "    add_eos_token = pretrained_model.startswith(\"gpt2\")\n",
    "\n",
    "    if save_model:\n",
    "        if not os.path.exists(output_fp):\n",
    "            os.makedirs(output_fp)\n",
    "    classifier_head_meta_fp = os.path.join(\n",
    "        output_fp, \"{}_classifier_head_meta.json\".format(dataset)\n",
    "    )\n",
    "    classifier_head_fp_pattern = os.path.join(\n",
    "        output_fp, \"{}_classifier_head_epoch\".format(dataset) + \"_{}.pt\"\n",
    "    )\n",
    "\n",
    "    print(\"Preprocessing {} dataset...\".format(dataset))\n",
    "    start = time.time()\n",
    "\n",
    "    if dataset == \"generic\":\n",
    "        # This assumes the input dataset is a TSV with the following structure:\n",
    "        # class \\t text\n",
    "\n",
    "        if dataset_fp is None:\n",
    "            raise ValueError(\"When generic dataset is selected, \"\n",
    "                             \"dataset_fp needs to be specified aswell.\")\n",
    "\n",
    "        idx2class = get_idx2class(dataset_fp)\n",
    "\n",
    "        discriminator = Discriminator(\n",
    "            class_size=len(idx2class),\n",
    "            pretrained_model=pretrained_model,\n",
    "            cached_mode=cached,\n",
    "            device=device\n",
    "        ).to(device)\n",
    "\n",
    "        full_dataset = get_generic_dataset(\n",
    "            dataset_fp, discriminator.tokenizer, device,\n",
    "            idx2class=idx2class, add_eos_token=add_eos_token\n",
    "        )\n",
    "        train_size = len(full_dataset)-200\n",
    "        test_size = 200\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "            full_dataset,\n",
    "            [train_size, test_size]\n",
    "        )\n",
    "\n",
    "        discriminator_meta = {\n",
    "            \"class_size\": len(idx2class),\n",
    "            \"embed_size\": discriminator.embed_size,\n",
    "            \"pretrained_model\": pretrained_model,\n",
    "            \"class_vocab\": {c: i for i, c in enumerate(idx2class)},\n",
    "            \"default_class\": 0,\n",
    "        }\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Preprocessed {} data points\".format(\n",
    "        len(train_dataset) + len(test_dataset))\n",
    "    )\n",
    "    print(\"Data preprocessing took: {:.3f}s\".format(end - start))\n",
    "\n",
    "    if cached:\n",
    "        print(\"Building representation cache...\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        train_loader = get_cached_data_loader(\n",
    "            train_dataset, batch_size, discriminator,\n",
    "            shuffle=True, device=device\n",
    "        )\n",
    "\n",
    "        test_loader = get_cached_data_loader(\n",
    "            test_dataset, batch_size, discriminator, device=device\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Building representation cache took: {:.3f}s\".format(end - start))\n",
    "\n",
    "    else:\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  collate_fn=collate_fn)\n",
    "\n",
    "    if save_model:\n",
    "        with open(classifier_head_meta_fp, \"w\") as meta_file:\n",
    "            json.dump(discriminator_meta, meta_file)\n",
    "\n",
    "    optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        print(\"\\nEpoch\", epoch + 1)\n",
    "\n",
    "        train_epoch(\n",
    "            discriminator=discriminator,\n",
    "            data_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            log_interval=log_interval,\n",
    "            device=device\n",
    "        )\n",
    "        test_loss, test_accuracy = evaluate_performance(\n",
    "            data_loader=test_loader,\n",
    "            discriminator=discriminator,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Epoch took: {:.3f}s\".format(end - start))\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(\"\\nExample prediction\")\n",
    "        predict(example_sentence, discriminator, idx2class,\n",
    "                cached=cached, device=device)\n",
    "\n",
    "        if save_model:\n",
    "            # torch.save(discriminator.state_dict(),\n",
    "            #           \"{}_discriminator_{}.pt\".format(\n",
    "            #               args.dataset, epoch + 1\n",
    "            #               ))\n",
    "            torch.save(discriminator.get_classifier().state_dict(),\n",
    "                       classifier_head_fp_pattern.format(epoch + 1))\n",
    "\n",
    "    min_loss = float(\"inf\")\n",
    "    min_loss_epoch = 0\n",
    "    max_acc = 0.0\n",
    "    max_acc_epoch = 0\n",
    "    print(\"Test performance per epoch\")\n",
    "    print(\"epoch\\tloss\\tacc\")\n",
    "    for e, (loss, acc) in enumerate(zip(test_losses, test_accuracies)):\n",
    "        print(\"{}\\t{}\\t{}\".format(e + 1, loss, acc))\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            min_loss_epoch = e + 1\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            max_acc_epoch = e + 1\n",
    "    print(\"Min loss: {} - Epoch: {}\".format(min_loss, min_loss_epoch))\n",
    "    print(\"Max acc: {} - Epoch: {}\".format(max_acc, max_acc_epoch))\n",
    "\n",
    "    return discriminator, discriminator_meta\n",
    "\n",
    "\n",
    "def load_classifier_head(weights_path, meta_path, device='cpu'):\n",
    "    with open(meta_path, 'r', encoding=\"utf8\") as f:\n",
    "        meta_params = json.load(f)\n",
    "    classifier_head = ClassificationHead(\n",
    "        class_size=meta_params['class_size'],\n",
    "        embed_size=meta_params['embed_size']\n",
    "    ).to(device)\n",
    "    classifier_head.load_state_dict(\n",
    "        torch.load(weights_path, map_location=device))\n",
    "    classifier_head.eval()\n",
    "    return classifier_head, meta_params\n",
    "\n",
    "\n",
    "def load_discriminator(weights_path, meta_path, device='cpu'):\n",
    "    classifier_head, meta_param = load_classifier_head(\n",
    "        weights_path, meta_path, device\n",
    "    )\n",
    "    discriminator =  Discriminator(\n",
    "        pretrained_model=meta_param['pretrained_model'],\n",
    "        classifier_head=classifier_head,\n",
    "        cached_mode=False,\n",
    "        device=device\n",
    "    )\n",
    "    return discriminator, meta_param\n",
    "\n",
    "train_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-person",
   "metadata": {
    "papermill": {
     "duration": 0.8679,
     "end_time": "2021-04-17T23:02:48.902790",
     "exception": false,
     "start_time": "2021-04-17T23:02:48.034890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25985.360611,
   "end_time": "2021-04-17T23:02:53.193566",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-17T15:49:47.832955",
   "version": "2.3.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0288aff47f694f30be560991d97b51a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "035d6c584baa474ca6415861fb4f7152": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "03619b3a2b824c9b98c125d1a9056fe2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0c1e9e0c0d984f018fc6fc596f701ad1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_93edb53774c5433ca0c172e0f984d1b5",
        "IPY_MODEL_82c3ebaffc85498d930b61cb9fc35f51",
        "IPY_MODEL_1885c659fd81470dab15c88e72d62abf"
       ],
       "layout": "IPY_MODEL_d15589d5c321471db221fd2f19896473"
      }
     },
     "1390031f729544b5b5564fee8d235345": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e52022faacfe4dc88bbc32435780b67f",
        "IPY_MODEL_3c01bf8d22e4423c8f914f2eb34ed97f",
        "IPY_MODEL_f3925d58a80348eea9227b5c42be25d4"
       ],
       "layout": "IPY_MODEL_035d6c584baa474ca6415861fb4f7152"
      }
     },
     "1885c659fd81470dab15c88e72d62abf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cfb91e750f344397a6ef141bc4d8e950",
       "placeholder": "​",
       "style": "IPY_MODEL_e27e6407c0fd4f5eac0a17674d26dc80",
       "value": " 456k/456k [00:00&lt;00:00, 865kB/s]"
      }
     },
     "2119e126703a4911884abb85ab51cdef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_66b1be30aca646e5b9ae4afb20c2639b",
        "IPY_MODEL_3719bbb8f76c4484ac19e3c89e397e5e",
        "IPY_MODEL_4e41749ba8154bbe80f28456b0e8b8be"
       ],
       "layout": "IPY_MODEL_990a3711dccc4af18a0be6d1bc22bb19"
      }
     },
     "3719bbb8f76c4484ac19e3c89e397e5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ec6014ba7c8a4a40a2695d08995f4fad",
       "max": 718.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c797bd5ccf2a443a844a40d1c13ec9df",
       "value": 718.0
      }
     },
     "3c01bf8d22e4423c8f914f2eb34ed97f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_57cd6d63f9b144bca446254ded672d60",
       "max": 1520013706.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bdd2317aebd14b87b49a80c3680cb150",
       "value": 1520013706.0
      }
     },
     "3e330c74bd794069b9f99b9a41529eb1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46598142688f4bbda2529abaf6ca9926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6fc3328cbd144f65a48a5a88734263c0",
       "max": 1042301.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fd0503f62d614b4796ea089dd9c16a40",
       "value": 1042301.0
      }
     },
     "48bbef5111ce4495906eab946ef59bf6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4e41749ba8154bbe80f28456b0e8b8be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3e330c74bd794069b9f99b9a41529eb1",
       "placeholder": "​",
       "style": "IPY_MODEL_60e39b29d9ae4b9486bc7d47ac4d1d77",
       "value": " 718/718 [00:00&lt;00:00, 25.0kB/s]"
      }
     },
     "52b42e96beb740f0b999e247ff3087cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "57cd6d63f9b144bca446254ded672d60": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "60e39b29d9ae4b9486bc7d47ac4d1d77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "66b1be30aca646e5b9ae4afb20c2639b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e5e3dc8e00c944218ac95777bc7f32ee",
       "placeholder": "​",
       "style": "IPY_MODEL_d1675643729b40e1a9e8bf3be39ff231",
       "value": "Downloading: 100%"
      }
     },
     "6fc3328cbd144f65a48a5a88734263c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e80d93ed2384610a92e3bf49f18afdb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82c3ebaffc85498d930b61cb9fc35f51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7e80d93ed2384610a92e3bf49f18afdb",
       "max": 456318.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cc8b3ce99cef49bc92426d3952b18188",
       "value": 456318.0
      }
     },
     "8d62d6a43cab426a80f6cd95c1ce1ea2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f409c7dd2585412d93b6d98e9640838f",
       "placeholder": "​",
       "style": "IPY_MODEL_03619b3a2b824c9b98c125d1a9056fe2",
       "value": " 1.04M/1.04M [00:00&lt;00:00, 2.90MB/s]"
      }
     },
     "93edb53774c5433ca0c172e0f984d1b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_52b42e96beb740f0b999e247ff3087cb",
       "placeholder": "​",
       "style": "IPY_MODEL_a986087e2cb04f94ac7b65113c8d2935",
       "value": "Downloading: 100%"
      }
     },
     "990a3711dccc4af18a0be6d1bc22bb19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a986087e2cb04f94ac7b65113c8d2935": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b1af21bd5cf04a1b8ec59182433f357f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bdd2317aebd14b87b49a80c3680cb150": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c797bd5ccf2a443a844a40d1c13ec9df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cb70ff2c434e4732b2612eef26624ad6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc8b3ce99cef49bc92426d3952b18188": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cfb91e750f344397a6ef141bc4d8e950": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d15589d5c321471db221fd2f19896473": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1675643729b40e1a9e8bf3be39ff231": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e27e6407c0fd4f5eac0a17674d26dc80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e52022faacfe4dc88bbc32435780b67f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e9411236696f4deb9edad2ed634d6ef3",
       "placeholder": "​",
       "style": "IPY_MODEL_f87ca30c78044f86b2667936900d3acf",
       "value": "Downloading: 100%"
      }
     },
     "e5e3dc8e00c944218ac95777bc7f32ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9411236696f4deb9edad2ed634d6ef3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec6014ba7c8a4a40a2695d08995f4fad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f10b00131736445596b02fc07049c1e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b1af21bd5cf04a1b8ec59182433f357f",
       "placeholder": "​",
       "style": "IPY_MODEL_0288aff47f694f30be560991d97b51a2",
       "value": "Downloading: 100%"
      }
     },
     "f3925d58a80348eea9227b5c42be25d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cb70ff2c434e4732b2612eef26624ad6",
       "placeholder": "​",
       "style": "IPY_MODEL_48bbef5111ce4495906eab946ef59bf6",
       "value": " 1.52G/1.52G [01:07&lt;00:00, 18.5MB/s]"
      }
     },
     "f409c7dd2585412d93b6d98e9640838f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f69b89fb37c0489ebe4b373a1a8c81cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f87ca30c78044f86b2667936900d3acf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fd0503f62d614b4796ea089dd9c16a40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ff1a1118bdae4610bcf1abd10ef5252c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f10b00131736445596b02fc07049c1e2",
        "IPY_MODEL_46598142688f4bbda2529abaf6ca9926",
        "IPY_MODEL_8d62d6a43cab426a80f6cd95c1ce1ea2"
       ],
       "layout": "IPY_MODEL_f69b89fb37c0489ebe4b373a1a8c81cb"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
