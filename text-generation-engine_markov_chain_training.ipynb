{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reasonable-technique",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-15T20:42:15.882835Z",
     "iopub.status.busy": "2021-04-15T20:42:15.860634Z",
     "iopub.status.idle": "2021-04-15T20:42:29.924177Z",
     "shell.execute_reply": "2021-04-15T20:42:29.924872Z"
    },
    "papermill": {
     "duration": 14.109674,
     "end_time": "2021-04-15T20:42:29.925204",
     "exception": false,
     "start_time": "2021-04-15T20:42:15.815530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\r\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\r\n",
      "Collecting beautifulsoup4\r\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 115 kB 843 kB/s \r\n",
      "\u001b[?25hCollecting soupsieve>1.2\r\n",
      "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\r\n",
      "Building wheels for collected packages: bs4\r\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=705f2cf4ccfcda6c1183fb851bf130d0feb5d05243b0daacc129f507f75598e8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/9e/ba/20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\r\n",
      "Successfully built bs4\r\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\r\n",
      "Successfully installed beautifulsoup4-4.9.3 bs4-0.0.1 soupsieve-2.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "import scipy.sparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import nltk\n",
    "import pickle\n",
    "from string import ascii_lowercase, digits\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from collections import Counter\n",
    "\n",
    "class Text:\n",
    "    def __init__(self, input_text, token2ind=None, ind2token=None):\n",
    "        self.content = input_text\n",
    "        self.tokens, self.tokens_distinct = self.tokenize()\n",
    "\n",
    "        if token2ind != None and ind2token != None:\n",
    "            self.token2ind, self.ind2token = token2ind, ind2token\n",
    "        else:\n",
    "            self.token2ind, self.ind2token = self.create_word_mapping(self.tokens_distinct)\n",
    "\n",
    "        self.tokens_ind = [self.token2ind[token] if token in self.token2ind.keys() else self.token2ind['<| unknown |>']\n",
    "                           for token in self.tokens]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_distinct)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_word_mapping(values_list):\n",
    "        values_list.append('<| unknown |>')\n",
    "        value2ind = {value: ind for ind, value in enumerate(values_list)}\n",
    "        ind2value = dict(enumerate(values_list))\n",
    "        return value2ind, ind2value\n",
    "\n",
    "    def preprocess(self):\n",
    "        punctuation_pad = '!?.,:-;'\n",
    "        punctuation_remove = '\"()_\\n'\n",
    "\n",
    "        self.content_preprocess = re.sub(r'(\\S)(\\n)(\\S)', r'\\1 \\2 \\3', self.content)\n",
    "        self.content_preprocess = self.content_preprocess.translate(str.maketrans('', '', punctuation_remove))\n",
    "        self.content_preprocess = self.content_preprocess.translate(\n",
    "            str.maketrans({key: ' {0} '.format(key) for key in punctuation_pad}))\n",
    "        self.content_preprocess = re.sub(' +', ' ', self.content_preprocess)\n",
    "        self.content = self.content_preprocess.strip()\n",
    "        \n",
    "    def tokenize(self):\n",
    "        self.preprocess()\n",
    "        tokens = self.content.split(' ')\n",
    "        return tokens, list(set(tokens))\n",
    "\n",
    "    def tokens_info(self):\n",
    "        print('total tokens: %d, distinct tokens: %d' % (len(self.tokens), len(self.tokens_distinct)))\n",
    "        \n",
    "class Chain:\n",
    "    def __init__(self, text_object, n=2):\n",
    "        self.text_object = text_object\n",
    "        self.n = n\n",
    "\n",
    "        self.tokens, self.tokens_distinct = text_object.tokens, text_object.tokens_distinct\n",
    "        self.ngrams, self.ngrams_distinct = self.create_ngrams()\n",
    "        self.token2ind, self.ind2token = text_object.token2ind, text_object.ind2token\n",
    "        self.ngram2ind, self.ind2ngram = text_object.create_word_mapping(self.ngrams_distinct)\n",
    "        self.transition_matrix_prob = self.create_transition_matrix_prob()\n",
    "\n",
    "    def create_ngrams(self):\n",
    "        sequences = [self.tokens[i:] for i in range(self.n)]\n",
    "        ngrams = [' '.join(ngram) for ngram in list(zip(*sequences))]\n",
    "        return ngrams, list(set(ngrams))\n",
    "\n",
    "    def tokens_info(self):\n",
    "        self.text_object.tokens_info()\n",
    "\n",
    "    def ngrams_info(self):\n",
    "        print('ngrams level: %d, total ngrams: %d, distinct ngrams: %d' % (\n",
    "        self.n, len(self.ngrams), len(self.ngrams_distinct)))\n",
    "\n",
    "    def random_ngram(self):\n",
    "        return np.random.choice(self.ngrams)\n",
    "\n",
    "    def create_transition_matrix(self):\n",
    "        row_ind, col_ind, values = [], [], []\n",
    "\n",
    "        for i in range(len(self.tokens[:-self.n])):\n",
    "            ngram = ' '.join(self.tokens[i:i + self.n])\n",
    "            ngram_ind = self.ngram2ind[ngram]\n",
    "            next_word_ind = self.token2ind[self.tokens[i + self.n]]\n",
    "\n",
    "            row_ind.extend([ngram_ind])\n",
    "            col_ind.extend([next_word_ind])\n",
    "            values.extend([1])\n",
    "\n",
    "        S = scipy.sparse.coo_matrix((values, (row_ind, col_ind)), shape=(len(self.ngram2ind), len(self.token2ind)))\n",
    "        return S\n",
    "\n",
    "    def create_transition_matrix_prob(self):\n",
    "        transition_matrix = self.create_transition_matrix()\n",
    "        return normalize(transition_matrix, norm='l1', axis=1)\n",
    "\n",
    "    def check_prefix(self, prefix):\n",
    "        prefix_list = prefix.split(' ')[-self.n:]\n",
    "        if len(prefix_list) < self.n:\n",
    "            warnings.warn(\n",
    "                'Prefix is too short, please provide prefix of length: %d. Random ngram used instead.' % self.n)\n",
    "            return self.random_ngram()\n",
    "        else:\n",
    "            prefix = ' '.join(prefix_list)\n",
    "            if prefix in self.ngrams:\n",
    "                return prefix\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    'Prefix is not included in ngrams of the model. Provide another prefix. Random ngram used instead.')\n",
    "                return self.random_ngram()\n",
    "\n",
    "    @staticmethod\n",
    "    def add_weights_temperature(input_weights, temperature):\n",
    "        weights = np.where(input_weights == 0, 0, np.log(input_weights + 1e-10)) / temperature\n",
    "        weights = np.exp(weights)\n",
    "        return weights / np.sum(weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def reverse_preprocess(text):\n",
    "        text_reverse = re.sub(r'\\s+([!?\"\\'().,;-])', r'\\1', text)\n",
    "        text_reverse = re.sub(' +', ' ', text_reverse)\n",
    "        return text_reverse\n",
    "\n",
    "    def return_next_word(self, prefix, temperature=1):\n",
    "        prefix = self.check_prefix(prefix)\n",
    "        prefix_ind = self.ngram2ind[prefix]\n",
    "        weights = self.transition_matrix_prob[prefix_ind].toarray()[0]\n",
    "        if temperature != 1:\n",
    "            weights = self.add_weights_temperature(weights, temperature)\n",
    "\n",
    "        token_ind = np.random.choice(range(len(weights)), p=weights)\n",
    "        next_word = self.ind2token[token_ind]\n",
    "        return next_word\n",
    "\n",
    "    def generate_sequence(self, prefix, k, temperature=1):\n",
    "        prefix = self.check_prefix(prefix)\n",
    "        sequence = prefix.split(' ')\n",
    "\n",
    "        for i in range(k):\n",
    "            next_word = self.return_next_word(prefix, temperature=temperature)\n",
    "            sequence.append(next_word)\n",
    "            prefix = ' '.join(sequence[-self.n:])\n",
    "\n",
    "        return self.reverse_preprocess(' '.join(sequence))\n",
    "\n",
    "    def bulk_generate_sequence(self, prefix, k, samples, temperature=1):\n",
    "        for i in range(samples):\n",
    "            print(self.generate_sequence(prefix, k, temperature=temperature))\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "# Loading and saving files\n",
    "\n",
    "def read_txt(path):\n",
    "    return open(path, 'r', encoding='utf-8').read()\n",
    "\n",
    "def save_txt(text, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "def save_pickle(variable, path):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(variable, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Web scraper functions\n",
    "\n",
    "def exclude_black_list(content, black_list):\n",
    "    return False if content.lower() in black_list else True\n",
    "\n",
    "def between(start, end, exclude=[]):\n",
    "    while start != end:\n",
    "        if isinstance(start, NavigableString):\n",
    "            yield start\n",
    "        elif start.name in exclude:\n",
    "            start = start.next_element\n",
    "        start = start.next_element\n",
    "\n",
    "def format_file_name(title, titles):\n",
    "    allowed_letters = ascii_lowercase + digits + '_'\n",
    "\n",
    "    title = title.split('\\n')\n",
    "    title = title[0] if len(title[0]) > 0 else title[1]\n",
    "    title = title.strip().replace(' ', '_').lower()\n",
    "    title = ''.join([letter for letter in title if letter in allowed_letters])\n",
    "    titles.append(title)\n",
    "\n",
    "    if title in titles:\n",
    "        title = title + str(titles.count(title))\n",
    "\n",
    "    return title + '.txt', titles\n",
    "\n",
    "def format_text(start, end, exclude, num_words):\n",
    "    text = ' '.join(t for t in between(start, end, exclude))\n",
    "    text = '\\n'.join(text.split(\"\\n\")[1:]).strip()\n",
    "    num_words.append(len(text.split(' ')))\n",
    "    return text, num_words\n",
    "\n",
    "def words_summary(num_words):\n",
    "    print('Number of unique files with fairy tales:', len(num_words))\n",
    "    print('Total number of words in all fairy tales:', sum(num_words))\n",
    "    print('Average number of words in a fairy tale: %d' % (sum(num_words)/len(num_words)))\n",
    "    print('Number of words in the shortest story: %d, in the longest story: %d' % (min(num_words), max(num_words)))\n",
    "\n",
    "def text_summary(text, exclude_words=[]):\n",
    "    text = text.replace('\\n', ' ').strip().split(' ')\n",
    "    words_counter = Counter(text).most_common()\n",
    "    unique_words = len(words_counter) - len(exclude_words)\n",
    "    total_words = sum([occ for word, occ in words_counter if word not in exclude_words])\n",
    "\n",
    "    print('Number of unique words:', unique_words)\n",
    "    print('Total number of words:', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "welcome-heater",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:42:29.965624Z",
     "iopub.status.busy": "2021-04-15T20:42:29.964769Z",
     "iopub.status.idle": "2021-04-15T20:43:40.459429Z",
     "shell.execute_reply": "2021-04-15T20:43:40.459963Z"
    },
    "papermill": {
     "duration": 70.520368,
     "end_time": "2021-04-15T20:43:40.460161",
     "exception": false,
     "start_time": "2021-04-15T20:42:29.939793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text file created\n",
      "training chain models\n",
      "model trained\n",
      "total tokens: 8102361, distinct tokens: 144261\n",
      "ngrams level: 3, total ngrams: 8102359, distinct ngrams: 5770935\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../input/eluvio-dataset/Eluvio_DS_Challenge.csv')\n",
    "#df = df.join(df.groupby('date_created')['up_votes'].mean(), on='date_created' ,rsuffix='_mean_date')\n",
    "#df = df.join(df.groupby('date_created')['down_votes'].mean(), on='date_created', rsuffix='_mean_date')#\n",
    "#df = df.join(df.groupby('author')['up_votes'].mean(), on='author', rsuffix='_mean_author')\n",
    "#df = df.join(df.groupby('author')['down_votes'].mean(), on='author', rsuffix = '_mean_author')\n",
    "df['title'].to_csv(r'title.txt', header=None, index=None, sep='\\n', mode='a')\n",
    "print('text file created')\n",
    "path = 'title.txt'\n",
    "input_text = read_txt(path)\n",
    "tales_text = Text(input_text)\n",
    "print('training chain models')\n",
    "chain_model_n3 = Chain(tales_text, n=3)\n",
    "print('model trained')\n",
    "chain_model_n3.tokens_info()\n",
    "chain_model_n3.ngrams_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alleged-least",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:43:40.502341Z",
     "iopub.status.busy": "2021-04-15T20:43:40.501181Z",
     "iopub.status.idle": "2021-04-15T20:43:40.769950Z",
     "shell.execute_reply": "2021-04-15T20:43:40.770436Z"
    },
    "papermill": {
     "duration": 0.296549,
     "end_time": "2021-04-15T20:43:40.770619",
     "exception": false,
     "start_time": "2021-04-15T20:43:40.474070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: 1\n",
      "Once upon a time La Dolce Vita Pics\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefixes = ['the young man', 'Once upon a', 'Time passed ,']\n",
    "temperatures = [1]\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(chain_model_n3.generate_sequence(np.random.choice(prefixes), 5, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organic-twelve",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:43:44.606288Z",
     "iopub.status.busy": "2021-04-15T20:43:41.065706Z",
     "iopub.status.idle": "2021-04-15T20:44:39.829587Z",
     "shell.execute_reply": "2021-04-15T20:44:39.828387Z"
    },
    "papermill": {
     "duration": 59.044278,
     "end_time": "2021-04-15T20:44:39.829840",
     "exception": false,
     "start_time": "2021-04-15T20:43:40.785562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 8102361, distinct tokens: 144261\n",
      "ngrams level: 5, total ngrams: 8102357, distinct ngrams: 7756811\n",
      "temperature: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: UserWarning: Prefix is not included in ngrams of the model. Provide another prefix. Random ngram used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kong Free Press Exposure to bright light can lead to greater sexual satisfaction\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain_model_n5 = Chain(tales_text, n=5)\n",
    "chain_model_n5.tokens_info()\n",
    "chain_model_n5.ngrams_info()\n",
    "\n",
    "prefixes_n5 = ['the rich men of the', 'Where are you going ?', 'Once upon a time there']\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(chain_model_n5.generate_sequence(np.random.choice(prefixes_n5), 8, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "traditional-share",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:44:43.739004Z",
     "iopub.status.busy": "2021-04-15T20:44:43.732409Z",
     "iopub.status.idle": "2021-04-15T20:45:07.843909Z",
     "shell.execute_reply": "2021-04-15T20:45:07.843306Z"
    },
    "papermill": {
     "duration": 27.957828,
     "end_time": "2021-04-15T20:45:07.844083",
     "exception": false,
     "start_time": "2021-04-15T20:44:39.886255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 8102361, distinct tokens: 144261\n",
      "ngrams level: 1, total ngrams: 8102361, distinct ngrams: 144261\n",
      "temperature: 1\n",
      "Spain passes anti- Qaeda Terrorists Was Typhoon Hagupit weakens, including the state sovereignty Erdogan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain_model_n1 = Chain(tales_text, n=1)\n",
    "chain_model_n1.tokens_info()\n",
    "chain_model_n1.ngrams_info()\n",
    "\n",
    "prefixes_n1 = ['Kuwait ', 'Spain', 'Moscow ']\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(chain_model_n1.generate_sequence(np.random.choice(prefixes_n1), 15, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "liable-oracle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:45:08.541500Z",
     "iopub.status.busy": "2021-04-15T20:45:08.540791Z",
     "iopub.status.idle": "2021-04-15T20:46:12.206359Z",
     "shell.execute_reply": "2021-04-15T20:46:12.205698Z"
    },
    "papermill": {
     "duration": 64.34504,
     "end_time": "2021-04-15T20:46:12.206558",
     "exception": false,
     "start_time": "2021-04-15T20:45:07.861518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens: 8102361, distinct tokens: 144261\n",
      "ngrams level: 7, total ngrams: 8102355, distinct ngrams: 7990827\n",
      "temperature: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:107: UserWarning: Prefix is too short, please provide prefix of length: 7. Random ngram used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film that is changing Kurdistan Merkel s sharp call to Obama after German intelligence produces plausible\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain_model_n7 = Chain(tales_text, n=7)\n",
    "chain_model_n7.tokens_info()\n",
    "chain_model_n7.ngrams_info()\n",
    "\n",
    "prefixes_n1 = ['Moscow ']\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(chain_model_n7.generate_sequence(np.random.choice(prefixes_n1), 9, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increased-tribute",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:46:12.352745Z",
     "iopub.status.busy": "2021-04-15T20:46:12.341530Z",
     "iopub.status.idle": "2021-04-15T20:48:26.884808Z",
     "shell.execute_reply": "2021-04-15T20:48:26.885366Z"
    },
    "papermill": {
     "duration": 134.658967,
     "end_time": "2021-04-15T20:48:26.885666",
     "exception": false,
     "start_time": "2021-04-15T20:46:12.226699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zlib dump duration: 134.607s\n",
      "Zlib file size: 109.649MB\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from joblib import dump, load\n",
    "pickle_file = 'chain_model_n1.joblib'\n",
    "start = time.time()\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    dump(chain_model_n1, f, compress='zlib')\n",
    "zlib_dump_duration = time.time() - start\n",
    "print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration)\n",
    "zlib_file_size = os.stat(pickle_file).st_size / 1e6\n",
    "print(\"Zlib file size: %0.3fMB\" % zlib_file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "peripheral-johnson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:48:26.930202Z",
     "iopub.status.busy": "2021-04-15T20:48:26.929193Z",
     "iopub.status.idle": "2021-04-15T20:53:35.721767Z",
     "shell.execute_reply": "2021-04-15T20:53:35.722303Z"
    },
    "papermill": {
     "duration": 308.818051,
     "end_time": "2021-04-15T20:53:35.722487",
     "exception": false,
     "start_time": "2021-04-15T20:48:26.904436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zlib dump duration: 308.770s\n",
      "Zlib file size: 280.487MB\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'chain_model_n3.joblib'\n",
    "start = time.time()\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    dump(chain_model_n3, f, compress='zlib')\n",
    "zlib_dump_duration = time.time() - start\n",
    "print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration)\n",
    "zlib_file_size = os.stat(pickle_file).st_size / 1e6\n",
    "print(\"Zlib file size: %0.3fMB\" % zlib_file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "harmful-department",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:53:35.901404Z",
     "iopub.status.busy": "2021-04-15T20:53:35.890712Z",
     "iopub.status.idle": "2021-04-15T20:59:38.592237Z",
     "shell.execute_reply": "2021-04-15T20:59:38.592750Z"
    },
    "papermill": {
     "duration": 362.81254,
     "end_time": "2021-04-15T20:59:38.592965",
     "exception": false,
     "start_time": "2021-04-15T20:53:35.780425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zlib dump duration: 362.765s\n",
      "Zlib file size: 329.151MB\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'chain_model_n5.joblib'\n",
    "start = time.time()\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    dump(chain_model_n5, f, compress='zlib')\n",
    "zlib_dump_duration = time.time() - start\n",
    "print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration)\n",
    "zlib_file_size = os.stat(pickle_file).st_size / 1e6\n",
    "print(\"Zlib file size: %0.3fMB\" % zlib_file_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intended-eleven",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:59:38.732101Z",
     "iopub.status.busy": "2021-04-15T20:59:38.731124Z",
     "iopub.status.idle": "2021-04-15T21:05:48.327441Z",
     "shell.execute_reply": "2021-04-15T21:05:48.327963Z"
    },
    "papermill": {
     "duration": 369.716225,
     "end_time": "2021-04-15T21:05:48.328160",
     "exception": false,
     "start_time": "2021-04-15T20:59:38.611935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zlib dump duration: 369.668s\n",
      "Zlib file size: 342.839MB\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'chain_model_n7.joblib'\n",
    "start = time.time()\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    dump(chain_model_n7, f, compress='zlib')\n",
    "zlib_dump_duration = time.time() - start\n",
    "print(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration)\n",
    "zlib_file_size = os.stat(pickle_file).st_size / 1e6\n",
    "print(\"Zlib file size: %0.3fMB\" % zlib_file_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1425.478403,
   "end_time": "2021-04-15T21:05:54.401157",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-15T20:42:08.922754",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
